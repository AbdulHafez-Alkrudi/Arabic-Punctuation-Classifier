{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d91bebe",
   "metadata": {},
   "source": [
    "# üîß Data Preprocessing Pipeline for Arabic Punctuation Dataset\n",
    "## SSAC-UNPC Component Preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Table of Contents\n",
    "\n",
    "1. [Introduction & Setup](#1-introduction--setup)\n",
    "2. [Part 1: Problem Inspection (Before Preprocessing)](#2-part-1-problem-inspection-before-preprocessing)\n",
    "   - 2.1 Character-Level Issues\n",
    "   - 2.2 Punctuation Issues\n",
    "   - 2.3 Sentence-Level Issues\n",
    "   - 2.4 Special Pattern Issues\n",
    "3. [Part 2: Mandatory Preprocessing Steps](#3-part-2-mandatory-preprocessing-steps)\n",
    "   - 3.1 Remove Diacritics (Tashkeel)\n",
    "   - 3.2 Normalize Alef Variations\n",
    "   - 3.3 Normalize Teh Marbuta and Alef Maksura\n",
    "   - 3.4 Remove Out-of-Vocabulary Characters\n",
    "   - 3.5 Remove Latin Letters\n",
    "   - 3.6 Unify Numbers (Arabic Numerals)\n",
    "   - 3.7 Unify Punctuation (Arabic Punctuation)\n",
    "   - 3.8 Handle Consecutive Punctuation\n",
    "   - 3.9 Normalize Whitespace and Punctuation Spacing\n",
    "   - 3.10 Remove Empty and Very Short Lines\n",
    "   - 3.11 Process Long Sentences\n",
    "4. [Part 3: Optional Preprocessing Steps (For Experimentation)](#4-part-3-optional-preprocessing-steps)\n",
    "   - 4.1 Separate Waw Conjunction from Words\n",
    "   - 4.2 Stopword Handling Strategies\n",
    "   - 4.3 Number Token Replacement\n",
    "   - 4.4 Rare Word Handling\n",
    "   - 4.5 Sentence Length Normalization\n",
    "   - 4.6 Remove/Replace Foreign Terms\n",
    "5. [Part 4: Complete Preprocessing Pipeline](#5-part-4-complete-preprocessing-pipeline)\n",
    "6. [Part 5: Post-Preprocessing Inspection](#6-part-5-post-preprocessing-inspection)\n",
    "7. [Part 6: Save Preprocessed Data](#7-part-6-save-preprocessed-data)\n",
    "8. [Summary & Recommendations](#8-summary--recommendations)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d195ac",
   "metadata": {},
   "source": [
    "## 1. Introduction & Setup\n",
    "\n",
    "### üéØ Purpose of This Notebook\n",
    "\n",
    "This notebook implements a comprehensive preprocessing pipeline for the SSAC-UNPC \n",
    "component of the Arabic Punctuation Dataset. Based on the EDA findings, we address:\n",
    "\n",
    "**Issues Identified in EDA:**\n",
    "\n",
    "| Issue | Severity | Solution |\n",
    "|-------|----------|----------|\n",
    "| Diacritics present (0.27%) | Medium | Remove all tashkeel |\n",
    "| Mixed Arabic/Latin punctuation | High | Normalize to Arabic |\n",
    "| Mixed Arabic/Western numerals | Medium | Normalize to Arabic |\n",
    "| Multiple Alef forms | Low | Normalize to bare Alef |\n",
    "| Out-of-vocabulary characters | Low | Remove or replace |\n",
    "| Latin letters in text | Medium | Remove |\n",
    "| Very short sentences (<3 words) | Medium | Filter out |\n",
    "| Very long sentences (>100 words) | Low | Truncate or split |\n",
    "| Consecutive punctuation | Low | Handle appropriately |\n",
    "| Attached punctuation | Medium | Add spacing |\n",
    "\n",
    "### üìä Expected Outcomes\n",
    "\n",
    "After preprocessing:\n",
    "- Clean, consistent Arabic text\n",
    "- Unified punctuation system (Arabic only)\n",
    "- Unified numeral system (Arabic only)\n",
    "- No diacritics or special marks\n",
    "- Proper spacing around punctuation\n",
    "- Filtered short/problematic sentences\n",
    "- Ready for tokenization and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c232abf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: IMPORTS AND CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# -----------------------------\n",
    "# Standard Library Imports\n",
    "# -----------------------------\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Optional, Generator, Callable\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# -----------------------------\n",
    "# Data Analysis Libraries\n",
    "# -----------------------------\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Progress Bar\n",
    "# -----------------------------\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    TQDM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TQDM_AVAILABLE = False\n",
    "    print(\"Note: tqdm not installed. Install with: pip install tqdm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "148854e2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: LOGGER CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class NotebookLogger:\n",
    "    \"\"\"\n",
    "    Minimal unified logger for Jupyter notebooks.\n",
    "    \n",
    "    - Prints to notebook output\n",
    "    - Appends logs to a file\n",
    "    - No timestamps\n",
    "    - No session headers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        log_file: str | Path = \"preprocessing.log\",\n",
    "        enable_console: bool = True,\n",
    "        enable_file: bool = True,\n",
    "    ):\n",
    "        self.log_file = Path(log_file)\n",
    "        self.enable_console = enable_console\n",
    "        self.enable_file = enable_file\n",
    "\n",
    "        if self.enable_file:\n",
    "            self.log_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _write(self, message: str):\n",
    "        if self.enable_console:\n",
    "            print(message, end=\"\")\n",
    "\n",
    "        if self.enable_file:\n",
    "            with self.log_file.open(\"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(message)\n",
    "\n",
    "    def info(self, message: str):\n",
    "        self._write(f\"{message}\\n\")\n",
    "\n",
    "    def warn(self, message: str):\n",
    "        self._write(f\"‚ö†Ô∏è  WARNING: {message}\\n\")\n",
    "\n",
    "    def error(self, message: str):\n",
    "        self._write(f\"‚ùå ERROR: {message}\\n\")\n",
    "\n",
    "    def success(self, message: str):\n",
    "        self._write(f\"‚úÖ {message}\\n\")\n",
    "\n",
    "    def section(self, title: str):\n",
    "        block = (\n",
    "            \"\\n\" + \"=\" * 70 +\n",
    "            f\"\\n{title}\\n\" +\n",
    "            \"=\" * 70 + \"\\n\"\n",
    "        )\n",
    "        self._write(block)\n",
    "\n",
    "    def subsection(self, title: str):\n",
    "        self._write(f\"\\n--- {title} ---\\n\")\n",
    "\n",
    "\n",
    "# Initialize logger\n",
    "logger = NotebookLogger(log_file=\"logs/preprocessing.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df080787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration initialized!\n",
      "   Input directory: ../SSAC-UNPC\n",
      "   Output directory: preprocessed_data\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class PreprocessingConfig:\n",
    "    \"\"\"\n",
    "    Configuration for preprocessing pipeline.\n",
    "    \n",
    "    Separates mandatory and optional preprocessing steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    # -----------------------------\n",
    "    # File Paths\n",
    "    # -----------------------------\n",
    "    input_dir: str = \"../SSAC-UNPC\"\n",
    "    output_dir: str = \"preprocessed_data\"\n",
    "    log_dir: str = \"logs\"\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Mandatory Preprocessing (Always Applied)\n",
    "    # -----------------------------\n",
    "    remove_diacritics: bool = True\n",
    "    normalize_alef: bool = True\n",
    "    normalize_teh_marbuta: bool = True  # ÿ© ‚Üí Ÿá (optional, some keep it)\n",
    "    normalize_alef_maksura: bool = True  # Ÿâ ‚Üí Ÿä\n",
    "    remove_tatweel: bool = True\n",
    "    remove_latin_letters: bool = True\n",
    "    remove_oov_chars: bool = True\n",
    "    unify_numbers_to_arabic: bool = True\n",
    "    unify_punctuation_to_arabic: bool = True\n",
    "    handle_consecutive_punct: bool = True\n",
    "    normalize_whitespace: bool = True\n",
    "    add_punct_spacing: bool = True\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Sentence Filtering\n",
    "    # -----------------------------\n",
    "    min_words: int = 3\n",
    "    max_words: int = 100\n",
    "    remove_empty_lines: bool = True\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Optional Preprocessing (For Experimentation)\n",
    "    # -----------------------------\n",
    "    separate_waw_conjunction: bool = True\n",
    "    remove_stopwords: bool = True\n",
    "    replace_numbers_with_token: bool = True\n",
    "    replace_rare_words: bool = True\n",
    "    rare_word_threshold: int = 5\n",
    "    remove_foreign_terms: bool = True\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Processing Parameters\n",
    "    # -----------------------------\n",
    "    sample_size: Optional[int] = 1_000_000  # None = process all\n",
    "    chunk_size: int = 1_000_000  # Lines per chunk for memory efficiency\n",
    "    random_seed: int = 42\n",
    "\n",
    "\n",
    "# Initialize configuration\n",
    "config = PreprocessingConfig()\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "os.makedirs(config.log_dir, exist_ok=True)\n",
    "\n",
    "logger.info(\"‚úÖ Configuration initialized!\")\n",
    "logger.info(f\"   Input directory: {config.input_dir}\")\n",
    "logger.info(f\"   Output directory: {config.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fd0e7eb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Arabic character definitions loaded!\n",
      "   - Arabic letters: 36\n",
      "   - Diacritics: 8\n",
      "   - Stopwords: 125\n",
      "   - Valid punctuation: 6\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: ARABIC CHARACTER DEFINITIONS\n",
    "# ============================================================================\n",
    "\n",
    "# -----------------------------\n",
    "# Valid Arabic Characters\n",
    "# -----------------------------\n",
    "# Based on EDA findings: All Arabic letters found in dataset\n",
    "\n",
    "ARABIC_LETTERS = set(\n",
    "    'ÿ° ÿ¢ ÿ£ ÿ§ ÿ• ÿ¶ ÿß ÿ® ÿ© ÿ™ ÿ´ ÿ¨ ÿ≠ ÿÆ ÿØ ÿ∞ ÿ± ÿ≤ ÿ≥ ÿ¥ ÿµ ÿ∂ ÿ∑ ÿ∏ ÿπ ÿ∫ ŸÅ ŸÇ ŸÉ ŸÑ ŸÖ ŸÜ Ÿá Ÿà Ÿä Ÿâ'\n",
    "    .split()\n",
    ")\n",
    "\n",
    "# Extended set including less common letters (Persian/Urdu influence in names)\n",
    "ARABIC_LETTERS_EXTENDED = ARABIC_LETTERS | set('Ÿæ ⁄Ü ⁄ò ⁄Ø ⁄§')\n",
    "\n",
    "# -----------------------------\n",
    "# Arabic Diacritics (Tashkeel)\n",
    "# -----------------------------\n",
    "ARABIC_DIACRITICS = {\n",
    "    '\\u064B': 'Fathatan',   # Ÿã\n",
    "    '\\u064C': 'Dammatan',   # Ÿå\n",
    "    '\\u064D': 'Kasratan',   # Ÿç\n",
    "    '\\u064E': 'Fatha',      # Ÿé\n",
    "    '\\u064F': 'Damma',      # Ÿè\n",
    "    '\\u0650': 'Kasra',      # Ÿê\n",
    "    '\\u0651': 'Shadda',     # Ÿë\n",
    "    '\\u0652': 'Sukun',      # Ÿí\n",
    "}\n",
    "\n",
    "DIACRITICS_PATTERN = re.compile(r'[\\u064B-\\u0652]')\n",
    "\n",
    "# -----------------------------\n",
    "# Arabic Numerals\n",
    "# -----------------------------\n",
    "ARABIC_NUMERALS = 'Ÿ†Ÿ°Ÿ¢Ÿ£Ÿ§Ÿ•Ÿ¶ŸßŸ®Ÿ©'\n",
    "WESTERN_NUMERALS = '0123456789'\n",
    "\n",
    "# Mapping tables\n",
    "WESTERN_TO_ARABIC_NUMS = str.maketrans(WESTERN_NUMERALS, ARABIC_NUMERALS)\n",
    "ARABIC_TO_WESTERN_NUMS = str.maketrans(ARABIC_NUMERALS, WESTERN_NUMERALS)\n",
    "\n",
    "# -----------------------------\n",
    "# Punctuation Marks\n",
    "# -----------------------------\n",
    "# Target punctuation (Arabic)\n",
    "ARABIC_PUNCTUATION = {\n",
    "    'ÿå': 'Arabic Comma',\n",
    "    'ÿõ': 'Arabic Semicolon',\n",
    "    'ÿü': 'Arabic Question Mark',\n",
    "    '.': 'Full Stop',\n",
    "    ':': 'Colon',\n",
    "    '!': 'Exclamation Mark',\n",
    "}\n",
    "\n",
    "# Latin equivalents to normalize\n",
    "LATIN_TO_ARABIC_PUNCT = {\n",
    "    ',': 'ÿå',   # Latin comma ‚Üí Arabic comma\n",
    "    ';': 'ÿõ',   # Latin semicolon ‚Üí Arabic semicolon\n",
    "    '?': 'ÿü',   # Latin question mark ‚Üí Arabic question mark\n",
    "}\n",
    "\n",
    "# All valid punctuation marks (after normalization)\n",
    "VALID_PUNCTUATION = set(ARABIC_PUNCTUATION.keys())\n",
    "\n",
    "# Sentence terminal marks\n",
    "SENTENCE_TERMINALS = {'.', 'ÿü', '!'}\n",
    "\n",
    "# -----------------------------\n",
    "# Alef Variations\n",
    "# -----------------------------\n",
    "ALEF_VARIATIONS = {\n",
    "    'ÿ£': 'ÿß',  # Alef with Hamza Above\n",
    "    'ÿ•': 'ÿß',  # Alef with Hamza Below\n",
    "    'ÿ¢': 'ÿß',  # Alef with Madda\n",
    "    'Ÿ±': 'ÿß',  # Alef Wasla\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Other Normalizations\n",
    "# -----------------------------\n",
    "# Teh Marbuta (ÿ©) - some normalize to Ÿá, others keep it\n",
    "# Alef Maksura (Ÿâ) - normalize to Ÿä\n",
    "\n",
    "# -----------------------------\n",
    "# Arabic Stopwords\n",
    "# -----------------------------\n",
    "ARABIC_STOPWORDS = set([\n",
    "    # Prepositions\n",
    "    'ŸÅŸä', 'ŸÖŸÜ', 'ÿπŸÑŸâ', 'ÿ•ŸÑŸâ', 'ÿßŸÑŸâ', 'ÿπŸÜ', 'ŸÖÿπ', 'ÿ®ŸäŸÜ', 'ÿπŸÜÿØ', 'ÿ≠ÿ™Ÿâ', 'ŸÖŸÜÿ∞',\n",
    "    'ÿßŸÑŸä', 'ŸÅŸâ', 'ÿπŸÑŸä',\n",
    "    # Demonstratives\n",
    "    'Ÿáÿ∞ÿß', 'Ÿáÿ∞Ÿá', 'ÿ∞ŸÑŸÉ', 'ÿ™ŸÑŸÉ', 'Ÿáÿ§ŸÑÿßÿ°', 'ÿ£ŸàŸÑÿ¶ŸÉ',\n",
    "    # Relative pronouns\n",
    "    'ÿßŸÑÿ™Ÿä', 'ÿßŸÑÿ∞Ÿä', 'ÿßŸÑŸÑÿ∞ÿßŸÜ', 'ÿßŸÑŸÑÿ™ÿßŸÜ', 'ÿßŸÑÿ∞ŸäŸÜ', 'ÿßŸÑŸÑÿßÿ™Ÿä', 'ÿßŸÑŸÑŸàÿßÿ™Ÿä',\n",
    "    # Conjunctions\n",
    "    'Ÿà', 'ÿ£Ÿà', 'ÿßŸà', 'ÿ´ŸÖ', 'ŸÑŸÉŸÜ', 'ÿ®ŸÑ', 'ÿ•ÿ∞ÿß', 'ŸÑŸà', 'ÿ•ÿ∞', 'ŸÅ',\n",
    "    # Particles\n",
    "    'ÿ£ŸÜ', 'ÿßŸÜ', 'ÿ•ŸÜ', 'ŸÇÿØ', 'ŸÑÿß', 'ŸÖÿß', 'ŸÑŸÖ', 'ŸÑŸÜ', 'ŸÑ', 'ÿ®', 'ŸÉ',\n",
    "    # Pronouns\n",
    "    'ŸáŸà', 'ŸáŸä', 'ŸáŸÖ', 'ŸáŸÜ', 'ÿ£ŸÜÿß', 'ŸÜÿ≠ŸÜ', 'ÿ£ŸÜÿ™', 'ÿ£ŸÜÿ™ŸÖ', 'ÿßŸÜÿß', 'ÿßŸÜÿ™',\n",
    "    # Auxiliary verbs\n",
    "    'ŸÉÿßŸÜ', 'ŸÉÿßŸÜÿ™', 'ŸäŸÉŸàŸÜ', 'ÿ™ŸÉŸàŸÜ', 'ŸÉÿßŸÜŸàÿß', 'ŸÑŸäÿ≥', 'ŸÑŸäÿ≥ÿ™',\n",
    "    # Others\n",
    "    'ŸÉŸÑ', 'ÿ®ÿπÿ∂', 'ÿ£Ÿä', 'ÿßŸä', 'ÿ∫Ÿäÿ±', 'ÿ®ÿπÿØ', 'ŸÇÿ®ŸÑ', 'ÿ≠Ÿäÿ´', 'ÿπŸÜÿØŸÖÿß',\n",
    "    'ÿ≠ŸàŸÑ', 'ÿØŸàŸÜ', 'ÿ∂ÿØ', 'ÿÆŸÑÿßŸÑ', 'ÿπÿ®ÿ±', 'ŸÜÿ≠Ÿà', 'ŸÅŸàŸÇ', 'ÿ™ÿ≠ÿ™',\n",
    "    # Common function words\n",
    "    'ŸàŸÅŸä', 'ŸàŸÖŸÜ', 'ŸàÿπŸÑŸâ', 'ŸàÿßŸÑŸâ', 'ŸàŸÖÿπ', 'ŸàŸáŸà', 'ŸàŸáŸä', 'ŸàŸáÿ∞ÿß', 'ŸàŸáÿ∞Ÿá',\n",
    "    'ŸÅÿ•ŸÜ', 'ŸÅÿßŸÜ', 'Ÿàÿ•ŸÜ', 'ŸàÿßŸÜ', 'ŸÑÿ£ŸÜ', 'ŸÑÿßŸÜ', 'ŸÉŸÖÿß', 'ŸÖŸÖÿß', 'ÿ•ŸÜŸá', 'ÿßŸÜŸá',\n",
    "    'ÿ•ŸÜŸáÿß', 'ÿßŸÜŸáÿß', 'ÿ£ŸÜŸá', 'ÿ£ŸÜŸáÿß', 'ÿ∞ÿßÿ™', 'ŸÑŸáÿß', 'ŸÑŸá', 'ŸÑŸáŸÖ', 'ÿ®Ÿáÿß', 'ÿ®Ÿá',\n",
    "    'ŸÅŸäŸáÿß', 'ŸÅŸäŸá', 'ŸÖŸÜŸáÿß', 'ŸÖŸÜŸá', 'ÿπŸÜŸáÿß', 'ÿπŸÜŸá', 'ÿ•ŸÑŸäŸáÿß', 'ÿ•ŸÑŸäŸá',\n",
    "    'ÿπŸÑŸäŸáÿß', 'ÿπŸÑŸäŸá', 'ŸÖÿπŸáÿß', 'ŸÖÿπŸá', 'ÿ®ŸäŸÜŸáÿß', 'ÿ®ŸäŸÜŸáŸÖ',\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# Waw Conjunction Patterns\n",
    "# -----------------------------\n",
    "# Words that commonly start with Ÿà (waw) as conjunction\n",
    "WAW_CONJUNCTION_MIN_LENGTH = 2  # Only separate if remaining word has 3+ chars\n",
    "\n",
    "logger.info(\"‚úÖ Arabic character definitions loaded!\")\n",
    "logger.info(f\"   - Arabic letters: {len(ARABIC_LETTERS)}\")\n",
    "logger.info(f\"   - Diacritics: {len(ARABIC_DIACRITICS)}\")\n",
    "logger.info(f\"   - Stopwords: {len(ARABIC_STOPWORDS)}\")\n",
    "logger.info(f\"   - Valid punctuation: {len(VALID_PUNCTUATION)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c647ea9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loading utilities ready!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: DATA LOADING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def iter_dataset_lines(dataset_dir: str, encoding: str = \"utf-8\") -> Generator[str, None, None]:\n",
    "    \"\"\"\n",
    "    Iterate over all dataset files as a single line stream.\n",
    "    \n",
    "    This function implements lazy loading to handle large datasets\n",
    "    that cannot fit in memory.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_dir : str\n",
    "        Path to directory containing .txt files\n",
    "    encoding : str\n",
    "        File encoding (default: utf-8)\n",
    "        \n",
    "    Yields:\n",
    "    -------\n",
    "    str\n",
    "        One sentence/line at a time (stripped of newline)\n",
    "    \"\"\"\n",
    "    # Get all text files sorted by name\n",
    "    txt_files = sorted(Path(dataset_dir).glob(\"*.txt\"))\n",
    "    \n",
    "    if not txt_files:\n",
    "        logger.error(f\"No .txt files found in {dataset_dir}\")\n",
    "        return\n",
    "    \n",
    "    for file_path in txt_files:\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=encoding) as f:\n",
    "                for line in f:\n",
    "                    yield line.rstrip(\"\\n\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "def count_total_lines(dataset_dir: str) -> int:\n",
    "    \"\"\"\n",
    "    Count total lines in dataset (for progress bar).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_dir : str\n",
    "        Path to dataset directory\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        Total number of lines\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for file_path in sorted(Path(dataset_dir).glob(\"*.txt\")):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            total += sum(1 for _ in f)\n",
    "    return total\n",
    "\n",
    "\n",
    "def get_sample_lines(dataset_dir: str, n: int = 1000, seed: int = 42) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get random sample of lines for inspection.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_dir : str\n",
    "        Path to dataset directory\n",
    "    n : int\n",
    "        Number of samples\n",
    "    seed : int\n",
    "        Random seed\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[str]\n",
    "        Sample lines\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Collect lines from beginning for sampling\n",
    "    lines = []\n",
    "    for i, line in enumerate(iter_dataset_lines(dataset_dir)):\n",
    "        if i >= n * 10:  # Get more than needed for random selection\n",
    "            break\n",
    "        if line.strip():\n",
    "            lines.append(line)\n",
    "    \n",
    "    return random.sample(lines, min(n, len(lines)))\n",
    "\n",
    "\n",
    "logger.info(\"‚úÖ Data loading utilities ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3d4da",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Part 1: Problem Inspection (Before Preprocessing)\n",
    "\n",
    "Before applying any preprocessing, we need to systematically identify and quantify\n",
    "all issues in the raw data. This helps us:\n",
    "\n",
    "1. Understand the scope of each problem\n",
    "2. Prioritize preprocessing steps\n",
    "3. Verify that preprocessing fixes the issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196dfe41",
   "metadata": {},
   "source": [
    "### 2.1 Character-Level Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92d5e892",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîç CHARACTER-LEVEL ISSUE INSPECTION\n",
      "======================================================================\n",
      "Inspecting 500,000 lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inspecting characters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500000/500000 [00:27<00:00, 18165.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Diacritics (Tashkeel) ---\n",
      "Total diacritics found: 48,041\n",
      "Lines with diacritics: 33,658 (6.73%)\n",
      "Diacritic distribution:\n",
      "   'Ÿã' (Fathatan): 24,831\n",
      "   'Ÿè' (Damma): 18,231\n",
      "   'Ÿë' (Shadda): 2,006\n",
      "   'Ÿé' (Fatha): 1,331\n",
      "   'Ÿê' (Kasra): 1,107\n",
      "   'Ÿç' (Kasratan): 339\n",
      "   'Ÿí' (Sukun): 186\n",
      "   'Ÿå' (Dammatan): 10\n",
      "\n",
      "--- Latin Letters ---\n",
      "Total Latin letters: 147,461\n",
      "Lines with Latin: 40,484 (8.10%)\n",
      "Top Latin letters:\n",
      "   'A': 28,172\n",
      "   'C': 16,057\n",
      "   'd': 14,566\n",
      "   'S': 13,273\n",
      "   'L': 6,214\n",
      "   'R': 5,070\n",
      "   'e': 4,721\n",
      "   'E': 4,309\n",
      "   'P': 4,106\n",
      "   'r': 3,879\n",
      "\n",
      "--- Alef Variations ---\n",
      "Total Alef variations: 0\n",
      "\n",
      "--- Tatweel (Elongation) ---\n",
      "Tatweel count: 0\n",
      "\n",
      "--- Out-of-Vocabulary Characters ---\n",
      "Total OOV characters: 178,920\n",
      "Lines with OOV: 96,269 (19.25%)\n",
      "Top OOV characters:\n",
      "   '/' (U+002F): 166,275\n",
      "   'Ô±†' (U+FC60): 5,628\n",
      "   'Ô±¢' (U+FC62): 3,159\n",
      "   '`' (U+0060): 1,979\n",
      "   'Ô±°' (U+FC61): 793\n",
      "   '‚óè' (U+25CF): 714\n",
      "   '+' (U+002B): 91\n",
      "   '=' (U+003D): 71\n",
      "   '√©' (U+00E9): 43\n",
      "   '‚ô¶' (U+2666): 40\n",
      "   '√§' (U+00E4): 37\n",
      "   '‚àô' (U+2219): 16\n",
      "   '√≥' (U+00F3): 11\n",
      "   'Ÿ™' (U+066A): 9\n",
      "   '‚Ä¢' (U+2022): 8\n",
      "   'Ôªã' (U+FECB): 7\n",
      "   '\\\\' (U+005C): 7\n",
      "   '√®' (U+00E8): 5\n",
      "   '√°' (U+00E1): 3\n",
      "   '@' (U+0040): 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INSPECTION 2.1: CHARACTER-LEVEL ISSUES\n",
    "# ============================================================================\n",
    "\n",
    "def inspect_character_issues(dataset_dir: str, sample_size: int = 500000) -> Dict:\n",
    "    \"\"\"\n",
    "    Inspect character-level issues in the dataset.\n",
    "    \n",
    "    Checks for:\n",
    "    - Diacritics (tashkeel)\n",
    "    - Out-of-vocabulary characters\n",
    "    - Latin letters\n",
    "    - Special characters\n",
    "    - Alef variations\n",
    "    - Tatweel (elongation)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_dir : str\n",
    "        Path to dataset directory\n",
    "    sample_size : int\n",
    "        Number of lines to inspect\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict\n",
    "        Dictionary containing issue statistics\n",
    "    \"\"\"\n",
    "    logger.section(\"üîç CHARACTER-LEVEL ISSUE INSPECTION\")\n",
    "    logger.info(f\"Inspecting {sample_size:,} lines...\")\n",
    "    \n",
    "    # Initialize counters\n",
    "    stats = {\n",
    "        'total_chars': 0,\n",
    "        'total_lines': 0,\n",
    "        'diacritics': Counter(),\n",
    "        'latin_letters': Counter(),\n",
    "        'oov_chars': Counter(),\n",
    "        'alef_variations': Counter(),\n",
    "        'tatweel_count': 0,\n",
    "        'lines_with_diacritics': 0,\n",
    "        'lines_with_latin': 0,\n",
    "        'lines_with_oov': 0,\n",
    "    }\n",
    "    \n",
    "    # Define valid character set\n",
    "    valid_chars = set()\n",
    "    valid_chars.update(ARABIC_LETTERS_EXTENDED)\n",
    "    valid_chars.update(ARABIC_NUMERALS)\n",
    "    valid_chars.update(WESTERN_NUMERALS)\n",
    "    valid_chars.update(VALID_PUNCTUATION)\n",
    "    valid_chars.update(LATIN_TO_ARABIC_PUNCT.keys())\n",
    "    valid_chars.update(' \\t\\n')  # Whitespace\n",
    "    valid_chars.update('()[]{}¬´¬ª\"\"\\'-‚Äì‚Äî')  # Brackets and quotes\n",
    "    valid_chars.update(ARABIC_DIACRITICS.keys())  # Diacritics (to count)\n",
    "    \n",
    "    # Latin letter pattern\n",
    "    latin_pattern = re.compile(r'[A-Za-z]')\n",
    "    \n",
    "    # Create iterator\n",
    "    iterator = iter_dataset_lines(dataset_dir)\n",
    "    if TQDM_AVAILABLE:\n",
    "        iterator = tqdm(iterator, total=sample_size, desc=\"Inspecting characters\")\n",
    "    \n",
    "    for i, line in enumerate(iterator):\n",
    "        if i >= sample_size:\n",
    "            break\n",
    "        \n",
    "        stats['total_lines'] += 1\n",
    "        stats['total_chars'] += len(line)\n",
    "        \n",
    "        has_diacritics = False\n",
    "        has_latin = False\n",
    "        has_oov = False\n",
    "        \n",
    "        for char in line:\n",
    "            # Check for diacritics\n",
    "            if char in ARABIC_DIACRITICS:\n",
    "                stats['diacritics'][char] += 1\n",
    "                has_diacritics = True\n",
    "            \n",
    "            # Check for Latin letters\n",
    "            if latin_pattern.match(char):\n",
    "                stats['latin_letters'][char] += 1\n",
    "                has_latin = True\n",
    "            \n",
    "            # Check for Alef variations\n",
    "            if char in ALEF_VARIATIONS:\n",
    "                stats['alef_variations'][char] += 1\n",
    "            \n",
    "            # Check for Tatweel\n",
    "            if char == '\\u0640':\n",
    "                stats['tatweel_count'] += 1\n",
    "            \n",
    "            # Check for OOV characters\n",
    "            if char not in valid_chars and not latin_pattern.match(char):\n",
    "                stats['oov_chars'][char] += 1\n",
    "                has_oov = True\n",
    "        \n",
    "        if has_diacritics:\n",
    "            stats['lines_with_diacritics'] += 1\n",
    "        if has_latin:\n",
    "            stats['lines_with_latin'] += 1\n",
    "        if has_oov:\n",
    "            stats['lines_with_oov'] += 1\n",
    "    \n",
    "    # Display results\n",
    "    logger.subsection(\"Diacritics (Tashkeel)\")\n",
    "    total_diacritics = sum(stats['diacritics'].values())\n",
    "    logger.info(f\"Total diacritics found: {total_diacritics:,}\")\n",
    "    logger.info(f\"Lines with diacritics: {stats['lines_with_diacritics']:,} ({stats['lines_with_diacritics']/stats['total_lines']*100:.2f}%)\")\n",
    "    \n",
    "    if stats['diacritics']:\n",
    "        logger.info(\"Diacritic distribution:\")\n",
    "        for char, count in stats['diacritics'].most_common():\n",
    "            name = ARABIC_DIACRITICS.get(char, 'Unknown')\n",
    "            logger.info(f\"   {repr(char)} ({name}): {count:,}\")\n",
    "    \n",
    "    logger.subsection(\"Latin Letters\")\n",
    "    total_latin = sum(stats['latin_letters'].values())\n",
    "    logger.info(f\"Total Latin letters: {total_latin:,}\")\n",
    "    logger.info(f\"Lines with Latin: {stats['lines_with_latin']:,} ({stats['lines_with_latin']/stats['total_lines']*100:.2f}%)\")\n",
    "    \n",
    "    if stats['latin_letters']:\n",
    "        logger.info(\"Top Latin letters:\")\n",
    "        for char, count in stats['latin_letters'].most_common(10):\n",
    "            logger.info(f\"   '{char}': {count:,}\")\n",
    "    \n",
    "    logger.subsection(\"Alef Variations\")\n",
    "    total_alef_var = sum(stats['alef_variations'].values())\n",
    "    logger.info(f\"Total Alef variations: {total_alef_var:,}\")\n",
    "    \n",
    "    if stats['alef_variations']:\n",
    "        for char, count in stats['alef_variations'].most_common():\n",
    "            logger.info(f\"   '{char}': {count:,}\")\n",
    "    \n",
    "    logger.subsection(\"Tatweel (Elongation)\")\n",
    "    logger.info(f\"Tatweel count: {stats['tatweel_count']:,}\")\n",
    "    \n",
    "    logger.subsection(\"Out-of-Vocabulary Characters\")\n",
    "    total_oov = sum(stats['oov_chars'].values())\n",
    "    logger.info(f\"Total OOV characters: {total_oov:,}\")\n",
    "    logger.info(f\"Lines with OOV: {stats['lines_with_oov']:,} ({stats['lines_with_oov']/stats['total_lines']*100:.2f}%)\")\n",
    "    \n",
    "    if stats['oov_chars']:\n",
    "        logger.info(\"Top OOV characters:\")\n",
    "        for char, count in stats['oov_chars'].most_common(20):\n",
    "            try:\n",
    "                char_name = f\"U+{ord(char):04X}\"\n",
    "            except:\n",
    "                char_name = \"Unknown\"\n",
    "            logger.info(f\"   {repr(char)} ({char_name}): {count:,}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# Run character inspection\n",
    "char_issues = inspect_character_issues(config.input_dir, sample_size=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c0c68",
   "metadata": {},
   "source": [
    "### 2.2 Punctuation Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4504e08",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîç PUNCTUATION ISSUE INSPECTION\n",
      "======================================================================\n",
      "Inspecting 500,000 lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inspecting punctuation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500000/500000 [00:12<00:00, 40799.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Arabic Punctuation ---\n",
      "   'ÿå' (Arabic Comma): 631,288\n",
      "   '.' (Full Stop): 498,701\n",
      "   'ÿõ' (Arabic Semicolon): 59,512\n",
      "   ':' (Colon): 26,744\n",
      "   'ÿü' (Arabic Question Mark): 2,609\n",
      "   '!' (Exclamation Mark): 59\n",
      "\n",
      "--- Latin Punctuation (needs normalization) ---\n",
      "   ',': 9,133 ‚Üí should become 'ÿå'\n",
      "   ';': 36 ‚Üí should become 'ÿõ'\n",
      "   '?': 1 ‚Üí should become 'ÿü'\n",
      "\n",
      "--- Mixed Punctuation Lines ---\n",
      "Lines with mixed Arabic/Latin punctuation: 6,494\n",
      "Percentage: 1.30%\n",
      "\n",
      "--- Consecutive Punctuation ---\n",
      "Total occurrences: 64\n",
      "Examples:\n",
      "   ',ÿå' in: )Ÿ°Ÿ¶Ÿ®( ÿßŸÑŸÇÿ±ÿßÿ± Ÿ°Ÿ§Ÿ§Ÿ§ )ÿØ - Ÿ•Ÿ•(ÿõ ÿßŸÑÿØŸàÿ±ÿ© ÿßŸÑÿπÿßÿØŸäÿ© ÿßŸÑÿ™ÿßÿ≥ÿπÿ© ŸàÿßŸÑÿπÿ¥ÿ±ŸàŸÜ ...\n",
      "   ':.' in: ÿßŸÑÿ≥ŸÜÿ© ÿßŸÑŸÖÿßŸÑŸäÿ©:....\n",
      "   'ÿå:' in: 8(ÿå ÿßŸÑŸÖÿ¨ŸÑÿØ ÿßŸÑÿßŸàŸÑÿå: ÿßŸÑŸÇÿ±ÿßÿ±ÿßÿ™ ÿßŸÑÿ™Ÿä ÿßÿπÿ™ŸÖÿØŸáÿß ÿßŸÑŸÖÿ§ÿ™ŸÖÿ±ÿå ÿßŸÑŸÇÿ±ÿßÿ± ÿßŸÑÿß...\n",
      "   '?,' in: , Latin American adjustment: how much has happened?, Institu...\n",
      "   'ÿåÿõ' in: ŸàŸÖÿ≤ÿßŸäÿß ÿßŸÑŸÑÿßŸÖÿ±ŸÉÿ≤Ÿäÿ© ŸÖÿπÿ±ŸàŸÅÿ©: ÿπÿØŸÖ ÿßÿ±ŸáÿßŸÇ ŸÉÿ®ÿßÿ± ÿßŸÑŸÖÿØŸäÿ±ŸäŸÜ ÿ®ÿßŸÑŸÖÿ≥ÿ§ŸàŸÑŸäÿß...\n",
      "\n",
      "--- Attached Punctuation ---\n",
      "Cases where punctuation lacks proper spacing: 13,935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INSPECTION 2.2: PUNCTUATION ISSUES\n",
    "# ============================================================================\n",
    "\n",
    "def inspect_punctuation_issues(dataset_dir: str, sample_size: int = 500000) -> Dict:\n",
    "    \"\"\"\n",
    "    Inspect punctuation-related issues.\n",
    "    \n",
    "    Checks for:\n",
    "    - Mixed Arabic/Latin punctuation\n",
    "    - Consecutive punctuation\n",
    "    - Missing spacing around punctuation\n",
    "    - Invalid punctuation marks\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_dir : str\n",
    "        Path to dataset directory\n",
    "    sample_size : int\n",
    "        Number of lines to inspect\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict\n",
    "        Dictionary containing issue statistics\n",
    "    \"\"\"\n",
    "    logger.section(\"üîç PUNCTUATION ISSUE INSPECTION\")\n",
    "    logger.info(f\"Inspecting {sample_size:,} lines...\")\n",
    "    \n",
    "    stats = {\n",
    "        'total_lines': 0,\n",
    "        'arabic_punct': Counter(),\n",
    "        'latin_punct': Counter(),\n",
    "        'other_punct': Counter(),\n",
    "        'consecutive_punct': [],  # Store examples\n",
    "        'consecutive_punct_count': 0,\n",
    "        'attached_punct_count': 0,\n",
    "        'lines_with_mixed_punct': 0,\n",
    "    }\n",
    "    \n",
    "    # All punctuation for detection\n",
    "    all_punct = set(ARABIC_PUNCTUATION.keys()) | set(LATIN_TO_ARABIC_PUNCT.keys())\n",
    "    \n",
    "    # Pattern for consecutive punctuation\n",
    "    consecutive_pattern = re.compile(r'[ÿåÿõÿü.,:;?!]{2,}')\n",
    "    \n",
    "    # Pattern for attached punctuation (no space before/after)\n",
    "    # Arabic word followed immediately by punctuation with no space\n",
    "    attached_pattern = re.compile(r'[\\u0600-\\u06FF][ÿåÿõÿü.:!][\\u0600-\\u06FF]')\n",
    "    \n",
    "    iterator = iter_dataset_lines(dataset_dir)\n",
    "    if TQDM_AVAILABLE:\n",
    "        iterator = tqdm(iterator, total=sample_size, desc=\"Inspecting punctuation\")\n",
    "    \n",
    "    for i, line in enumerate(iterator):\n",
    "        if i >= sample_size:\n",
    "            break\n",
    "        \n",
    "        stats['total_lines'] += 1\n",
    "        \n",
    "        has_arabic_punct = False\n",
    "        has_latin_punct = False\n",
    "        \n",
    "        # Count punctuation types\n",
    "        for char in line:\n",
    "            if char in ARABIC_PUNCTUATION:\n",
    "                stats['arabic_punct'][char] += 1\n",
    "                has_arabic_punct = True\n",
    "            elif char in LATIN_TO_ARABIC_PUNCT:\n",
    "                stats['latin_punct'][char] += 1\n",
    "                has_latin_punct = True\n",
    "            elif char in '()[]{}¬´¬ª\"\"\\'':\n",
    "                stats['other_punct'][char] += 1\n",
    "        \n",
    "        # Check for mixed punctuation\n",
    "        if has_arabic_punct and has_latin_punct:\n",
    "            stats['lines_with_mixed_punct'] += 1\n",
    "        \n",
    "        # Check for consecutive punctuation\n",
    "        consecutive_matches = consecutive_pattern.findall(line)\n",
    "        if consecutive_matches:\n",
    "            stats['consecutive_punct_count'] += len(consecutive_matches)\n",
    "            if len(stats['consecutive_punct']) < 20:  # Store examples\n",
    "                for match in consecutive_matches:\n",
    "                    stats['consecutive_punct'].append((match, line[:100]))\n",
    "        \n",
    "        # Check for attached punctuation\n",
    "        attached_matches = attached_pattern.findall(line)\n",
    "        if attached_matches:\n",
    "            stats['attached_punct_count'] += len(attached_matches)\n",
    "    \n",
    "    # Display results\n",
    "    logger.subsection(\"Arabic Punctuation\")\n",
    "    for char, count in stats['arabic_punct'].most_common():\n",
    "        name = ARABIC_PUNCTUATION.get(char, 'Unknown')\n",
    "        logger.info(f\"   '{char}' ({name}): {count:,}\")\n",
    "    \n",
    "    logger.subsection(\"Latin Punctuation (needs normalization)\")\n",
    "    for char, count in stats['latin_punct'].most_common():\n",
    "        logger.info(f\"   '{char}': {count:,} ‚Üí should become '{LATIN_TO_ARABIC_PUNCT.get(char, char)}'\")\n",
    "    \n",
    "    logger.subsection(\"Mixed Punctuation Lines\")\n",
    "    logger.info(f\"Lines with mixed Arabic/Latin punctuation: {stats['lines_with_mixed_punct']:,}\")\n",
    "    logger.info(f\"Percentage: {stats['lines_with_mixed_punct']/stats['total_lines']*100:.2f}%\")\n",
    "    \n",
    "    logger.subsection(\"Consecutive Punctuation\")\n",
    "    logger.info(f\"Total occurrences: {stats['consecutive_punct_count']:,}\")\n",
    "    if stats['consecutive_punct']:\n",
    "        logger.info(\"Examples:\")\n",
    "        for match, context in stats['consecutive_punct'][:5]:\n",
    "            logger.info(f\"   '{match}' in: {context[:60]}...\")\n",
    "    \n",
    "    logger.subsection(\"Attached Punctuation\")\n",
    "    logger.info(f\"Cases where punctuation lacks proper spacing: {stats['attached_punct_count']:,}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# Run punctuation inspection\n",
    "punct_issues = inspect_punctuation_issues(config.input_dir, sample_size=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5418d51c",
   "metadata": {},
   "source": [
    "### 2.3 Sentence-Level Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4a78ff2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîç SENTENCE-LEVEL ISSUE INSPECTION\n",
      "======================================================================\n",
      "Inspecting 1,000,000 lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inspecting sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000000/1000000 [00:16<00:00, 59504.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Empty Lines ---\n",
      "Empty lines: 0 (0.0000%)\n",
      "\n",
      "--- Sentence Length Statistics ---\n",
      "Mean: 25.78 words\n",
      "Median: 22.00 words\n",
      "Min: 1 words\n",
      "Max: 4036 words\n",
      "Std: 30.16 words\n",
      "\n",
      "--- Very Short Sentences (<3 words) ---\n",
      "Count: 30,783 (3.08%)\n",
      "Examples:\n",
      "   '1(.'\n",
      "   'M.'\n",
      "   ', S.'\n",
      "   'J.'\n",
      "   'D.'\n",
      "\n",
      "--- Very Long Sentences (>100 words) ---\n",
      "Count: 7,861 (0.79%)\n",
      "Examples:\n",
      "   [156 words] )ÿß( ÿ™ÿ™ÿπŸáÿØ ÿ®ÿßŸÑÿßŸÖÿ™ŸÜÿßÿπ ÿßÿ´ŸÜÿßÿ° Ÿáÿ∞Ÿá ÿßŸÑŸÅÿ™ÿ±ÿ© ÿπŸÜ ÿßŸÑÿ™ŸÇÿØŸÖ ÿ®ÿßŸä ÿßŸÇÿ™ÿ±ÿßÿ≠ ŸÑŸÜÿ≤ÿπ ÿßŸÑÿ´ŸÇÿ© ŸÖŸÜ ÿ≠ŸÉŸàŸÖÿ© ÿßŸÑŸàŸÅÿßŸÇ ÿßŸÑŸàÿ∑ŸÜŸäÿ© ÿ∑ÿßŸÑŸÖÿß ÿ™...\n",
      "   [101 words] ÿßŸÑŸÅ ÿßŸÑÿπÿØŸäÿØ ŸÖŸÜ ÿßŸÑŸÉÿ™ÿ® ŸàÿßŸÑŸÖŸÇÿßŸÑÿßÿ™ÿå ŸÖŸÜŸáÿß Les exceptions pr√©liminaires dans la proc√©dure de la cour intern...\n",
      "   [770 words] Ÿ° - ÿ™ÿ≠Ÿäÿ∑ ÿπŸÑŸÖÿß ŸÖÿπ ÿßŸÑÿ™ŸÇÿØŸäÿ± ÿ®ÿ™ŸÇÿ±Ÿäÿ± ÿßŸÑÿßŸÖŸäŸÜ ÿßŸÑÿπÿßŸÖ ÿπŸÜ ÿ≠ÿßŸÑÿ© ÿßŸÑÿßÿπŸÖÿßŸÑ ÿßŸÑÿ™ÿ≠ÿ∂Ÿäÿ±Ÿäÿ© ŸÑŸÑÿ≥ŸÜÿ© ÿßŸÑÿØŸàŸÑŸäÿ© ŸÑŸÑÿßÿ≥ÿ±ÿ©ÿõŸ¢ - ÿ™ÿπÿ±ÿ®...\n",
      "\n",
      "--- Terminal Punctuation Issues ---\n",
      "Lines missing standard terminal: 0\n",
      "\n",
      "--- Multiple Sentence Terminals ---\n",
      "Lines with terminal punctuation mid-sentence: 2,327\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INSPECTION 2.3: SENTENCE-LEVEL ISSUES\n",
    "# ============================================================================\n",
    "\n",
    "def inspect_sentence_issues(dataset_dir: str, sample_size: int = 1000000) -> Dict:\n",
    "    \"\"\"\n",
    "    Inspect sentence-level issues.\n",
    "    \n",
    "    Checks for:\n",
    "    - Empty lines\n",
    "    - Very short sentences\n",
    "    - Very long sentences\n",
    "    - Missing terminal punctuation\n",
    "    - Multiple sentences in one line\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_dir : str\n",
    "        Path to dataset directory\n",
    "    sample_size : int\n",
    "        Number of lines to inspect\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict\n",
    "        Dictionary containing issue statistics\n",
    "    \"\"\"\n",
    "    logger.section(\"üîç SENTENCE-LEVEL ISSUE INSPECTION\")\n",
    "    logger.info(f\"Inspecting {sample_size:,} lines...\")\n",
    "    \n",
    "    stats = {\n",
    "        'total_lines': 0,\n",
    "        'empty_lines': 0,\n",
    "        'very_short': [],  # < 3 words\n",
    "        'very_long': [],   # > 100 words\n",
    "        'word_counts': [],\n",
    "        'missing_terminal': 0,\n",
    "        'wrong_terminal': Counter(),\n",
    "        'multiple_terminals': 0,\n",
    "    }\n",
    "    \n",
    "    iterator = iter_dataset_lines(dataset_dir)\n",
    "    if TQDM_AVAILABLE:\n",
    "        iterator = tqdm(iterator, total=sample_size, desc=\"Inspecting sentences\")\n",
    "    \n",
    "    for i, line in enumerate(iterator):\n",
    "        if i >= sample_size:\n",
    "            break\n",
    "        \n",
    "        stats['total_lines'] += 1\n",
    "        \n",
    "        # Check empty\n",
    "        stripped = line.strip()\n",
    "        if not stripped:\n",
    "            stats['empty_lines'] += 1\n",
    "            continue\n",
    "        \n",
    "        # Count words\n",
    "        words = stripped.split()\n",
    "        word_count = len(words)\n",
    "        stats['word_counts'].append(word_count)\n",
    "        \n",
    "        # Check very short\n",
    "        if word_count < 3:\n",
    "            if len(stats['very_short']) < 20:  # Store examples\n",
    "                stats['very_short'].append(stripped)\n",
    "        \n",
    "        # Check very long\n",
    "        if word_count > 100:\n",
    "            if len(stats['very_long']) < 10:\n",
    "                stats['very_long'].append((word_count, stripped[:100] + \"...\"))\n",
    "        \n",
    "        # Check terminal punctuation\n",
    "        if stripped:\n",
    "            last_char = stripped[-1]\n",
    "            if last_char not in SENTENCE_TERMINALS:\n",
    "                stats['missing_terminal'] += 1\n",
    "                stats['wrong_terminal'][last_char] += 1\n",
    "        \n",
    "        # Check for multiple sentence terminals within line\n",
    "        terminal_count = sum(1 for c in stripped[:-1] if c in SENTENCE_TERMINALS)\n",
    "        if terminal_count > 0:\n",
    "            stats['multiple_terminals'] += 1\n",
    "    \n",
    "    # Display results\n",
    "    logger.subsection(\"Empty Lines\")\n",
    "    logger.info(f\"Empty lines: {stats['empty_lines']:,} ({stats['empty_lines']/stats['total_lines']*100:.4f}%)\")\n",
    "    \n",
    "    logger.subsection(\"Sentence Length Statistics\")\n",
    "    if stats['word_counts']:\n",
    "        word_arr = np.array(stats['word_counts'])\n",
    "        logger.info(f\"Mean: {np.mean(word_arr):.2f} words\")\n",
    "        logger.info(f\"Median: {np.median(word_arr):.2f} words\")\n",
    "        logger.info(f\"Min: {np.min(word_arr)} words\")\n",
    "        logger.info(f\"Max: {np.max(word_arr)} words\")\n",
    "        logger.info(f\"Std: {np.std(word_arr):.2f} words\")\n",
    "    \n",
    "    logger.subsection(\"Very Short Sentences (<3 words)\")\n",
    "    short_count = sum(1 for w in stats['word_counts'] if w < 3)\n",
    "    logger.info(f\"Count: {short_count:,} ({short_count/stats['total_lines']*100:.2f}%)\")\n",
    "    if stats['very_short']:\n",
    "        logger.info(\"Examples:\")\n",
    "        for example in stats['very_short'][:5]:\n",
    "            logger.info(f\"   '{example}'\")\n",
    "    \n",
    "    logger.subsection(\"Very Long Sentences (>100 words)\")\n",
    "    long_count = sum(1 for w in stats['word_counts'] if w > 100)\n",
    "    logger.info(f\"Count: {long_count:,} ({long_count/stats['total_lines']*100:.2f}%)\")\n",
    "    if stats['very_long']:\n",
    "        logger.info(\"Examples:\")\n",
    "        for wc, example in stats['very_long'][:3]:\n",
    "            logger.info(f\"   [{wc} words] {example}\")\n",
    "    \n",
    "    logger.subsection(\"Terminal Punctuation Issues\")\n",
    "    logger.info(f\"Lines missing standard terminal: {stats['missing_terminal']:,}\")\n",
    "    if stats['wrong_terminal']:\n",
    "        logger.info(\"Non-standard terminals found:\")\n",
    "        for char, count in stats['wrong_terminal'].most_common(10):\n",
    "            logger.info(f\"   '{char}': {count:,}\")\n",
    "    \n",
    "    logger.subsection(\"Multiple Sentence Terminals\")\n",
    "    logger.info(f\"Lines with terminal punctuation mid-sentence: {stats['multiple_terminals']:,}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# Run sentence inspection\n",
    "sentence_issues = inspect_sentence_issues(config.input_dir, sample_size=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ceb3e8",
   "metadata": {},
   "source": [
    "### 2.4 Special Pattern Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a44459c7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîç SPECIAL PATTERN INSPECTION\n",
      "======================================================================\n",
      "Inspecting 500,000 lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inspecting patterns: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500000/500000 [00:13<00:00, 38260.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Waw Conjunction Patterns ---\n",
      "Total words starting with Ÿà: 1,212,040\n",
      "Most common ŸàŸé-attached words:\n",
      "   'ŸàŸÅŸä': 36,107\n",
      "   'ŸàŸÇÿØ': 21,846\n",
      "   'ŸàŸÖŸÜ': 16,316\n",
      "   'ŸàÿßŸÜ': 16,131\n",
      "   'ŸàŸÑÿß': 13,358\n",
      "   'ŸàÿπŸÑŸâ': 8,929\n",
      "   'ŸàŸáŸä': 8,436\n",
      "   'Ÿàÿ∂ÿπ': 8,331\n",
      "   'ŸàŸÅŸÇÿß': 7,859\n",
      "   'ŸàŸáŸà': 7,644\n",
      "   'ŸàŸäŸÜÿ®ÿ∫Ÿä': 7,473\n",
      "   'ŸàÿßŸÑÿ™ŸÜŸÖŸäÿ©': 7,015\n",
      "   'Ÿàÿ∞ŸÑŸÉ': 6,968\n",
      "   'ŸàŸÉÿ∞ŸÑŸÉ': 5,639\n",
      "   'ŸàŸÅŸäŸÖÿß': 5,473\n",
      "\n",
      "--- Number Systems ---\n",
      "Arabic numerals found: 531,683\n",
      "Western numerals found: 81,871\n",
      "Lines with mixed numerals: 16,461\n",
      "\n",
      "--- Document References ---\n",
      "Examples: ['A/47/10', 'S/25704', 'A/47/975', 'S/26063', 'A/47/975', 'S/26063', 'S/5634', 'S/5910', 'S/12342', 'S/25704']\n",
      "\n",
      "--- Repeated Characters ---\n",
      "Lines with repeated characters:\n",
      "   )ÿß( ÿßÿπÿ™ŸÖÿßÿØ ŸÖÿ®ŸÑÿ∫ ÿßÿ¨ŸÖÿßŸÑŸä ŸÇÿØÿ±Ÿá Ÿ®Ÿ†Ÿ† Ÿ¢Ÿ•Ÿ® Ÿ¢Ÿ• ŸÖŸÜ ÿØŸàŸÑÿßÿ±ÿßÿ™ ÿßŸÑŸàŸÑÿßŸäÿßÿ™ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ© )ÿµÿßŸÅŸäŸá Ÿ†Ÿ†Ÿ† Ÿ¢Ÿ°\n",
      "   ŸàŸÉÿßÿ¨ÿ±ÿßÿ° ŸÖÿ§ŸÇÿ™ÿå Ÿäÿ¨ÿ±Ÿä ŸÇÿ≥ŸÖÿ© ÿßŸÑŸÖÿ®ŸÑÿ∫ ÿßŸÑÿ∞Ÿä ÿ™ŸÇÿ±ÿ±Ÿá ÿßŸÑŸÑÿ¨ŸÜÿ© ÿßŸÑÿßÿ≥ÿ™ÿ¥ÿßÿ±Ÿäÿ© ÿ®ŸäŸÜ ÿßŸÑÿØŸàŸÑ ÿßŸÑÿßÿπÿ∂ÿßÿ°ÿå Ÿà\n",
      "   )Ÿ¢( ŸÖÿπÿßŸáÿØÿ© ÿßŸÑŸÖÿ®ÿßÿØÿ¶ ÿßŸÑŸÖŸÜÿ∏ŸÖÿ© ŸÑÿßŸÜÿ¥ÿ∑ÿ© ÿßŸÑÿØŸàŸÑ ŸÅŸä ŸÖŸäÿØÿßŸÜ ÿßÿ≥ÿ™ŸÉÿ¥ÿßŸÅ Ÿàÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑŸÅÿ∂ÿßÿ° ÿßŸÑÿÆÿßÿ±ÿ¨Ÿä\n",
      "\n",
      "--- Foreign Words ---\n",
      "Unique foreign words: 2,851\n",
      "Most common foreign words:\n",
      "   'Add': 6,581\n",
      "   'Rev': 1,386\n",
      "   'Corr': 828\n",
      "   'CRP': 527\n",
      "   'FCCC': 510\n",
      "   'Sub': 401\n",
      "   'PCN': 314\n",
      "   'LOS': 312\n",
      "   'CCPR': 307\n",
      "   'CONF': 301\n",
      "   'UNEP': 202\n",
      "   'and': 187\n",
      "   'Part': 182\n",
      "   'SCN': 167\n",
      "   'NUW': 153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INSPECTION 2.4: SPECIAL PATTERN ISSUES\n",
    "# ============================================================================\n",
    "\n",
    "def inspect_special_patterns(dataset_dir: str, sample_size: int = 500000) -> Dict:\n",
    "    \"\"\"\n",
    "    Inspect special patterns that might need handling.\n",
    "    \n",
    "    Checks for:\n",
    "    - Waw conjunction attached to words\n",
    "    - Numbers (Arabic and Western)\n",
    "    - Document references (e.g., A/47/10)\n",
    "    - URLs or email-like patterns\n",
    "    - Repeated characters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_dir : str\n",
    "        Path to dataset directory\n",
    "    sample_size : int\n",
    "        Number of lines to inspect\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict\n",
    "        Dictionary containing pattern statistics\n",
    "    \"\"\"\n",
    "    logger.section(\"üîç SPECIAL PATTERN INSPECTION\")\n",
    "    logger.info(f\"Inspecting {sample_size:,} lines...\")\n",
    "    \n",
    "    stats = {\n",
    "        'total_lines': 0,\n",
    "        'waw_attached': Counter(),  # Words starting with Ÿà\n",
    "        'arabic_numbers': 0,\n",
    "        'western_numbers': 0,\n",
    "        'mixed_number_lines': 0,\n",
    "        'doc_references': [],\n",
    "        'repeated_chars': [],\n",
    "        'foreign_words': Counter(),\n",
    "    }\n",
    "    \n",
    "    # Patterns\n",
    "    waw_word_pattern = re.compile(r'\\bŸà[\\u0600-\\u06FF]{2,}\\b')\n",
    "    arabic_num_pattern = re.compile(r'[Ÿ†-Ÿ©]+')\n",
    "    western_num_pattern = re.compile(r'[0-9]+')\n",
    "    doc_ref_pattern = re.compile(r'[A-Z]/[0-9]+(?:/[0-9A-Z]+)*')\n",
    "    repeated_pattern = re.compile(r'(.)\\1{3,}')  # Same char 4+ times\n",
    "    foreign_word_pattern = re.compile(r'\\b[A-Za-z]{3,}\\b')  # 3+ Latin letters\n",
    "    \n",
    "    iterator = iter_dataset_lines(dataset_dir)\n",
    "    if TQDM_AVAILABLE:\n",
    "        iterator = tqdm(iterator, total=sample_size, desc=\"Inspecting patterns\")\n",
    "    \n",
    "    for i, line in enumerate(iterator):\n",
    "        if i >= sample_size:\n",
    "            break\n",
    "        \n",
    "        stats['total_lines'] += 1\n",
    "        \n",
    "        # Waw-attached words\n",
    "        waw_matches = waw_word_pattern.findall(line)\n",
    "        for match in waw_matches:\n",
    "            stats['waw_attached'][match] += 1\n",
    "        \n",
    "        # Number systems\n",
    "        has_arabic = bool(arabic_num_pattern.search(line))\n",
    "        has_western = bool(western_num_pattern.search(line))\n",
    "        \n",
    "        if has_arabic:\n",
    "            stats['arabic_numbers'] += len(arabic_num_pattern.findall(line))\n",
    "        if has_western:\n",
    "            stats['western_numbers'] += len(western_num_pattern.findall(line))\n",
    "        if has_arabic and has_western:\n",
    "            stats['mixed_number_lines'] += 1\n",
    "        \n",
    "        # Document references\n",
    "        doc_refs = doc_ref_pattern.findall(line)\n",
    "        if doc_refs and len(stats['doc_references']) < 20:\n",
    "            stats['doc_references'].extend(doc_refs[:2])\n",
    "        \n",
    "        # Repeated characters\n",
    "        repeated = repeated_pattern.findall(line)\n",
    "        if repeated and len(stats['repeated_chars']) < 10:\n",
    "            stats['repeated_chars'].append(line[:80])\n",
    "        \n",
    "        # Foreign words\n",
    "        foreign = foreign_word_pattern.findall(line)\n",
    "        for word in foreign:\n",
    "            stats['foreign_words'][word] += 1\n",
    "    \n",
    "    # Display results\n",
    "    logger.subsection(\"Waw Conjunction Patterns\")\n",
    "    logger.info(f\"Total words starting with Ÿà: {sum(stats['waw_attached'].values()):,}\")\n",
    "    logger.info(\"Most common ŸàŸé-attached words:\")\n",
    "    for word, count in stats['waw_attached'].most_common(15):\n",
    "        logger.info(f\"   '{word}': {count:,}\")\n",
    "    \n",
    "    logger.subsection(\"Number Systems\")\n",
    "    logger.info(f\"Arabic numerals found: {stats['arabic_numbers']:,}\")\n",
    "    logger.info(f\"Western numerals found: {stats['western_numbers']:,}\")\n",
    "    logger.info(f\"Lines with mixed numerals: {stats['mixed_number_lines']:,}\")\n",
    "    \n",
    "    logger.subsection(\"Document References\")\n",
    "    logger.info(f\"Examples: {stats['doc_references'][:10]}\")\n",
    "    \n",
    "    logger.subsection(\"Repeated Characters\")\n",
    "    if stats['repeated_chars']:\n",
    "        logger.info(\"Lines with repeated characters:\")\n",
    "        for example in stats['repeated_chars'][:3]:\n",
    "            logger.info(f\"   {example}\")\n",
    "    else:\n",
    "        logger.info(\"No significant repeated character patterns found\")\n",
    "    \n",
    "    logger.subsection(\"Foreign Words\")\n",
    "    logger.info(f\"Unique foreign words: {len(stats['foreign_words']):,}\")\n",
    "    logger.info(\"Most common foreign words:\")\n",
    "    for word, count in stats['foreign_words'].most_common(15):\n",
    "        logger.info(f\"   '{word}': {count:,}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# Run special pattern inspection\n",
    "special_issues = inspect_special_patterns(config.input_dir, sample_size=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb2f3e",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Part 2: Mandatory Preprocessing Steps\n",
    "\n",
    "These preprocessing steps are **always applied** as they address fundamental\n",
    "data quality issues that would negatively impact model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f8364c",
   "metadata": {},
   "source": [
    "### 3.1 Remove Diacritics (Tashkeel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea92fb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß PREPROCESSING: Remove Diacritics\n",
      "======================================================================\n",
      "Test cases:\n",
      "   'ÿßŸÑŸíÿπŸéÿ±Ÿéÿ®ŸêŸäŸéŸëÿ©' ‚Üí 'ÿßŸÑÿπÿ±ÿ®Ÿäÿ©'\n",
      "   'ŸÖŸèÿ≠ŸéŸÖŸéŸëÿØ' ‚Üí 'ŸÖÿ≠ŸÖÿØ'\n",
      "   'ÿßŸÑÿ£ŸèŸÖŸéŸÖŸè ÿßŸÑŸÖŸèÿ™ŸéŸëÿ≠ŸêÿØŸéÿ©' ‚Üí 'ÿßŸÑÿ£ŸÖŸÖ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ©'\n",
      "   'text without diacritics' ‚Üí 'text without diacritics'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESSING 3.1: REMOVE DIACRITICS\n",
    "# ============================================================================\n",
    "\n",
    "def remove_diacritics(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove Arabic diacritics (tashkeel) from text.\n",
    "    \n",
    "    Diacritics removed:\n",
    "    - Fathatan (Ÿã)\n",
    "    - Dammatan (Ÿå)\n",
    "    - Kasratan (Ÿç)\n",
    "    - Fatha (Ÿé)\n",
    "    - Damma (Ÿè)\n",
    "    - Kasra (Ÿê)\n",
    "    - Shadda (Ÿë)\n",
    "    - Sukun (Ÿí)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text with potential diacritics\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with diacritics removed\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    >>> remove_diacritics(\"ÿßŸÑŸíÿπŸéÿ±Ÿéÿ®ŸêŸäŸéŸëÿ©\")\n",
    "    'ÿßŸÑÿπÿ±ÿ®Ÿäÿ©'\n",
    "    \"\"\"\n",
    "    return DIACRITICS_PATTERN.sub('', text)\n",
    "\n",
    "\n",
    "# Test the function\n",
    "logger.section(\"üîß PREPROCESSING: Remove Diacritics\")\n",
    "\n",
    "test_cases = [\n",
    "    \"ÿßŸÑŸíÿπŸéÿ±Ÿéÿ®ŸêŸäŸéŸëÿ©\",\n",
    "    \"ŸÖŸèÿ≠ŸéŸÖŸéŸëÿØ\",\n",
    "    \"ÿßŸÑÿ£ŸèŸÖŸéŸÖŸè ÿßŸÑŸÖŸèÿ™ŸéŸëÿ≠ŸêÿØŸéÿ©\",\n",
    "    \"text without diacritics\",\n",
    "]\n",
    "\n",
    "logger.info(\"Test cases:\")\n",
    "for test in test_cases:\n",
    "    result = remove_diacritics(test)\n",
    "    logger.info(f\"   '{test}' ‚Üí '{result}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cd66ca",
   "metadata": {},
   "source": [
    "### 3.2 Normalize Alef Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "698f8df6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß PREPROCESSING: Normalize Alef\n",
      "======================================================================\n",
      "Test cases:\n",
      "   'ÿ£ÿ≠ŸÖÿØ' ‚Üí 'ÿßÿ≠ŸÖÿØ'\n",
      "   'ÿ•ÿ®ÿ±ÿßŸáŸäŸÖ' ‚Üí 'ÿßÿ®ÿ±ÿßŸáŸäŸÖ'\n",
      "   'ÿ¢ÿØŸÖ' ‚Üí 'ÿßÿØŸÖ'\n",
      "   'ÿßŸÑÿ£ŸÖŸÖ' ‚Üí 'ÿßŸÑÿßŸÖŸÖ'\n",
      "   'ÿßŸÑÿ•ŸÜÿ≥ÿßŸÜ' ‚Üí 'ÿßŸÑÿßŸÜÿ≥ÿßŸÜ'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESSING 3.2: NORMALIZE ALEF VARIATIONS\n",
    "# ============================================================================\n",
    "\n",
    "# Compile pattern for efficiency\n",
    "ALEF_PATTERN = re.compile(r'[ÿ£ÿ•ÿ¢Ÿ±]')\n",
    "\n",
    "def normalize_alef(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize all Alef variations to bare Alef (ÿß).\n",
    "    \n",
    "    Normalizations:\n",
    "    - ÿ£ (Alef with Hamza Above) ‚Üí ÿß\n",
    "    - ÿ• (Alef with Hamza Below) ‚Üí ÿß\n",
    "    - ÿ¢ (Alef with Madda) ‚Üí ÿß\n",
    "    - Ÿ± (Alef Wasla) ‚Üí ÿß\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text with potential Alef variations\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with normalized Alef\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    >>> normalize_alef(\"ÿ£ÿ≠ŸÖÿØ ÿ•ÿ®ÿ±ÿßŸáŸäŸÖ ÿ¢ÿØŸÖ\")\n",
    "    'ÿßÿ≠ŸÖÿØ ÿßÿ®ÿ±ÿßŸáŸäŸÖ ÿßÿØŸÖ'\n",
    "    \"\"\"\n",
    "    return ALEF_PATTERN.sub('ÿß', text)\n",
    "\n",
    "\n",
    "# Test the function\n",
    "logger.section(\"üîß PREPROCESSING: Normalize Alef\")\n",
    "\n",
    "test_cases = [\n",
    "    \"ÿ£ÿ≠ŸÖÿØ\",\n",
    "    \"ÿ•ÿ®ÿ±ÿßŸáŸäŸÖ\",\n",
    "    \"ÿ¢ÿØŸÖ\",\n",
    "    \"ÿßŸÑÿ£ŸÖŸÖ\",\n",
    "    \"ÿßŸÑÿ•ŸÜÿ≥ÿßŸÜ\",\n",
    "]\n",
    "\n",
    "logger.info(\"Test cases:\")\n",
    "for test in test_cases:\n",
    "    result = normalize_alef(test)\n",
    "    logger.info(f\"   '{test}' ‚Üí '{result}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99ba32d",
   "metadata": {},
   "source": [
    "### 3.3 Normalize Teh Marbuta and Alef Maksura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "559b0058",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß PREPROCESSING: Normalize Teh Marbuta & Alef Maksura\n",
      "======================================================================\n",
      "Test cases:\n",
      "   'ŸÖÿØÿ±ÿ≥ÿ©' (Teh Marbuta)\n",
      "      Teh Marbuta (keep): 'ŸÖÿØÿ±ÿ≥ÿ©'\n",
      "      Alef Maksura ‚Üí Yeh: 'ŸÖÿØÿ±ÿ≥ÿ©'\n",
      "   'ÿπŸÑŸâ' (Alef Maksura)\n",
      "      Teh Marbuta (keep): 'ÿπŸÑŸâ'\n",
      "      Alef Maksura ‚Üí Yeh: 'ÿπŸÑŸä'\n",
      "   'ŸÖÿ≥ÿ™ÿ¥ŸÅŸâ' (Alef Maksura)\n",
      "      Teh Marbuta (keep): 'ŸÖÿ≥ÿ™ÿ¥ŸÅŸâ'\n",
      "      Alef Maksura ‚Üí Yeh: 'ŸÖÿ≥ÿ™ÿ¥ŸÅŸä'\n",
      "   'ÿßŸÑŸÇÿßŸáÿ±ÿ©' (Teh Marbuta)\n",
      "      Teh Marbuta (keep): 'ÿßŸÑŸÇÿßŸáÿ±ÿ©'\n",
      "      Alef Maksura ‚Üí Yeh: 'ÿßŸÑŸÇÿßŸáÿ±ÿ©'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESSING 3.3: NORMALIZE TEH MARBUTA AND ALEF MAKSURA\n",
    "# ============================================================================\n",
    "\n",
    "def normalize_teh_marbuta(text: str, to_heh: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Handle Teh Marbuta (ÿ©).\n",
    "    \n",
    "    Options:\n",
    "    - Keep as is (default for this task)\n",
    "    - Normalize to Heh (Ÿá) - some NLP applications do this\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "    to_heh : bool\n",
    "        If True, convert ÿ© to Ÿá\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Processed text\n",
    "    \"\"\"\n",
    "    if to_heh:\n",
    "        return text.replace('ÿ©', 'Ÿá')\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_alef_maksura(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize Alef Maksura (Ÿâ) to Yeh (Ÿä).\n",
    "    \n",
    "    Note: In some contexts, Ÿâ is kept distinct. For punctuation\n",
    "    prediction, normalizing improves consistency.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with Ÿâ ‚Üí Ÿä\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    >>> normalize_alef_maksura(\"ÿπŸÑŸâ\")\n",
    "    'ÿπŸÑŸä'\n",
    "    \"\"\"\n",
    "    return text.replace('Ÿâ', 'Ÿä')\n",
    "\n",
    "\n",
    "# Test the functions\n",
    "logger.section(\"üîß PREPROCESSING: Normalize Teh Marbuta & Alef Maksura\")\n",
    "\n",
    "test_cases = [\n",
    "    (\"ŸÖÿØÿ±ÿ≥ÿ©\", \"Teh Marbuta\"),\n",
    "    (\"ÿπŸÑŸâ\", \"Alef Maksura\"),\n",
    "    (\"ŸÖÿ≥ÿ™ÿ¥ŸÅŸâ\", \"Alef Maksura\"),\n",
    "    (\"ÿßŸÑŸÇÿßŸáÿ±ÿ©\", \"Teh Marbuta\"),\n",
    "]\n",
    "\n",
    "logger.info(\"Test cases:\")\n",
    "for test, note in test_cases:\n",
    "    result_tm = normalize_teh_marbuta(test, to_heh=False)\n",
    "    result_am = normalize_alef_maksura(test)\n",
    "    logger.info(f\"   '{test}' ({note})\")\n",
    "    logger.info(f\"      Teh Marbuta (keep): '{result_tm}'\")\n",
    "    logger.info(f\"      Alef Maksura ‚Üí Yeh: '{result_am}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59568789",
   "metadata": {},
   "source": [
    "### 3.4 Remove Out-of-Vocabulary Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d47a1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß PREPROCESSING: Remove OOV Characters\n",
      "======================================================================\n",
      "Valid charset size: 62\n",
      "Test cases:\n",
      "   'ÿßŸÑŸÜÿµ ÿßŸÑÿπÿ±ÿ®Ÿä ŸÖÿπ English text'\n",
      "   ‚Üí 'ÿßŸÑŸÜÿµ ÿßŸÑÿπÿ±ÿ®Ÿä ŸÖÿπ  '\n",
      "   'ÿ±ŸÇŸÖ: Ÿ°Ÿ¢Ÿ£ Ÿà 456'\n",
      "   ‚Üí 'ÿ±ŸÇŸÖ: Ÿ°Ÿ¢Ÿ£ Ÿà '\n",
      "   'ŸÖÿπ ÿ±ŸÖŸàÿ≤ ÿÆÿßÿµÿ©: @#$%'\n",
      "   ‚Üí 'ŸÖÿπ ÿ±ŸÖŸàÿ≤ ÿÆÿßÿµÿ©: '\n",
      "   '¬´ŸÜÿµ ÿ®ŸäŸÜ ÿ£ŸÇŸàÿßÿ≥¬ª'\n",
      "   ‚Üí 'ŸÜÿµ ÿ®ŸäŸÜ ÿ£ŸÇŸàÿßÿ≥'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESSING 3.4: REMOVE OUT-OF-VOCABULARY CHARACTERS\n",
    "# ============================================================================\n",
    "\n",
    "def build_valid_charset() -> set:\n",
    "    \"\"\"\n",
    "    Build the set of valid characters for Arabic punctuation task.\n",
    "    \n",
    "    Valid characters include:\n",
    "    - Arabic letters (including extended)\n",
    "    - Arabic numerals\n",
    "    - Valid punctuation marks\n",
    "    - Whitespace\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    set\n",
    "        Set of valid characters\n",
    "    \"\"\"\n",
    "    valid = set()\n",
    "    \n",
    "    # Arabic letters (basic + extended)\n",
    "    valid.update('ÿ°ÿ¢ÿ£ÿ§ÿ•ÿ¶ÿßÿ®ÿ©ÿ™ÿ´ÿ¨ÿ≠ÿÆÿØÿ∞ÿ±ÿ≤ÿ≥ÿ¥ÿµÿ∂ÿ∑ÿ∏ÿπÿ∫ŸÅŸÇŸÉŸÑŸÖŸÜŸáŸàŸä')\n",
    "    valid.update('ŸâŸæ⁄Ü⁄ò⁄Ø⁄§')  # Extended\n",
    "    \n",
    "    # Arabic numerals\n",
    "    valid.update('Ÿ†Ÿ°Ÿ¢Ÿ£Ÿ§Ÿ•Ÿ¶ŸßŸ®Ÿ©')\n",
    "    \n",
    "    # Valid punctuation (Arabic)\n",
    "    valid.update('ÿåÿõÿü.:!')\n",
    "    \n",
    "    # Basic structural characters\n",
    "    valid.update(' ')  # Space\n",
    "    \n",
    "    # Keep some brackets for structure (will be handled later if needed)\n",
    "    valid.update('()[]')\n",
    "    \n",
    "    return valid\n",
    "\n",
    "\n",
    "VALID_CHARSET = build_valid_charset()\n",
    "\n",
    "\n",
    "def remove_oov_characters(text: str, valid_chars: set = None, replacement: str = '') -> str:\n",
    "    \"\"\"\n",
    "    Remove characters not in the valid character set.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "    valid_chars : set\n",
    "        Set of valid characters (uses VALID_CHARSET if None)\n",
    "    replacement : str\n",
    "        Character to replace OOV chars with (default: remove)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with OOV characters removed\n",
    "    \"\"\"\n",
    "    if valid_chars is None:\n",
    "        valid_chars = VALID_CHARSET\n",
    "    \n",
    "    result = []\n",
    "    for char in text:\n",
    "        if char in valid_chars:\n",
    "            result.append(char)\n",
    "        elif replacement:\n",
    "            result.append(replacement)\n",
    "    \n",
    "    return ''.join(result)\n",
    "\n",
    "\n",
    "# Test the function\n",
    "logger.section(\"üîß PREPROCESSING: Remove OOV Characters\")\n",
    "\n",
    "test_cases = [\n",
    "    \"ÿßŸÑŸÜÿµ ÿßŸÑÿπÿ±ÿ®Ÿä ŸÖÿπ English text\",\n",
    "    \"ÿ±ŸÇŸÖ: Ÿ°Ÿ¢Ÿ£ Ÿà 456\",\n",
    "    \"ŸÖÿπ ÿ±ŸÖŸàÿ≤ ÿÆÿßÿµÿ©: @#$%\",\n",
    "    \"¬´ŸÜÿµ ÿ®ŸäŸÜ ÿ£ŸÇŸàÿßÿ≥¬ª\",\n",
    "]\n",
    "\n",
    "logger.info(f\"Valid charset size: {len(VALID_CHARSET)}\")\n",
    "logger.info(\"Test cases:\")\n",
    "for test in test_cases:\n",
    "    result = remove_oov_characters(test)\n",
    "    logger.info(f\"   '{test}'\")\n",
    "    logger.info(f\"   ‚Üí '{result}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1f6bf",
   "metadata": {},
   "source": [
    "### 3.5 Remove Latin Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ab582f2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß PREPROCESSING: Remove Latin Letters\n",
      "======================================================================\n",
      "Test cases:\n",
      "   'ÿßŸÑÿ£ŸÖŸÖ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ© United Nations' ‚Üí 'ÿßŸÑÿ£ŸÖŸÖ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ©  '\n",
      "   'ÿßŸÑŸàÿ´ŸäŸÇÿ© A/47/10' ‚Üí 'ÿßŸÑŸàÿ´ŸäŸÇÿ© /47/10'\n",
      "   'ÿ®ÿ±ŸÜÿßŸÖÿ¨ UNDP ŸÑŸÑÿ™ŸÜŸÖŸäÿ©' ‚Üí 'ÿ®ÿ±ŸÜÿßŸÖÿ¨  ŸÑŸÑÿ™ŸÜŸÖŸäÿ©'\n",
      "   'add. 1' ‚Üí '. 1'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESSING 3.5: REMOVE LATIN LETTERS\n",
    "# ============================================================================\n",
    "\n",
    "LATIN_PATTERN = re.compile(r'[A-Za-z]+')\n",
    "\n",
    "def remove_latin_letters(text: str, replacement: str = '') -> str:\n",
    "    \"\"\"\n",
    "    Remove Latin letters from text.\n",
    "    \n",
    "    This handles:\n",
    "    - Standalone English words\n",
    "    - Document references (A/47/10 ‚Üí /47/10)\n",
    "    - Mixed text\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "    replacement : str\n",
    "        Replacement for Latin sequences (default: remove)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text without Latin letters\n",
    "    \"\"\"\n",
    "    return LATIN_PATTERN.sub(replacement, text)\n",
    "\n",
    "\n",
    "# Test the function\n",
    "logger.section(\"üîß PREPROCESSING: Remove Latin Letters\")\n",
    "\n",
    "test_cases = [\n",
    "    \"ÿßŸÑÿ£ŸÖŸÖ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ© United Nations\",\n",
    "    \"ÿßŸÑŸàÿ´ŸäŸÇÿ© A/47/10\",\n",
    "    \"ÿ®ÿ±ŸÜÿßŸÖÿ¨ UNDP ŸÑŸÑÿ™ŸÜŸÖŸäÿ©\",\n",
    "    \"add. 1\",\n",
    "]\n",
    "\n",
    "logger.info(\"Test cases:\")\n",
    "for test in test_cases:\n",
    "    result = remove_latin_letters(test)\n",
    "    logger.info(f\"   '{test}' ‚Üí '{result}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4d4214",
   "metadata": {},
   "source": [
    "### 3.6 Unify Numbers (Arabic Numerals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d246b63",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß PREPROCESSING: Unify Numbers\n",
      "======================================================================\n",
      "Test cases (‚Üí Arabic numerals):\n",
      "   'ÿπÿßŸÖ 2024' ‚Üí 'ÿπÿßŸÖ Ÿ¢Ÿ†Ÿ¢Ÿ§'\n",
      "   'ÿ±ŸÇŸÖ Ÿ°Ÿ¢Ÿ£' ‚Üí 'ÿ±ŸÇŸÖ Ÿ°Ÿ¢Ÿ£'\n",
      "   'ŸÖÿ®ŸÑÿ∫ 100 ÿØŸàŸÑÿßÿ±' ‚Üí 'ŸÖÿ®ŸÑÿ∫ Ÿ°Ÿ†Ÿ† ÿØŸàŸÑÿßÿ±'\n",
      "   'Ÿ•Ÿ†Ÿ† + 500 = Ÿ°Ÿ†Ÿ†Ÿ†' ‚Üí 'Ÿ•Ÿ†Ÿ† + Ÿ•Ÿ†Ÿ† = Ÿ°Ÿ†Ÿ†Ÿ†'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESSING 3.6: UNIFY NUMBERS TO ARABIC\n",
    "# ============================================================================\n",
    "\n",
    "def unify_numbers_to_arabic(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert all Western numerals (0-9) to Arabic numerals (Ÿ†-Ÿ©).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text with potential Western numerals\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with Arabic numerals only\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    >>> unify_numbers_to_arabic(\"ÿπÿßŸÖ 2024\")\n",
    "    'ÿπÿßŸÖ Ÿ¢Ÿ†Ÿ¢Ÿ§'\n",
    "    \"\"\"\n",
    "    return text.translate(WESTERN_TO_ARABIC_NUMS)\n",
    "\n",
    "\n",
    "def unify_numbers_to_western(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert all Arabic numerals (Ÿ†-Ÿ©) to Western numerals (0-9).\n",
    "    \n",
    "    Alternative approach - some models prefer Western numerals.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text with potential Arabic numerals\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with Western numerals only\n",
    "    \"\"\"\n",
    "    return text.translate(ARABIC_TO_WESTERN_NUMS)\n",
    "\n",
    "\n",
    "# Test the function\n",
    "logger.section(\"üîß PREPROCESSING: Unify Numbers\")\n",
    "\n",
    "test_cases = [\n",
    "    \"ÿπÿßŸÖ 2024\",\n",
    "    \"ÿ±ŸÇŸÖ Ÿ°Ÿ¢Ÿ£\",\n",
    "    \"ŸÖÿ®ŸÑÿ∫ 100 ÿØŸàŸÑÿßÿ±\",\n",
    "    \"Ÿ•Ÿ†Ÿ† + 500 = Ÿ°Ÿ†Ÿ†Ÿ†\",\n",
    "]\n",
    "\n",
    "logger.info(\"Test cases (‚Üí Arabic numerals):\")\n",
    "for test in test_cases:\n",
    "    result = unify_numbers_to_arabic(test)\n",
    "    logger.info(f\"   '{test}' ‚Üí '{result}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54305c7",
   "metadata": {},
   "source": [
    "### 3.7 Unify Punctuation (Arabic Punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b7e0767",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß PREPROCESSING: Unify Punctuation\n",
      "======================================================================\n",
      "Test cases:\n",
      "   'ÿ£ŸàŸÑÿßŸã, ÿ´ÿßŸÜŸäÿßŸã, ÿ´ÿßŸÑÿ´ÿßŸã'\n",
      "   ‚Üí 'ÿ£ŸàŸÑÿßŸãÿå ÿ´ÿßŸÜŸäÿßŸãÿå ÿ´ÿßŸÑÿ´ÿßŸã'\n",
      "   'ŸáŸÑ Ÿáÿ∞ÿß ÿµÿ≠Ÿäÿ≠?'\n",
      "   ‚Üí 'ŸáŸÑ Ÿáÿ∞ÿß ÿµÿ≠Ÿäÿ≠ÿü'\n",
      "   'ŸÖŸÑÿßÿ≠ÿ∏ÿ©; Ÿáÿ∞ÿß ŸÖŸáŸÖ'\n",
      "   ‚Üí 'ŸÖŸÑÿßÿ≠ÿ∏ÿ©ÿõ Ÿáÿ∞ÿß ŸÖŸáŸÖ'\n",
      "   'ÿßŸÑŸÜÿµ Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ , Ÿà ; Ÿà ?'\n",
      "   ‚Üí 'ÿßŸÑŸÜÿµ Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿå Ÿà ÿõ Ÿà ÿü'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESSING 3.7: UNIFY PUNCTUATION TO ARABIC\n",
    "# ============================================================================\n",
    "\n",
    "def unify_punctuation_to_arabic(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert Latin punctuation to Arabic equivalents.\n",
    "    \n",
    "    Conversions:\n",
    "    - , ‚Üí ÿå  (comma)\n",
    "    - ; ‚Üí ÿõ  (semicolon)\n",
    "    - ? ‚Üí ÿü  (question mark)\n",
    "    \n",
    "    Note: Period (.), colon (:), and exclamation (!) remain unchanged\n",
    "    as they are used in both systems.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text with potential Latin punctuation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with Arabic punctuation\n",
    "    \"\"\"\n",
    "    for latin, arabic in LATIN_TO_ARABIC_PUNCT.items():\n",
    "        text = text.replace(latin, arabic)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Test the function\n",
    "logger.section(\"üîß PREPROCESSING: Unify Punctuation\")\n",
    "\n",
    "test_cases = [\n",
    "    \"ÿ£ŸàŸÑÿßŸã, ÿ´ÿßŸÜŸäÿßŸã, ÿ´ÿßŸÑÿ´ÿßŸã\",\n",
    "    \"ŸáŸÑ Ÿáÿ∞ÿß ÿµÿ≠Ÿäÿ≠?\",\n",
    "    \"ŸÖŸÑÿßÿ≠ÿ∏ÿ©; Ÿáÿ∞ÿß ŸÖŸáŸÖ\",\n",
    "    \"ÿßŸÑŸÜÿµ Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ , Ÿà ; Ÿà ?\",\n",
    "]\n",
    "\n",
    "logger.info(\"Test cases:\")\n",
    "for test in test_cases:\n",
    "    result = unify_punctuation_to_arabic(test)\n",
    "    logger.info(f\"   '{test}'\")\n",
    "    logger.info(f\"   ‚Üí '{result}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2882301",
   "metadata": {},
   "source": [
    "### 3.8 Handle Consecutive Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c19fb21",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß PREPROCESSING: Handle Consecutive Punctuation\n",
      "======================================================================\n",
      "Test cases:\n",
      "   'ŸÖÿßÿ∞ÿßÿüÿüÿü'\n",
      "      Keep first: 'ŸÖÿßÿ∞ÿßÿü'\n",
      "      Smart:      'ŸÖÿßÿ∞ÿßÿü'\n",
      "   'Ÿáÿ∞ÿß ÿµÿ≠Ÿäÿ≠..'\n",
      "      Keep first: 'Ÿáÿ∞ÿß ÿµÿ≠Ÿäÿ≠.'\n",
      "      Smart:      'Ÿáÿ∞ÿß ÿµÿ≠Ÿäÿ≠.'\n",
      "   'ÿ£ŸàŸÑÿßŸãÿåÿå'\n",
      "      Keep first: 'ÿ£ŸàŸÑÿßŸãÿå'\n",
      "      Smart:      'ÿ£ŸàŸÑÿßŸãÿå'\n",
      "   'ÿßŸÜÿ™ŸáŸâ.ÿü'\n",
      "      Keep first: 'ÿßŸÜÿ™ŸáŸâ.'\n",
      "      Smart:      'ÿßŸÜÿ™ŸáŸâ.'\n",
      "   'ŸÜŸáÿßŸäÿ© ÿßŸÑŸÜÿµ.,'\n",
      "      Keep first: 'ŸÜŸáÿßŸäÿ© ÿßŸÑŸÜÿµ.'\n",
      "      Smart:      'ŸÜŸáÿßŸäÿ© ÿßŸÑŸÜÿµ.'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESSING 3.8: HANDLE CONSECUTIVE PUNCTUATION\n",
    "# ============================================================================\n",
    "\n",
    "def handle_consecutive_punctuation(text: str, keep_first: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Handle consecutive punctuation marks.\n",
    "    \n",
    "    Strategies:\n",
    "    - keep_first: Keep the first punctuation, remove rest\n",
    "    - keep_last: Keep the last punctuation, remove rest\n",
    "    - keep_strongest: Keep the \"strongest\" (. > ? > ! > ; > , > :)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "    keep_first : bool\n",
    "        If True, keep first punctuation in sequence\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with consecutive punctuation handled\n",
    "    \"\"\"\n",
    "    # Pattern matches 2+ punctuation marks in sequence\n",
    "    punct_chars = r'ÿåÿõÿü.,:;?!'\n",
    "    pattern = re.compile(f'([{punct_chars}])([{punct_chars}]+)')\n",
    "    \n",
    "    if keep_first:\n",
    "        # Keep first, remove subsequent\n",
    "        return pattern.sub(r'\\1', text)\n",
    "    else:\n",
    "        # Keep last\n",
    "        def keep_last_match(m):\n",
    "            return m.group(0)[-1]\n",
    "        return pattern.sub(keep_last_match, text)\n",
    "\n",
    "\n",
    "def handle_consecutive_punctuation_smart(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Smart handling of consecutive punctuation.\n",
    "    \n",
    "    Uses priority: . > ÿü > ! > ÿõ > ÿå > :\n",
    "    Keeps the highest priority punctuation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with consecutive punctuation reduced\n",
    "    \"\"\"\n",
    "    # Priority order (highest first)\n",
    "    priority = {'.': 6, 'ÿü': 5, '?': 5, '!': 4, 'ÿõ': 3, ';': 3, 'ÿå': 2, ',': 2, ':': 1}\n",
    "    \n",
    "    punct_chars = r'ÿåÿõÿü.,:;?!'\n",
    "    pattern = re.compile(f'[{punct_chars}]{{2,}}')\n",
    "    \n",
    "    def replace_func(match):\n",
    "        sequence = match.group(0)\n",
    "        # Find highest priority punctuation\n",
    "        best_char = sequence[0]\n",
    "        best_priority = priority.get(best_char, 0)\n",
    "        \n",
    "        for char in sequence:\n",
    "            char_priority = priority.get(char, 0)\n",
    "            if char_priority > best_priority:\n",
    "                best_char = char\n",
    "                best_priority = char_priority\n",
    "        \n",
    "        return best_char\n",
    "    \n",
    "    return pattern.sub(replace_func, text)\n",
    "\n",
    "\n",
    "# Test the function\n",
    "logger.section(\"üîß PREPROCESSING: Handle Consecutive Punctuation\")\n",
    "\n",
    "test_cases = [\n",
    "    \"ŸÖÿßÿ∞ÿßÿüÿüÿü\",\n",
    "    \"Ÿáÿ∞ÿß ÿµÿ≠Ÿäÿ≠..\",\n",
    "    \"ÿ£ŸàŸÑÿßŸãÿåÿå\",\n",
    "    \"ÿßŸÜÿ™ŸáŸâ.ÿü\",\n",
    "    \"ŸÜŸáÿßŸäÿ© ÿßŸÑŸÜÿµ.,\",\n",
    "]\n",
    "\n",
    "logger.info(\"Test cases:\")\n",
    "for test in test_cases:\n",
    "    result_first = handle_consecutive_punctuation(test, keep_first=True)\n",
    "    result_smart = handle_consecutive_punctuation_smart(test)\n",
    "    logger.info(f\"   '{test}'\")\n",
    "    logger.info(f\"      Keep first: '{result_first}'\")\n",
    "    logger.info(f\"      Smart:      '{result_smart}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a206e0a",
   "metadata": {},
   "source": [
    "### 3.9 Normalize Whitespace and Punctuation Spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54dd3231",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß PREPROCESSING: Normalize Whitespace & Punctuation Spacing\n",
      "======================================================================\n",
      "Test cases:\n",
      "   'ÿßŸÑŸÜÿµ   ŸÖÿπ   ŸÖÿ≥ÿßŸÅÿßÿ™    ŸÉÿ´Ÿäÿ±ÿ©'\n",
      "      After whitespace norm: 'ÿßŸÑŸÜÿµ ŸÖÿπ ŸÖÿ≥ÿßŸÅÿßÿ™ ŸÉÿ´Ÿäÿ±ÿ©'\n",
      "      After punct spacing:   'ÿßŸÑŸÜÿµ ŸÖÿπ ŸÖÿ≥ÿßŸÅÿßÿ™ ŸÉÿ´Ÿäÿ±ÿ©'\n",
      "   'ŸÉŸÑŸÖÿ©ÿåŸÉŸÑŸÖÿ©'\n",
      "      After whitespace norm: 'ŸÉŸÑŸÖÿ©ÿåŸÉŸÑŸÖÿ©'\n",
      "      After punct spacing:   'ŸÉŸÑŸÖÿ©ÿå ŸÉŸÑŸÖÿ©'\n",
      "   'ŸÜÿµ .ŸÖÿπ ŸÖÿ≥ÿßŸÅÿ© ŸÇÿ®ŸÑ ÿßŸÑŸÜŸÇÿ∑ÿ©'\n",
      "      After whitespace norm: 'ŸÜÿµ .ŸÖÿπ ŸÖÿ≥ÿßŸÅÿ© ŸÇÿ®ŸÑ ÿßŸÑŸÜŸÇÿ∑ÿ©'\n",
      "      After punct spacing:   'ŸÜÿµ. ŸÖÿπ ŸÖÿ≥ÿßŸÅÿ© ŸÇÿ®ŸÑ ÿßŸÑŸÜŸÇÿ∑ÿ©'\n",
      "   'ÿ≥ÿ§ÿßŸÑÿüÿ¨Ÿàÿßÿ®'\n",
      "      After whitespace norm: 'ÿ≥ÿ§ÿßŸÑÿüÿ¨Ÿàÿßÿ®'\n",
      "      After punct spacing:   'ÿ≥ÿ§ÿßŸÑÿü ÿ¨Ÿàÿßÿ®'\n",
      "   'ÿ£ŸàŸÑÿßŸã ÿå ÿ´ÿßŸÜŸäÿßŸã ÿå ÿ´ÿßŸÑÿ´ÿßŸã'\n",
      "      After whitespace norm: 'ÿ£ŸàŸÑÿßŸã ÿå ÿ´ÿßŸÜŸäÿßŸã ÿå ÿ´ÿßŸÑÿ´ÿßŸã'\n",
      "      After punct spacing:   'ÿ£ŸàŸÑÿßŸãÿå ÿ´ÿßŸÜŸäÿßŸãÿå ÿ´ÿßŸÑÿ´ÿßŸã'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESSING 3.9: NORMALIZE WHITESPACE AND PUNCTUATION SPACING\n",
    "# ============================================================================\n",
    "\n",
    "def normalize_whitespace(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize whitespace in text.\n",
    "    \n",
    "    - Replace multiple spaces with single space\n",
    "    - Remove leading/trailing whitespace\n",
    "    - Replace tabs with spaces\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with normalized whitespace\n",
    "    \"\"\"\n",
    "    # Replace tabs with spaces\n",
    "    text = text.replace('\\t', ' ')\n",
    "    \n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def add_punctuation_spacing(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensure proper spacing around punctuation marks.\n",
    "    \n",
    "    Rules:\n",
    "    - Space after punctuation (if followed by letter/number)\n",
    "    - No space before punctuation\n",
    "    - Handle Arabic RTL properly\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with proper punctuation spacing\n",
    "    \"\"\"\n",
    "    punct_marks = 'ÿåÿõÿü.:!'\n",
    "    \n",
    "    # Remove space before punctuation\n",
    "    for p in punct_marks:\n",
    "        text = re.sub(rf'\\s+{re.escape(p)}', p, text)\n",
    "    \n",
    "    # Add space after punctuation if followed by Arabic letter or number\n",
    "    for p in punct_marks:\n",
    "        # After punct, if followed by Arabic char without space, add space\n",
    "        text = re.sub(\n",
    "            rf'{re.escape(p)}([\\u0600-\\u06FFŸ†-Ÿ©])',\n",
    "            rf'{p} \\1',\n",
    "            text\n",
    "        )\n",
    "    \n",
    "    # Clean up multiple spaces that might have been created\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Test the functions\n",
    "logger.section(\"üîß PREPROCESSING: Normalize Whitespace & Punctuation Spacing\")\n",
    "\n",
    "test_cases = [\n",
    "    \"ÿßŸÑŸÜÿµ   ŸÖÿπ   ŸÖÿ≥ÿßŸÅÿßÿ™    ŸÉÿ´Ÿäÿ±ÿ©\",\n",
    "    \"ŸÉŸÑŸÖÿ©ÿåŸÉŸÑŸÖÿ©\",\n",
    "    \"ŸÜÿµ .ŸÖÿπ ŸÖÿ≥ÿßŸÅÿ© ŸÇÿ®ŸÑ ÿßŸÑŸÜŸÇÿ∑ÿ©\",\n",
    "    \"ÿ≥ÿ§ÿßŸÑÿüÿ¨Ÿàÿßÿ®\",\n",
    "    \"ÿ£ŸàŸÑÿßŸã ÿå ÿ´ÿßŸÜŸäÿßŸã ÿå ÿ´ÿßŸÑÿ´ÿßŸã\",\n",
    "]\n",
    "\n",
    "logger.info(\"Test cases:\")\n",
    "for test in test_cases:\n",
    "    result_ws = normalize_whitespace(test)\n",
    "    result_spacing = add_punctuation_spacing(result_ws)\n",
    "    logger.info(f\"   '{test}'\")\n",
    "    logger.info(f\"      After whitespace norm: '{result_ws}'\")\n",
    "    logger.info(f\"      After punct spacing:   '{result_spacing}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eaf7bd",
   "metadata": {},
   "source": [
    "### 3.10 Remove Empty and Very Short Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3810da5e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß PREPROCESSING: Filter Short/Empty Sentences\n",
      "======================================================================\n",
      "Minimum words required: 3\n",
      "Test cases:\n",
      "   '' ‚Üí ‚ùå Invalid\n",
      "   'ŸÜÿπŸÖ' ‚Üí ‚ùå Invalid\n",
      "   'ÿ£' ‚Üí ‚ùå Invalid\n",
      "   'ŸÜÿπŸÖ ŸÑÿß' ‚Üí ‚ùå Invalid\n",
      "   'Ÿáÿ∞ÿß ŸÜÿµ ÿµÿ≠Ÿäÿ≠ ŸàŸÖŸÇÿ®ŸàŸÑ' ‚Üí ‚úÖ Valid\n",
      "   '   ' ‚Üí ‚ùå Invalid\n",
      "   '1.' ‚Üí ‚ùå Invalid\n",
      "   'ŸÜÿµ ŸÇÿµŸäÿ± ÿ¨ÿØÿß' ‚Üí ‚úÖ Valid\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESSING 3.10: REMOVE EMPTY AND VERY SHORT LINES\n",
    "# ============================================================================\n",
    "\n",
    "def is_valid_sentence(text: str, min_words: int = 3) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a sentence meets minimum requirements.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text (should be preprocessed)\n",
    "    min_words : int\n",
    "        Minimum number of words required\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if sentence is valid\n",
    "    \"\"\"\n",
    "    # Strip whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Check for empty\n",
    "    if not text:\n",
    "        return False\n",
    "    \n",
    "    # Count Arabic words only\n",
    "    arabic_words = re.findall(r'[\\u0600-\\u06FF]+', text)\n",
    "    \n",
    "    return len(arabic_words) >= min_words\n",
    "\n",
    "\n",
    "def filter_sentence(text: str, min_words: int = 3) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Filter and return sentence if valid, None otherwise.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "    min_words : int\n",
    "        Minimum number of words\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Optional[str]\n",
    "        Sentence if valid, None otherwise\n",
    "    \"\"\"\n",
    "    if is_valid_sentence(text, min_words):\n",
    "        return text\n",
    "    return None\n",
    "\n",
    "\n",
    "# Test the function\n",
    "logger.section(\"üîß PREPROCESSING: Filter Short/Empty Sentences\")\n",
    "\n",
    "test_cases = [\n",
    "    \"\",\n",
    "    \"ŸÜÿπŸÖ\",\n",
    "    \"ÿ£\",\n",
    "    \"ŸÜÿπŸÖ ŸÑÿß\",\n",
    "    \"Ÿáÿ∞ÿß ŸÜÿµ ÿµÿ≠Ÿäÿ≠ ŸàŸÖŸÇÿ®ŸàŸÑ\",\n",
    "    \"   \",\n",
    "    \"1.\",\n",
    "    \"ŸÜÿµ ŸÇÿµŸäÿ± ÿ¨ÿØÿß\",\n",
    "]\n",
    "\n",
    "logger.info(f\"Minimum words required: {config.min_words}\")\n",
    "logger.info(\"Test cases:\")\n",
    "for test in test_cases:\n",
    "    is_valid = is_valid_sentence(test, min_words=config.min_words)\n",
    "    status = \"‚úÖ Valid\" if is_valid else \"‚ùå Invalid\"\n",
    "    logger.info(f\"   '{test}' ‚Üí {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a78e4c6",
   "metadata": {},
   "source": [
    "### 3.11 Process Long Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f12922f2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß PREPROCESSING: Handle Long Sentences\n",
      "======================================================================\n",
      "Original length: 150 words\n",
      "After truncation: 100 words\n",
      "Ends with: 'ŸÉŸÑŸÖÿ© ŸÉŸÑŸÖÿ©.'\n",
      "\n",
      "Split into 10 segments:\n",
      "   Segment 1: 26 words\n",
      "   Segment 2: 26 words\n",
      "   Segment 3: 26 words\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESSING 3.11: PROCESS LONG SENTENCES\n",
    "# ============================================================================\n",
    "\n",
    "def truncate_sentence(text: str, max_words: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Truncate sentence to maximum word count.\n",
    "    \n",
    "    Strategy: Truncate at word boundary, try to end at punctuation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "    max_words : int\n",
    "        Maximum number of words\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Truncated text\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    \n",
    "    if len(words) <= max_words:\n",
    "        return text\n",
    "    \n",
    "    # Truncate to max_words\n",
    "    truncated_words = words[:max_words]\n",
    "    truncated = ' '.join(truncated_words)\n",
    "    \n",
    "    # Ensure ends with punctuation\n",
    "    if truncated and truncated[-1] not in SENTENCE_TERMINALS:\n",
    "        truncated += '.'\n",
    "    \n",
    "    return truncated\n",
    "\n",
    "\n",
    "def split_long_sentence(text: str, max_words: int = 100) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split long sentence into multiple sentences at natural boundaries.\n",
    "    \n",
    "    Strategy: Split at punctuation marks (ÿåÿõ) that create natural breaks.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "    max_words : int\n",
    "        Maximum words per segment\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[str]\n",
    "        List of sentence segments\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    \n",
    "    if len(words) <= max_words:\n",
    "        return [text]\n",
    "    \n",
    "    # Find potential split points (after ÿå or ÿõ)\n",
    "    segments = []\n",
    "    current_segment = []\n",
    "    word_count = 0\n",
    "    \n",
    "    for word in words:\n",
    "        current_segment.append(word)\n",
    "        word_count += 1\n",
    "        \n",
    "        # Check if this word ends with comma/semicolon and we're past halfway\n",
    "        if word_count >= max_words // 2:\n",
    "            if word.endswith('ÿå') or word.endswith('ÿõ'):\n",
    "                # Create segment\n",
    "                segment_text = ' '.join(current_segment)\n",
    "                segments.append(segment_text)\n",
    "                current_segment = []\n",
    "                word_count = 0\n",
    "        \n",
    "        # Force split if we hit max\n",
    "        if word_count >= max_words:\n",
    "            segment_text = ' '.join(current_segment)\n",
    "            if not segment_text.endswith(('.', 'ÿü', '!')):\n",
    "                segment_text += '.'\n",
    "            segments.append(segment_text)\n",
    "            current_segment = []\n",
    "            word_count = 0\n",
    "    \n",
    "    # Add remaining\n",
    "    if current_segment:\n",
    "        segment_text = ' '.join(current_segment)\n",
    "        segments.append(segment_text)\n",
    "    \n",
    "    return segments\n",
    "\n",
    "\n",
    "# Test the functions\n",
    "logger.section(\"üîß PREPROCESSING: Handle Long Sentences\")\n",
    "\n",
    "# Create a long test sentence\n",
    "long_sentence = \" \".join([\"ŸÉŸÑŸÖÿ©\"] * 150)\n",
    "logger.info(f\"Original length: {len(long_sentence.split())} words\")\n",
    "\n",
    "truncated = truncate_sentence(long_sentence, max_words=100)\n",
    "logger.info(f\"After truncation: {len(truncated.split())} words\")\n",
    "logger.info(f\"Ends with: '{truncated[-10:]}'\")\n",
    "\n",
    "# Test with natural breaks\n",
    "long_with_punct = \"Ÿáÿ∞ÿß ŸÜÿµ ÿ∑ŸàŸäŸÑ Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ŸÅŸÇÿ±ÿßÿ™ ŸÖÿ™ÿπÿØÿØÿ©ÿå ŸàŸÉŸÑ ŸÅŸÇÿ±ÿ© ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÖŸáŸÖÿ©ÿå \" * 20\n",
    "segments = split_long_sentence(long_with_punct, max_words=50)\n",
    "logger.info(f\"\\nSplit into {len(segments)} segments:\")\n",
    "for i, seg in enumerate(segments[:3]):\n",
    "    logger.info(f\"   Segment {i+1}: {len(seg.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bca2c1",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Part 3: Optional Preprocessing Steps\n",
    "\n",
    "These preprocessing steps are **optional** and can be toggled for experimentation.\n",
    "They may improve or degrade model performance depending on the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb84cc38",
   "metadata": {},
   "source": [
    "### 4.1 Separate Waw Conjunction from Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca079647",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß OPTIONAL: Separate Waw Conjunction\n",
      "======================================================================\n",
      "Test cases:\n",
      "   'ŸàŸÇÿßŸÑ ÿßŸÑÿ±ÿ¨ŸÑ'\n",
      "   ‚Üí 'Ÿà ŸÇÿßŸÑ ÿßŸÑÿ±ÿ¨ŸÑ'\n",
      "   'ŸàÿßŸÑÿ£ŸÖŸÖ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ©'\n",
      "   ‚Üí 'Ÿà ÿßŸÑÿ£ŸÖŸÖ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ©'\n",
      "   'ŸàŸÅŸä Ÿáÿ∞ÿß ÿßŸÑÿµÿØÿØ'\n",
      "   ‚Üí 'ŸàŸÅŸä Ÿáÿ∞ÿß ÿßŸÑÿµÿØÿØ'\n",
      "   'ŸàŸÇÿ™ ÿßŸÑÿßÿ¨ÿ™ŸÖÿßÿπ'\n",
      "   ‚Üí 'ŸàŸÇÿ™ ÿßŸÑÿßÿ¨ÿ™ŸÖÿßÿπ'\n",
      "   'Ÿàÿ´ŸäŸÇÿ© ŸÖŸáŸÖÿ©'\n",
      "   ‚Üí 'Ÿàÿ´ŸäŸÇÿ© ŸÖŸáŸÖÿ©'\n",
      "   'ŸàÿßŸÑÿ™ŸÜŸÖŸäÿ© ÿßŸÑŸÖÿ≥ÿ™ÿØÿßŸÖÿ©'\n",
      "   ‚Üí 'Ÿà ÿßŸÑÿ™ŸÜŸÖŸäÿ© ÿßŸÑŸÖÿ≥ÿ™ÿØÿßŸÖÿ©'\n",
      "   'ŸàŸáŸà ŸäÿπŸÖŸÑ'\n",
      "   ‚Üí 'ŸàŸáŸà ŸäÿπŸÖŸÑ'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL 4.1: SEPARATE WAW CONJUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def separate_waw_conjunction(text: str, min_remaining_length: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Separate the conjunction Ÿà (waw) from the beginning of words.\n",
    "    \n",
    "    In Arabic, waw is often attached to the following word as a prefix\n",
    "    meaning \"and\". Separating it can help with:\n",
    "    - Better tokenization\n",
    "    - Consistent word boundaries\n",
    "    - Improved punctuation prediction before conjunctions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "    min_remaining_length : int\n",
    "        Only separate if remaining word has this many chars\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with separated waw conjunctions\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    >>> separate_waw_conjunction(\"ŸàŸÇÿßŸÑ ÿßŸÑÿ±ÿ¨ŸÑ Ÿàÿ∞Ÿáÿ®\")\n",
    "    'Ÿà ŸÇÿßŸÑ ÿßŸÑÿ±ÿ¨ŸÑ Ÿà ÿ∞Ÿáÿ®'\n",
    "    \"\"\"\n",
    "    # Words where waw is part of the root (should NOT be separated)\n",
    "    waw_root_words = {\n",
    "        'ŸàŸÇÿ™', 'Ÿàÿ¨Ÿá', 'Ÿàÿ∂ÿπ', 'ŸàÿµŸÑ', 'ŸàŸÇÿπ', 'Ÿàÿ≤ŸÜ', 'ŸàŸÅÿØ', 'Ÿàÿ±ŸÇ', 'Ÿàÿ∑ŸÜ',\n",
    "        'Ÿàÿ≥ÿ∑', 'Ÿàÿ≠ÿØÿ©', 'Ÿàÿ≤Ÿäÿ±', 'Ÿàÿ≤ÿßÿ±ÿ©', 'ŸàŸÑÿßŸäÿ©', 'ŸàŸÑÿØ', 'ŸàÿßŸÑÿØ', 'ŸàÿßŸÑÿØÿ©',\n",
    "        'Ÿàÿ´ŸäŸÇÿ©', 'Ÿàÿ´ÿßÿ¶ŸÇ', 'ŸàÿßŸÇÿπ', 'Ÿàÿßÿ¨ÿ®', 'ŸàŸÅÿßÿ©', 'ŸàŸÉÿßŸÑÿ©', 'ŸàŸÉŸäŸÑ',\n",
    "        'Ÿàÿßÿ≠ÿØ', 'Ÿàÿßÿ≠ÿØÿ©', 'Ÿàÿ≥ŸäŸÑÿ©', 'Ÿàÿ≥ÿßÿ¶ŸÑ', 'Ÿàÿ±ÿ¥ÿ©', 'Ÿàÿ∏ŸäŸÅÿ©', 'Ÿàÿ∏ÿßÿ¶ŸÅ',\n",
    "    }\n",
    "    \n",
    "    def should_separate(word: str) -> bool:\n",
    "        \"\"\"Check if waw should be separated from this word.\"\"\"\n",
    "        if not word.startswith('Ÿà'):\n",
    "            return False\n",
    "        \n",
    "        if len(word) < min_remaining_length + 1:  # +1 for the waw\n",
    "            return False\n",
    "        \n",
    "        # Check if word (with waw) is a root word\n",
    "        if word in waw_root_words:\n",
    "            return False\n",
    "        \n",
    "        # Check if word without waw exists as valid word\n",
    "        # (This is a heuristic - ideally use a dictionary)\n",
    "        remaining = word[1:]\n",
    "        \n",
    "        # Common prefixes that indicate waw is conjunction\n",
    "        conjunction_indicators = [\n",
    "            'ÿßŸÑ',   # Ÿà + ÿßŸÑ (definite article)\n",
    "            'ŸáŸà', 'ŸáŸä', 'ŸáŸÖ',  # pronouns\n",
    "            'ŸÇÿØ', 'ŸÑŸÖ', 'ŸÑŸÜ',  # particles\n",
    "            'ŸÉÿßŸÜ', 'ŸäŸÉŸàŸÜ',     # verbs\n",
    "            'ÿ£ŸÜ', 'ÿ•ŸÜ',        # particles\n",
    "        ]\n",
    "        \n",
    "        for indicator in conjunction_indicators:\n",
    "            if remaining.startswith(indicator):\n",
    "                return True\n",
    "        \n",
    "        # If remaining word starts with definite article, likely conjunction\n",
    "        if remaining.startswith('ÿßŸÑ'):\n",
    "            return True\n",
    "        \n",
    "        return len(remaining) >= min_remaining_length\n",
    "    \n",
    "    words = text.split()\n",
    "    result = []\n",
    "    \n",
    "    for word in words:\n",
    "        if should_separate(word):\n",
    "            result.append('Ÿà')\n",
    "            result.append(word[1:])\n",
    "        else:\n",
    "            result.append(word)\n",
    "    \n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "# Test the function\n",
    "logger.section(\"üîß OPTIONAL: Separate Waw Conjunction\")\n",
    "\n",
    "test_cases = [\n",
    "    \"ŸàŸÇÿßŸÑ ÿßŸÑÿ±ÿ¨ŸÑ\",\n",
    "    \"ŸàÿßŸÑÿ£ŸÖŸÖ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ©\",\n",
    "    \"ŸàŸÅŸä Ÿáÿ∞ÿß ÿßŸÑÿµÿØÿØ\",\n",
    "    \"ŸàŸÇÿ™ ÿßŸÑÿßÿ¨ÿ™ŸÖÿßÿπ\",  # Should NOT separate (ŸàŸÇÿ™ is a root word)\n",
    "    \"Ÿàÿ´ŸäŸÇÿ© ŸÖŸáŸÖÿ©\",   # Should NOT separate\n",
    "    \"ŸàÿßŸÑÿ™ŸÜŸÖŸäÿ© ÿßŸÑŸÖÿ≥ÿ™ÿØÿßŸÖÿ©\",\n",
    "    \"ŸàŸáŸà ŸäÿπŸÖŸÑ\",\n",
    "]\n",
    "\n",
    "logger.info(\"Test cases:\")\n",
    "for test in test_cases:\n",
    "    result = separate_waw_conjunction(test)\n",
    "    logger.info(f\"   '{test}'\")\n",
    "    logger.info(f\"   ‚Üí '{result}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75d83c5",
   "metadata": {},
   "source": [
    "### 4.2 Stopword Handling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "849bb99f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß OPTIONAL: Stopword Handling\n",
      "======================================================================\n",
      "Test cases:\n",
      "   Original: 'ŸàŸÅŸä Ÿáÿ∞ÿß ÿßŸÑÿµÿØÿØ'\n",
      "   Marked:   'ŸàŸÅŸä<SW> Ÿáÿ∞ÿß<SW> ÿßŸÑÿµÿØÿØ'\n",
      "   Removed:  'ÿßŸÑÿµÿØÿØ'\n",
      "   Original: 'ŸÖŸÜ ÿ£ÿ¨ŸÑ ÿßŸÑÿ™ŸÜŸÖŸäÿ©'\n",
      "   Marked:   'ŸÖŸÜ<SW> ÿ£ÿ¨ŸÑ ÿßŸÑÿ™ŸÜŸÖŸäÿ©'\n",
      "   Removed:  'ÿ£ÿ¨ŸÑ ÿßŸÑÿ™ŸÜŸÖŸäÿ©'\n",
      "   Original: 'ÿßŸÑÿ£ŸÖŸÖ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ© ŸáŸä ŸÖŸÜÿ∏ŸÖÿ© ÿØŸàŸÑŸäÿ©'\n",
      "   Marked:   'ÿßŸÑÿ£ŸÖŸÖ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ© ŸáŸä<SW> ŸÖŸÜÿ∏ŸÖÿ© ÿØŸàŸÑŸäÿ©'\n",
      "   Removed:  'ÿßŸÑÿ£ŸÖŸÖ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ© ŸÖŸÜÿ∏ŸÖÿ© ÿØŸàŸÑŸäÿ©'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL 4.2: STOPWORD HANDLING\n",
    "# ============================================================================\n",
    "\n",
    "def mark_stopwords(text: str, marker: str = '<SW>') -> str:\n",
    "    \"\"\"\n",
    "    Mark stopwords with a special token (for analysis/experimentation).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "    marker : str\n",
    "        Marker to add after stopwords\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with marked stopwords\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    result = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Remove punctuation for checking\n",
    "        clean_word = re.sub(r'[ÿåÿõÿü.:!]', '', word)\n",
    "        \n",
    "        if clean_word in ARABIC_STOPWORDS:\n",
    "            result.append(word + marker)\n",
    "        else:\n",
    "            result.append(word)\n",
    "    \n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "def remove_stopwords(text: str, keep_structure: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Remove stopwords from text.\n",
    "    \n",
    "    WARNING: This may hurt punctuation prediction as stopwords\n",
    "    provide important structural context.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "    keep_structure : bool\n",
    "        If True, keep punctuation even if attached to stopwords\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text without stopwords\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    result = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Separate word from trailing punctuation\n",
    "        punct = ''\n",
    "        clean_word = word\n",
    "        \n",
    "        if word and word[-1] in 'ÿåÿõÿü.:!':\n",
    "            punct = word[-1]\n",
    "            clean_word = word[:-1]\n",
    "        \n",
    "        if clean_word not in ARABIC_STOPWORDS:\n",
    "            result.append(word)\n",
    "        elif keep_structure and punct:\n",
    "            # Keep punctuation even if word is stopword\n",
    "            if result:\n",
    "                result[-1] += punct\n",
    "    \n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "# Test the function\n",
    "logger.section(\"üîß OPTIONAL: Stopword Handling\")\n",
    "\n",
    "test_cases = [\n",
    "    \"ŸàŸÅŸä Ÿáÿ∞ÿß ÿßŸÑÿµÿØÿØ\",\n",
    "    \"ŸÖŸÜ ÿ£ÿ¨ŸÑ ÿßŸÑÿ™ŸÜŸÖŸäÿ©\",\n",
    "    \"ÿßŸÑÿ£ŸÖŸÖ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ© ŸáŸä ŸÖŸÜÿ∏ŸÖÿ© ÿØŸàŸÑŸäÿ©\",\n",
    "]\n",
    "\n",
    "logger.info(\"Test cases:\")\n",
    "for test in test_cases:\n",
    "    marked = mark_stopwords(test)\n",
    "    removed = remove_stopwords(test)\n",
    "    logger.info(f\"   Original: '{test}'\")\n",
    "    logger.info(f\"   Marked:   '{marked}'\")\n",
    "    logger.info(f\"   Removed:  '{removed}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be3a8d",
   "metadata": {},
   "source": [
    "### 4.3 Number Token Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55f9ff6b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß OPTIONAL: Number Token Replacement\n",
      "======================================================================\n",
      "Test cases:\n",
      "   'ŸÅŸä ÿπÿßŸÖ Ÿ¢Ÿ†Ÿ¢Ÿ§'\n",
      "   ‚Üí 'ŸÅŸä ÿπÿßŸÖ <NUM>'\n",
      "   'ŸÖÿ®ŸÑÿ∫ Ÿ°Ÿ†Ÿ†Ÿ†Ÿ†Ÿ† ÿØŸàŸÑÿßÿ±'\n",
      "   ‚Üí 'ŸÖÿ®ŸÑÿ∫ <NUM> ÿØŸàŸÑÿßÿ±'\n",
      "   'ŸÖŸÜ Ÿ• ÿ•ŸÑŸâ Ÿ°Ÿ† ÿ≥ŸÜŸàÿßÿ™'\n",
      "   ‚Üí 'ŸÖŸÜ <NUM> ÿ•ŸÑŸâ <NUM> ÿ≥ŸÜŸàÿßÿ™'\n",
      "   'ÿßŸÑŸÇÿ±ÿßÿ± ÿ±ŸÇŸÖ Ÿ§Ÿß/Ÿ°Ÿ¢Ÿ£'\n",
      "   ‚Üí 'ÿßŸÑŸÇÿ±ÿßÿ± ÿ±ŸÇŸÖ <NUM>/<NUM>'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL 4.3: NUMBER TOKEN REPLACEMENT\n",
    "# ============================================================================\n",
    "\n",
    "def replace_numbers_with_token(text: str, token: str = '<NUM>') -> str:\n",
    "    \"\"\"\n",
    "    Replace all numbers with a special token.\n",
    "    \n",
    "    This can help:\n",
    "    - Reduce vocabulary size\n",
    "    - Focus model on structure rather than specific numbers\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "    token : str\n",
    "        Token to replace numbers with\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with numbers replaced\n",
    "    \"\"\"\n",
    "    # Pattern for Arabic or Western numerals\n",
    "    number_pattern = re.compile(r'[Ÿ†-Ÿ©0-9]+')\n",
    "    return number_pattern.sub(token, text)\n",
    "\n",
    "\n",
    "def normalize_number_format(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize number formatting (e.g., thousands separators).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with normalized number formats\n",
    "    \"\"\"\n",
    "    # Remove thousands separators (both , and Ÿ¨)\n",
    "    text = re.sub(r'(\\d),(\\d)', r'\\1\\2', text)\n",
    "    text = re.sub(r'([\\u0660-\\u0669])Ÿ¨([\\u0660-\\u0669])', r'\\1\\2', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Test the function\n",
    "logger.section(\"üîß OPTIONAL: Number Token Replacement\")\n",
    "\n",
    "test_cases = [\n",
    "    \"ŸÅŸä ÿπÿßŸÖ Ÿ¢Ÿ†Ÿ¢Ÿ§\",\n",
    "    \"ŸÖÿ®ŸÑÿ∫ Ÿ°Ÿ†Ÿ†Ÿ†Ÿ†Ÿ† ÿØŸàŸÑÿßÿ±\",\n",
    "    \"ŸÖŸÜ Ÿ• ÿ•ŸÑŸâ Ÿ°Ÿ† ÿ≥ŸÜŸàÿßÿ™\",\n",
    "    \"ÿßŸÑŸÇÿ±ÿßÿ± ÿ±ŸÇŸÖ Ÿ§Ÿß/Ÿ°Ÿ¢Ÿ£\",\n",
    "]\n",
    "\n",
    "logger.info(\"Test cases:\")\n",
    "for test in test_cases:\n",
    "    result = replace_numbers_with_token(test)\n",
    "    logger.info(f\"   '{test}'\")\n",
    "    logger.info(f\"   ‚Üí '{result}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e910548",
   "metadata": {},
   "source": [
    "### 4.4 Rare Word Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c52ef09c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß OPTIONAL: Rare Word Handling\n",
      "======================================================================\n",
      "Building vocabulary is computationally expensive.\n",
      "In production, vocabulary should be built once and saved.\n",
      "\n",
      "Example usage:\n",
      "   vocab = build_vocabulary(dataset_dir, sample_size=1000000)\n",
      "   text = replace_rare_words(text, vocab, threshold=5)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL 4.4: RARE WORD HANDLING\n",
    "# ============================================================================\n",
    "\n",
    "def build_vocabulary(dataset_dir: str, sample_size: int = 1000000) -> Counter:\n",
    "    \"\"\"\n",
    "    Build vocabulary with word frequencies.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_dir : str\n",
    "        Path to dataset\n",
    "    sample_size : int\n",
    "        Number of lines to process\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Counter\n",
    "        Word frequency counter\n",
    "    \"\"\"\n",
    "    vocab = Counter()\n",
    "    arabic_word_pattern = re.compile(r'[\\u0600-\\u06FF]+')\n",
    "    \n",
    "    for i, line in enumerate(iter_dataset_lines(dataset_dir)):\n",
    "        if i >= sample_size:\n",
    "            break\n",
    "        \n",
    "        words = arabic_word_pattern.findall(line)\n",
    "        vocab.update(words)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def replace_rare_words(text: str, vocab: Counter, threshold: int = 5, \n",
    "                       token: str = '<UNK>') -> str:\n",
    "    \"\"\"\n",
    "    Replace rare words (below frequency threshold) with a special token.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "    vocab : Counter\n",
    "        Vocabulary with frequencies\n",
    "    threshold : int\n",
    "        Minimum frequency to keep word\n",
    "    token : str\n",
    "        Replacement token\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with rare words replaced\n",
    "    \"\"\"\n",
    "    arabic_word_pattern = re.compile(r'[\\u0600-\\u06FF]+')\n",
    "    \n",
    "    def replace_if_rare(match):\n",
    "        word = match.group(0)\n",
    "        if vocab.get(word, 0) < threshold:\n",
    "            return token\n",
    "        return word\n",
    "    \n",
    "    return arabic_word_pattern.sub(replace_if_rare, text)\n",
    "\n",
    "\n",
    "# Note: Building vocabulary is expensive, so we'll demonstrate with a mock\n",
    "logger.section(\"üîß OPTIONAL: Rare Word Handling\")\n",
    "\n",
    "logger.info(\"Building vocabulary is computationally expensive.\")\n",
    "logger.info(\"In production, vocabulary should be built once and saved.\")\n",
    "logger.info(\"\\nExample usage:\")\n",
    "logger.info(\"   vocab = build_vocabulary(dataset_dir, sample_size=1000000)\")\n",
    "logger.info(\"   text = replace_rare_words(text, vocab, threshold=5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be35fe9",
   "metadata": {},
   "source": [
    "### 4.5 Sentence Length Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64cdf986",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß OPTIONAL: Sentence Length Normalization\n",
      "======================================================================\n",
      "Original: 12 words\n",
      "Split into 4 chunks:\n",
      "   Chunk 1: 'Ÿáÿ∞ÿß ŸÜÿµ ÿ∑ŸàŸäŸÑ Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ'\n",
      "   Chunk 2: 'Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿßŸÑÿπÿØŸäÿØ ŸÖŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™'\n",
      "   Chunk 3: 'ŸÖŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿ™Ÿä ÿ™ÿ≠ÿ™ÿßÿ¨ ÿ•ŸÑŸâ'\n",
      "   Chunk 4: 'ÿ™ÿ≠ÿ™ÿßÿ¨ ÿ•ŸÑŸâ ŸÖÿπÿßŸÑÿ¨ÿ©'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL 4.5: SENTENCE LENGTH NORMALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def pad_sentence(text: str, target_length: int, pad_token: str = '<PAD>') -> str:\n",
    "    \"\"\"\n",
    "    Pad sentence to target length.\n",
    "    \n",
    "    Note: This is typically done during batching, not preprocessing.\n",
    "    Including here for completeness.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "    target_length : int\n",
    "        Target number of words\n",
    "    pad_token : str\n",
    "        Token to use for padding\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Padded text\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    \n",
    "    if len(words) >= target_length:\n",
    "        return text\n",
    "    \n",
    "    padding = [pad_token] * (target_length - len(words))\n",
    "    return text + ' ' + ' '.join(padding)\n",
    "\n",
    "\n",
    "def split_into_chunks(text: str, chunk_size: int = 50, overlap: int = 10) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split long text into overlapping chunks.\n",
    "    \n",
    "    Useful for very long documents where context needs to be preserved.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "    chunk_size : int\n",
    "        Target words per chunk\n",
    "    overlap : int\n",
    "        Words to overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[str]\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    \n",
    "    if len(words) <= chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(words):\n",
    "        end = min(start + chunk_size, len(words))\n",
    "        chunk_words = words[start:end]\n",
    "        chunks.append(' '.join(chunk_words))\n",
    "        \n",
    "        # Move start with overlap\n",
    "        start = end - overlap\n",
    "        if start >= len(words) - overlap:\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Test the functions\n",
    "logger.section(\"üîß OPTIONAL: Sentence Length Normalization\")\n",
    "\n",
    "test_text = \"Ÿáÿ∞ÿß ŸÜÿµ ÿ∑ŸàŸäŸÑ Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿßŸÑÿπÿØŸäÿØ ŸÖŸÜ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿ™Ÿä ÿ™ÿ≠ÿ™ÿßÿ¨ ÿ•ŸÑŸâ ŸÖÿπÿßŸÑÿ¨ÿ©\"\n",
    "logger.info(f\"Original: {len(test_text.split())} words\")\n",
    "\n",
    "chunks = split_into_chunks(test_text, chunk_size=5, overlap=2)\n",
    "logger.info(f\"Split into {len(chunks)} chunks:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    logger.info(f\"   Chunk {i+1}: '{chunk}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f678eb0",
   "metadata": {},
   "source": [
    "### 4.6 Remove/Replace Foreign Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74ab2c8c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß OPTIONAL: Handle Foreign Terms\n",
      "======================================================================\n",
      "Test cases:\n",
      "   Original: 'ÿßŸÑŸàÿ´ŸäŸÇÿ© A/47/10 ÿßŸÑŸÖÿ§ÿ±ÿÆÿ©'\n",
      "   Remove refs: 'ÿßŸÑŸàÿ´ŸäŸÇÿ©  ÿßŸÑŸÖÿ§ÿ±ÿÆÿ©'\n",
      "   Replace foreign: 'ÿßŸÑŸàÿ´ŸäŸÇÿ© A/47/10 ÿßŸÑŸÖÿ§ÿ±ÿÆÿ©'\n",
      "   Original: 'ÿ®ÿ±ŸÜÿßŸÖÿ¨ UNDP ŸÑŸÑÿ™ŸÜŸÖŸäÿ©'\n",
      "   Remove refs: 'ÿ®ÿ±ŸÜÿßŸÖÿ¨ UNDP ŸÑŸÑÿ™ŸÜŸÖŸäÿ©'\n",
      "   Replace foreign: 'ÿ®ÿ±ŸÜÿßŸÖÿ¨ <FOREIGN> ŸÑŸÑÿ™ŸÜŸÖŸäÿ©'\n",
      "   Original: 'ŸÇÿ±ÿßÿ± ŸÖÿ¨ŸÑÿ≥ ÿßŸÑÿ£ŸÖŸÜ S/RES/1234'\n",
      "   Remove refs: 'ŸÇÿ±ÿßÿ± ŸÖÿ¨ŸÑÿ≥ ÿßŸÑÿ£ŸÖŸÜ '\n",
      "   Replace foreign: 'ŸÇÿ±ÿßÿ± ŸÖÿ¨ŸÑÿ≥ ÿßŸÑÿ£ŸÖŸÜ S/<FOREIGN>/1234'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL 4.6: HANDLE FOREIGN TERMS\n",
    "# ============================================================================\n",
    "\n",
    "def remove_document_references(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove UN-style document references (e.g., A/47/10, S/RES/1234).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text without document references\n",
    "    \"\"\"\n",
    "    # Pattern for document references\n",
    "    doc_pattern = re.compile(r'[A-Z]/[A-Z0-9]+(?:/[A-Z0-9]+)*')\n",
    "    return doc_pattern.sub('', text)\n",
    "\n",
    "\n",
    "def replace_foreign_with_token(text: str, token: str = '<FOREIGN>') -> str:\n",
    "    \"\"\"\n",
    "    Replace foreign (non-Arabic) terms with a special token.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "    token : str\n",
    "        Replacement token\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with foreign terms replaced\n",
    "    \"\"\"\n",
    "    # Pattern for Latin words (3+ letters)\n",
    "    foreign_pattern = re.compile(r'\\b[A-Za-z]{3,}\\b')\n",
    "    return foreign_pattern.sub(token, text)\n",
    "\n",
    "\n",
    "# Test the functions\n",
    "logger.section(\"üîß OPTIONAL: Handle Foreign Terms\")\n",
    "\n",
    "test_cases = [\n",
    "    \"ÿßŸÑŸàÿ´ŸäŸÇÿ© A/47/10 ÿßŸÑŸÖÿ§ÿ±ÿÆÿ©\",\n",
    "    \"ÿ®ÿ±ŸÜÿßŸÖÿ¨ UNDP ŸÑŸÑÿ™ŸÜŸÖŸäÿ©\",\n",
    "    \"ŸÇÿ±ÿßÿ± ŸÖÿ¨ŸÑÿ≥ ÿßŸÑÿ£ŸÖŸÜ S/RES/1234\",\n",
    "]\n",
    "\n",
    "logger.info(\"Test cases:\")\n",
    "for test in test_cases:\n",
    "    no_refs = remove_document_references(test)\n",
    "    replaced = replace_foreign_with_token(test)\n",
    "    logger.info(f\"   Original: '{test}'\")\n",
    "    logger.info(f\"   Remove refs: '{no_refs}'\")\n",
    "    logger.info(f\"   Replace foreign: '{replaced}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4356b370",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Part 4: Complete Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a6813a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Preprocessor initialized!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: COMPLETE PREPROCESSING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class PreprocessingStats:\n",
    "    \"\"\"Statistics collected during preprocessing.\"\"\"\n",
    "    total_input_lines: int = 0\n",
    "    total_output_lines: int = 0\n",
    "    empty_lines_removed: int = 0\n",
    "    short_lines_removed: int = 0\n",
    "    long_lines_truncated: int = 0\n",
    "    diacritics_removed: int = 0\n",
    "    alef_normalized: int = 0\n",
    "    punct_normalized: int = 0\n",
    "    numbers_normalized: int = 0\n",
    "    latin_removed: int = 0\n",
    "    oov_removed: int = 0\n",
    "    consecutive_punct_fixed: int = 0\n",
    "    whitespace_fixed: int = 0\n",
    "\n",
    "\n",
    "class ArabicTextPreprocessor:\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for Arabic punctuation dataset.\n",
    "    \n",
    "    This class provides a configurable preprocessing pipeline that can be\n",
    "    used with both mandatory and optional preprocessing steps.\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    config : PreprocessingConfig\n",
    "        Configuration object containing all settings\n",
    "    stats : PreprocessingStats\n",
    "        Statistics collected during preprocessing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: PreprocessingConfig):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor with configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        config : PreprocessingConfig\n",
    "            Configuration object\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.stats = PreprocessingStats()\n",
    "        self.vocab = None  # For rare word handling\n",
    "        \n",
    "        # Compile regex patterns for efficiency\n",
    "        self._compile_patterns()\n",
    "    \n",
    "    def _compile_patterns(self):\n",
    "        \"\"\"Compile regex patterns for efficiency.\"\"\"\n",
    "        self.diacritics_pattern = re.compile(r'[\\u064B-\\u0652]')\n",
    "        self.alef_pattern = re.compile(r'[ÿ£ÿ•ÿ¢Ÿ±]')\n",
    "        self.latin_pattern = re.compile(r'[A-Za-z]+')\n",
    "        self.number_western = re.compile(r'[0-9]')\n",
    "        self.consecutive_punct = re.compile(r'[ÿåÿõÿü.,:;?!]{2,}')\n",
    "        self.multi_space = re.compile(r' +')\n",
    "        self.arabic_word = re.compile(r'[\\u0600-\\u06FF]+')\n",
    "    \n",
    "    def preprocess_line(self, text: str, apply_optional: bool = False) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Apply full preprocessing pipeline to a single line.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            Input text line\n",
    "        apply_optional : bool\n",
    "            Whether to apply optional preprocessing steps\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        Optional[str]\n",
    "            Preprocessed text, or None if line should be filtered\n",
    "        \"\"\"\n",
    "        original = text\n",
    "        \n",
    "        # Track changes for statistics\n",
    "        had_diacritics = bool(self.diacritics_pattern.search(text))\n",
    "        had_alef_var = bool(self.alef_pattern.search(text))\n",
    "        had_latin = bool(self.latin_pattern.search(text))\n",
    "        had_consec_punct = bool(self.consecutive_punct.search(text))\n",
    "        \n",
    "        # ============================================\n",
    "        # MANDATORY PREPROCESSING\n",
    "        # ============================================\n",
    "        \n",
    "        # 1. Remove diacritics\n",
    "        if self.config.remove_diacritics:\n",
    "            text = self.diacritics_pattern.sub('', text)\n",
    "            if had_diacritics:\n",
    "                self.stats.diacritics_removed += 1\n",
    "        \n",
    "        # 2. Normalize Alef\n",
    "        if self.config.normalize_alef:\n",
    "            text = self.alef_pattern.sub('ÿß', text)\n",
    "            if had_alef_var:\n",
    "                self.stats.alef_normalized += 1\n",
    "        \n",
    "        # 3. Normalize Alef Maksura (Ÿâ ‚Üí Ÿä)\n",
    "        if self.config.normalize_alef_maksura:\n",
    "            text = text.replace('Ÿâ', 'Ÿä')\n",
    "        \n",
    "        # 4. Remove Tatweel\n",
    "        if self.config.remove_tatweel:\n",
    "            text = text.replace('\\u0640', '')\n",
    "        \n",
    "        # 5. Unify punctuation to Arabic\n",
    "        if self.config.unify_punctuation_to_arabic:\n",
    "            for latin, arabic in LATIN_TO_ARABIC_PUNCT.items():\n",
    "                if latin in text:\n",
    "                    text = text.replace(latin, arabic)\n",
    "                    self.stats.punct_normalized += 1\n",
    "        \n",
    "        # 6. Unify numbers to Arabic\n",
    "        if self.config.unify_numbers_to_arabic:\n",
    "            if self.number_western.search(text):\n",
    "                text = text.translate(WESTERN_TO_ARABIC_NUMS)\n",
    "                self.stats.numbers_normalized += 1\n",
    "        \n",
    "        # 7. Remove Latin letters\n",
    "        if self.config.remove_latin_letters:\n",
    "            if had_latin:\n",
    "                text = self.latin_pattern.sub('', text)\n",
    "                self.stats.latin_removed += 1\n",
    "        \n",
    "        # 8. Remove OOV characters\n",
    "        if self.config.remove_oov_chars:\n",
    "            original_len = len(text)\n",
    "            text = remove_oov_characters(text)\n",
    "            if len(text) < original_len:\n",
    "                self.stats.oov_removed += 1\n",
    "        \n",
    "        # 9. Handle consecutive punctuation\n",
    "        if self.config.handle_consecutive_punct:\n",
    "            if had_consec_punct:\n",
    "                text = handle_consecutive_punctuation_smart(text)\n",
    "                self.stats.consecutive_punct_fixed += 1\n",
    "        \n",
    "        # 10. Normalize whitespace\n",
    "        if self.config.normalize_whitespace:\n",
    "            text = self.multi_space.sub(' ', text)\n",
    "            text = text.strip()\n",
    "            self.stats.whitespace_fixed += 1\n",
    "        \n",
    "        # 11. Add punctuation spacing\n",
    "        if self.config.add_punct_spacing:\n",
    "            text = add_punctuation_spacing(text)\n",
    "        \n",
    "        # ============================================\n",
    "        # OPTIONAL PREPROCESSING\n",
    "        # ============================================\n",
    "        \n",
    "        if apply_optional:\n",
    "            # Separate waw conjunction\n",
    "            if self.config.separate_waw_conjunction:\n",
    "                text = separate_waw_conjunction(text)\n",
    "            \n",
    "            # Replace numbers with token\n",
    "            if self.config.replace_numbers_with_token:\n",
    "                text = replace_numbers_with_token(text, '<NUM>')\n",
    "            \n",
    "            # Remove foreign terms\n",
    "            if self.config.remove_foreign_terms:\n",
    "                text = remove_document_references(text)\n",
    "        \n",
    "        # ============================================\n",
    "        # FILTERING\n",
    "        # ============================================\n",
    "        \n",
    "        # Final whitespace cleanup\n",
    "        text = self.multi_space.sub(' ', text).strip()\n",
    "        \n",
    "        # Check for empty\n",
    "        if self.config.remove_empty_lines and not text:\n",
    "            self.stats.empty_lines_removed += 1\n",
    "            return None\n",
    "        \n",
    "        # Check word count\n",
    "        word_count = len(self.arabic_word.findall(text))\n",
    "        \n",
    "        # Filter short sentences\n",
    "        if word_count < self.config.min_words:\n",
    "            self.stats.short_lines_removed += 1\n",
    "            return None\n",
    "        \n",
    "        # Handle long sentences\n",
    "        if word_count > self.config.max_words:\n",
    "            text = truncate_sentence(text, self.config.max_words)\n",
    "            self.stats.long_lines_truncated += 1\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def process_dataset(self, input_dir: str, output_file: str, \n",
    "                        apply_optional: bool = False,\n",
    "                        sample_size: Optional[int] = None) -> PreprocessingStats:\n",
    "        \"\"\"\n",
    "        Process entire dataset and save to output file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dir : str\n",
    "            Path to input dataset directory\n",
    "        output_file : str\n",
    "            Path to output file\n",
    "        apply_optional : bool\n",
    "            Whether to apply optional preprocessing\n",
    "        sample_size : Optional[int]\n",
    "            Limit processing to this many lines (None = all)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        PreprocessingStats\n",
    "            Statistics from preprocessing\n",
    "        \"\"\"\n",
    "        logger.section(\"üöÄ PROCESSING DATASET\")\n",
    "        logger.info(f\"Input: {input_dir}\")\n",
    "        logger.info(f\"Output: {output_file}\")\n",
    "        logger.info(f\"Apply optional steps: {apply_optional}\")\n",
    "        \n",
    "        # Reset statistics\n",
    "        self.stats = PreprocessingStats()\n",
    "        \n",
    "        # Count total lines for progress bar\n",
    "        if sample_size is None:\n",
    "            total_lines = count_total_lines(input_dir)\n",
    "            logger.info(f\"Total lines to process: {total_lines:,}\")\n",
    "        else:\n",
    "            total_lines = sample_size\n",
    "            logger.info(f\"Processing sample of {total_lines:,} lines\")\n",
    "        \n",
    "        # Create output directory if needed\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        \n",
    "        # Process and write\n",
    "        iterator = iter_dataset_lines(input_dir)\n",
    "        if TQDM_AVAILABLE:\n",
    "            iterator = tqdm(iterator, total=total_lines, desc=\"Preprocessing\")\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(iterator):\n",
    "                if sample_size and i >= sample_size:\n",
    "                    break\n",
    "                \n",
    "                self.stats.total_input_lines += 1\n",
    "                \n",
    "                # Preprocess line\n",
    "                processed = self.preprocess_line(line, apply_optional)\n",
    "                \n",
    "                # Write if not filtered\n",
    "                if processed:\n",
    "                    f.write(processed + '\\n')\n",
    "                    self.stats.total_output_lines += 1\n",
    "        \n",
    "        # Log statistics\n",
    "        self._log_statistics()\n",
    "        \n",
    "        return self.stats\n",
    "    \n",
    "    def _log_statistics(self):\n",
    "        \"\"\"Log preprocessing statistics.\"\"\"\n",
    "        logger.section(\"üìä PREPROCESSING STATISTICS\")\n",
    "        \n",
    "        logger.info(f\"Input lines:  {self.stats.total_input_lines:,}\")\n",
    "        logger.info(f\"Output lines: {self.stats.total_output_lines:,}\")\n",
    "        \n",
    "        kept_pct = self.stats.total_output_lines / max(self.stats.total_input_lines, 1) * 100\n",
    "        logger.info(f\"Lines kept:   {kept_pct:.2f}%\")\n",
    "        \n",
    "        logger.subsection(\"Filtering Statistics\")\n",
    "        logger.info(f\"Empty lines removed:     {self.stats.empty_lines_removed:,}\")\n",
    "        logger.info(f\"Short lines removed:     {self.stats.short_lines_removed:,}\")\n",
    "        logger.info(f\"Long lines truncated:    {self.stats.long_lines_truncated:,}\")\n",
    "        \n",
    "        logger.subsection(\"Normalization Statistics\")\n",
    "        logger.info(f\"Diacritics removed:      {self.stats.diacritics_removed:,}\")\n",
    "        logger.info(f\"Alef normalized:         {self.stats.alef_normalized:,}\")\n",
    "        logger.info(f\"Punctuation normalized:  {self.stats.punct_normalized:,}\")\n",
    "        logger.info(f\"Numbers normalized:      {self.stats.numbers_normalized:,}\")\n",
    "        logger.info(f\"Latin removed:           {self.stats.latin_removed:,}\")\n",
    "        logger.info(f\"OOV chars removed:       {self.stats.oov_removed:,}\")\n",
    "        logger.info(f\"Consec. punct fixed:     {self.stats.consecutive_punct_fixed:,}\")\n",
    "\n",
    "\n",
    "# Instantiate preprocessor\n",
    "preprocessor = ArabicTextPreprocessor(config)\n",
    "logger.success(\"Preprocessor initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad9cd3",
   "metadata": {},
   "source": [
    "### Test the Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e58e1bb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üß™ TESTING COMPLETE PIPELINE\n",
      "======================================================================\n",
      "Processing test cases:\n",
      "\n",
      "Test 1:\n",
      "   Input:  'ÿßŸÑÿ£ŸèŸÖŸéŸÖŸè ÿßŸÑŸÖŸèÿ™ŸéŸëÿ≠ŸêÿØŸéÿ© ŸÖŸèŸÜŸéÿ∏ŸéŸëŸÖŸéÿ© ÿØŸéŸàŸíŸÑŸêŸäŸéŸëÿ©.'\n",
      "   Output: 'ÿßŸÑÿßŸÖŸÖ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ© ŸÖŸÜÿ∏ŸÖÿ© ÿØŸàŸÑŸäÿ©.'\n",
      "\n",
      "Test 2:\n",
      "   Input:  'ÿ£ŸàŸÑÿßŸã, ÿ´ÿßŸÜŸäÿßŸã; ÿ´ÿßŸÑÿ´ÿßŸã?'\n",
      "   Output: 'ÿßŸàŸÑÿßÿå ÿ´ÿßŸÜŸäÿßÿõ ÿ´ÿßŸÑÿ´ÿßÿü'\n",
      "\n",
      "Test 3:\n",
      "   Input:  'ÿ£ÿ≠ŸÖÿØ Ÿàÿ•ÿ®ÿ±ÿßŸáŸäŸÖ Ÿàÿ¢ÿØŸÖ'\n",
      "   Output: 'ÿßÿ≠ŸÖÿØ Ÿàÿßÿ®ÿ±ÿßŸáŸäŸÖ ŸàÿßÿØŸÖ'\n",
      "\n",
      "Test 4:\n",
      "   Input:  'ŸÅŸä ÿπÿßŸÖ 2024 ŸàÿµŸÑ ÿßŸÑÿπÿØÿØ ÿ•ŸÑŸâ 100'\n",
      "   Output: 'ŸÅŸä ÿπÿßŸÖ Ÿ¢Ÿ†Ÿ¢Ÿ§ ŸàÿµŸÑ ÿßŸÑÿπÿØÿØ ÿßŸÑŸä Ÿ°Ÿ†Ÿ†'\n",
      "\n",
      "Test 5:\n",
      "   Input:  'ÿßŸÑŸàÿ´ŸäŸÇÿ© A/47/10 ŸàÿßŸÑŸÇÿ±ÿßÿ± UNDP/2024'\n",
      "   Output: 'ÿßŸÑŸàÿ´ŸäŸÇÿ© Ÿ§ŸßŸ°Ÿ† ŸàÿßŸÑŸÇÿ±ÿßÿ± Ÿ¢Ÿ†Ÿ¢Ÿ§'\n",
      "\n",
      "Test 6:\n",
      "   Input:  'ŸÖÿßÿ∞ÿßÿüÿüÿü Ÿáÿ∞ÿß ÿµÿ≠Ÿäÿ≠...'\n",
      "   Output: 'ŸÖÿßÿ∞ÿßÿü Ÿáÿ∞ÿß ÿµÿ≠Ÿäÿ≠.'\n",
      "\n",
      "Test 7:\n",
      "   Input:  'ÿßŸÑŸÜÿµ   ŸÖÿπ    ŸÖÿ≥ÿßŸÅÿßÿ™   ŸÉÿ´Ÿäÿ±ÿ©'\n",
      "   Output: 'ÿßŸÑŸÜÿµ ŸÖÿπ ŸÖÿ≥ÿßŸÅÿßÿ™ ŸÉÿ´Ÿäÿ±ÿ©'\n",
      "\n",
      "Test 8:\n",
      "   Input:  'ŸÜÿπŸÖ'\n",
      "   Output: [FILTERED]\n",
      "\n",
      "Test 9:\n",
      "   Input:  'ŸàŸéŸÇŸéÿßŸÑŸé ÿ£ÿ≠ŸÖÿØ: Ÿáÿ∞ÿß ŸÖŸèŸáŸêŸÖŸåŸë ÿ¨ŸêÿØŸéŸëÿßŸã,, ŸàÿßŸÑŸÑŸá!!'\n",
      "   Output: 'ŸàŸÇÿßŸÑ ÿßÿ≠ŸÖÿØ: Ÿáÿ∞ÿß ŸÖŸáŸÖ ÿ¨ÿØÿßÿå ŸàÿßŸÑŸÑŸá!'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TEST THE COMPLETE PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "logger.section(\"üß™ TESTING COMPLETE PIPELINE\")\n",
    "\n",
    "# Test cases covering various issues\n",
    "test_cases = [\n",
    "    # Diacritics\n",
    "    \"ÿßŸÑÿ£ŸèŸÖŸéŸÖŸè ÿßŸÑŸÖŸèÿ™ŸéŸëÿ≠ŸêÿØŸéÿ© ŸÖŸèŸÜŸéÿ∏ŸéŸëŸÖŸéÿ© ÿØŸéŸàŸíŸÑŸêŸäŸéŸëÿ©.\",\n",
    "    \n",
    "    # Mixed punctuation\n",
    "    \"ÿ£ŸàŸÑÿßŸã, ÿ´ÿßŸÜŸäÿßŸã; ÿ´ÿßŸÑÿ´ÿßŸã?\",\n",
    "    \n",
    "    # Alef variations\n",
    "    \"ÿ£ÿ≠ŸÖÿØ Ÿàÿ•ÿ®ÿ±ÿßŸáŸäŸÖ Ÿàÿ¢ÿØŸÖ\",\n",
    "    \n",
    "    # Numbers\n",
    "    \"ŸÅŸä ÿπÿßŸÖ 2024 ŸàÿµŸÑ ÿßŸÑÿπÿØÿØ ÿ•ŸÑŸâ 100\",\n",
    "    \n",
    "    # Latin text\n",
    "    \"ÿßŸÑŸàÿ´ŸäŸÇÿ© A/47/10 ŸàÿßŸÑŸÇÿ±ÿßÿ± UNDP/2024\",\n",
    "    \n",
    "    # Consecutive punctuation\n",
    "    \"ŸÖÿßÿ∞ÿßÿüÿüÿü Ÿáÿ∞ÿß ÿµÿ≠Ÿäÿ≠...\",\n",
    "    \n",
    "    # Whitespace issues\n",
    "    \"ÿßŸÑŸÜÿµ   ŸÖÿπ    ŸÖÿ≥ÿßŸÅÿßÿ™   ŸÉÿ´Ÿäÿ±ÿ©\",\n",
    "    \n",
    "    # Short sentence (should be filtered)\n",
    "    \"ŸÜÿπŸÖ\",\n",
    "    \n",
    "    # Combined issues\n",
    "    \"ŸàŸéŸÇŸéÿßŸÑŸé ÿ£ÿ≠ŸÖÿØ: Ÿáÿ∞ÿß ŸÖŸèŸáŸêŸÖŸåŸë ÿ¨ŸêÿØŸéŸëÿßŸã,, ŸàÿßŸÑŸÑŸá!!\",\n",
    "]\n",
    "\n",
    "logger.info(\"Processing test cases:\\n\")\n",
    "\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    result = preprocessor.preprocess_line(test, apply_optional=False)\n",
    "    logger.info(f\"Test {i}:\")\n",
    "    logger.info(f\"   Input:  '{test}'\")\n",
    "    if result:\n",
    "        logger.info(f\"   Output: '{result}'\")\n",
    "    else:\n",
    "        logger.info(f\"   Output: [FILTERED]\")\n",
    "    logger.info(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7306e0b",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Part 5: Post-Preprocessing Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af68d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: POST-PREPROCESSING INSPECTION\n",
    "# ============================================================================\n",
    "\n",
    "def inspect_preprocessed_data(file_path: str, sample_size: int = 100000) -> Dict:\n",
    "    \"\"\"\n",
    "    Inspect preprocessed data to verify quality.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to preprocessed file\n",
    "    sample_size : int\n",
    "        Number of lines to inspect\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict\n",
    "        Inspection results\n",
    "    \"\"\"\n",
    "    logger.section(\"üîç POST-PREPROCESSING INSPECTION\")\n",
    "    logger.info(f\"Inspecting: {file_path}\")\n",
    "    \n",
    "    stats = {\n",
    "        'total_lines': 0,\n",
    "        'total_words': 0,\n",
    "        'total_chars': 0,\n",
    "        'word_counts': [],\n",
    "        'remaining_issues': {\n",
    "            'diacritics': 0,\n",
    "            'latin_letters': 0,\n",
    "            'western_numbers': 0,\n",
    "            'latin_punct': 0,\n",
    "            'consecutive_punct': 0,\n",
    "            'alef_variations': 0,\n",
    "        },\n",
    "        'punctuation_dist': Counter(),\n",
    "        'sample_lines': [],\n",
    "    }\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        logger.error(f\"File not found: {file_path}\")\n",
    "        return stats\n",
    "    \n",
    "    # Patterns for checking\n",
    "    diacritics_pattern = re.compile(r'[\\u064B-\\u0652]')\n",
    "    latin_pattern = re.compile(r'[A-Za-z]')\n",
    "    western_num_pattern = re.compile(r'[0-9]')\n",
    "    consecutive_punct_pattern = re.compile(r'[ÿåÿõÿü.,:;?!]{2,}')\n",
    "    alef_var_pattern = re.compile(r'[ÿ£ÿ•ÿ¢Ÿ±]')\n",
    "    arabic_word_pattern = re.compile(r'[\\u0600-\\u06FF]+')\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= sample_size:\n",
    "                break\n",
    "            \n",
    "            line = line.rstrip('\\n')\n",
    "            stats['total_lines'] += 1\n",
    "            stats['total_chars'] += len(line)\n",
    "            \n",
    "            words = arabic_word_pattern.findall(line)\n",
    "            stats['total_words'] += len(words)\n",
    "            stats['word_counts'].append(len(words))\n",
    "            \n",
    "            # Store sample lines\n",
    "            if len(stats['sample_lines']) < 10:\n",
    "                stats['sample_lines'].append(line)\n",
    "            \n",
    "            # Check for remaining issues\n",
    "            if diacritics_pattern.search(line):\n",
    "                stats['remaining_issues']['diacritics'] += 1\n",
    "            if latin_pattern.search(line):\n",
    "                stats['remaining_issues']['latin_letters'] += 1\n",
    "            if western_num_pattern.search(line):\n",
    "                stats['remaining_issues']['western_numbers'] += 1\n",
    "            if consecutive_punct_pattern.search(line):\n",
    "                stats['remaining_issues']['consecutive_punct'] += 1\n",
    "            if alef_var_pattern.search(line):\n",
    "                stats['remaining_issues']['alef_variations'] += 1\n",
    "            \n",
    "            # Count punctuation\n",
    "            for char in line:\n",
    "                if char in VALID_PUNCTUATION:\n",
    "                    stats['punctuation_dist'][char] += 1\n",
    "    \n",
    "    # Display results\n",
    "    logger.subsection(\"Basic Statistics\")\n",
    "    logger.info(f\"Total lines: {stats['total_lines']:,}\")\n",
    "    logger.info(f\"Total words: {stats['total_words']:,}\")\n",
    "    logger.info(f\"Total chars: {stats['total_chars']:,}\")\n",
    "    \n",
    "    if stats['word_counts']:\n",
    "        word_arr = np.array(stats['word_counts'])\n",
    "        logger.info(f\"Avg words/line: {np.mean(word_arr):.2f}\")\n",
    "        logger.info(f\"Min words: {np.min(word_arr)}\")\n",
    "        logger.info(f\"Max words: {np.max(word_arr)}\")\n",
    "    \n",
    "    logger.subsection(\"Remaining Issues Check\")\n",
    "    total_issues = sum(stats['remaining_issues'].values())\n",
    "    if total_issues == 0:\n",
    "        logger.success(\"No issues found! Data is clean.\")\n",
    "    else:\n",
    "        logger.warn(f\"Found {total_issues} potential issues:\")\n",
    "        for issue, count in stats['remaining_issues'].items():\n",
    "            if count > 0:\n",
    "                logger.info(f\"   {issue}: {count:,} lines\")\n",
    "    \n",
    "    logger.subsection(\"Punctuation Distribution\")\n",
    "    for char, count in stats['punctuation_dist'].most_common():\n",
    "        name = ARABIC_PUNCTUATION.get(char, 'Unknown')\n",
    "        logger.info(f\"   '{char}' ({name}): {count:,}\")\n",
    "    \n",
    "    logger.subsection(\"Sample Lines\")\n",
    "    for i, line in enumerate(stats['sample_lines'][:5], 1):\n",
    "        display = line[:80] + \"...\" if len(line) > 80 else line\n",
    "        logger.info(f\"   {i}. {display}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# This will be run after processing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6c4112",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Part 6: Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "653d3b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: PROCESS AND SAVE DATASET\n",
    "# ============================================================================\n",
    "\n",
    "def run_preprocessing_pipeline(\n",
    "    input_dir: str,\n",
    "    output_dir: str,\n",
    "    config: PreprocessingConfig,\n",
    "    create_variants: bool = True,\n",
    "    sample_size: Optional[int] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the complete preprocessing pipeline and save results.\n",
    "    \n",
    "    Creates multiple output variants:\n",
    "    1. mandatory_only.txt - Only mandatory preprocessing\n",
    "    2. with_waw_separation.txt - + waw separation\n",
    "    3. with_number_tokens.txt - + number replacement\n",
    "    4. full_preprocessing.txt - All preprocessing steps\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dir : str\n",
    "        Path to input dataset\n",
    "    output_dir : str\n",
    "        Path to output directory\n",
    "    config : PreprocessingConfig\n",
    "        Configuration object\n",
    "    create_variants : bool\n",
    "        Whether to create multiple preprocessing variants\n",
    "    sample_size : Optional[int]\n",
    "        Limit processing (None = full dataset)\n",
    "    \"\"\"\n",
    "    logger.section(\"üöÄ RUNNING PREPROCESSING PIPELINE\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = ArabicTextPreprocessor(config)\n",
    "    \n",
    "    # ============================================\n",
    "    # Variant 1: Mandatory preprocessing only\n",
    "    # ============================================\n",
    "    logger.subsection(\"Variant 1: Mandatory Preprocessing Only\")\n",
    "    \n",
    "    output_file_mandatory = os.path.join(output_dir, \"mandatory_only.txt\")\n",
    "    \n",
    "    # Ensure optional steps are off\n",
    "    config.separate_waw_conjunction = False\n",
    "    config.replace_numbers_with_token = False\n",
    "    config.remove_foreign_terms = False\n",
    "    \n",
    "    preprocessor = ArabicTextPreprocessor(config)\n",
    "    stats_mandatory = preprocessor.process_dataset(\n",
    "        input_dir, \n",
    "        output_file_mandatory,\n",
    "        apply_optional=False,\n",
    "        sample_size=sample_size\n",
    "    )\n",
    "    \n",
    "    # Save stats\n",
    "    stats_file = os.path.join(output_dir, \"stats_mandatory.json\")\n",
    "    with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            'total_input': stats_mandatory.total_input_lines,\n",
    "            'total_output': stats_mandatory.total_output_lines,\n",
    "            'empty_removed': stats_mandatory.empty_lines_removed,\n",
    "            'short_removed': stats_mandatory.short_lines_removed,\n",
    "            'long_truncated': stats_mandatory.long_lines_truncated,\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    if not create_variants:\n",
    "        return\n",
    "    \n",
    "    # ============================================\n",
    "    # Variant 2: With Waw Separation\n",
    "    # ============================================\n",
    "    logger.subsection(\"Variant 2: With Waw Separation\")\n",
    "    \n",
    "    config.separate_waw_conjunction = True\n",
    "    preprocessor = ArabicTextPreprocessor(config)\n",
    "    \n",
    "    output_file_waw = os.path.join(output_dir, \"with_waw_separation.txt\")\n",
    "    stats_waw = preprocessor.process_dataset(\n",
    "        input_dir,\n",
    "        output_file_waw,\n",
    "        apply_optional=True,\n",
    "        sample_size=sample_size\n",
    "    )\n",
    "    \n",
    "    config.separate_waw_conjunction = False  # Reset\n",
    "    \n",
    "    # ============================================\n",
    "    # Variant 3: With Number Tokens\n",
    "    # ============================================\n",
    "    logger.subsection(\"Variant 3: With Number Tokens\")\n",
    "    \n",
    "    config.replace_numbers_with_token = True\n",
    "    preprocessor = ArabicTextPreprocessor(config)\n",
    "    \n",
    "    output_file_nums = os.path.join(output_dir, \"with_number_tokens.txt\")\n",
    "    stats_nums = preprocessor.process_dataset(\n",
    "        input_dir,\n",
    "        output_file_nums,\n",
    "        apply_optional=True,\n",
    "        sample_size=sample_size\n",
    "    )\n",
    "    \n",
    "    config.replace_numbers_with_token = False  # Reset\n",
    "    \n",
    "    # ============================================\n",
    "    # Variant 4: Full Preprocessing\n",
    "    # ============================================\n",
    "    logger.subsection(\"Variant 4: Full Preprocessing (All Options)\")\n",
    "    \n",
    "    config.separate_waw_conjunction = True\n",
    "    config.replace_numbers_with_token = True\n",
    "    config.remove_foreign_terms = True\n",
    "    \n",
    "    preprocessor = ArabicTextPreprocessor(config)\n",
    "    \n",
    "    output_file_full = os.path.join(output_dir, \"full_preprocessing.txt\")\n",
    "    stats_full = preprocessor.process_dataset(\n",
    "        input_dir,\n",
    "        output_file_full,\n",
    "        apply_optional=True,\n",
    "        sample_size=sample_size\n",
    "    )\n",
    "    \n",
    "    logger.section(\"‚úÖ PREPROCESSING COMPLETE\")\n",
    "    logger.info(f\"Output files saved to: {output_dir}\")\n",
    "    logger.info(\"Files created:\")\n",
    "    logger.info(f\"   1. mandatory_only.txt\")\n",
    "    logger.info(f\"   2. with_waw_separation.txt\")\n",
    "    logger.info(f\"   3. with_number_tokens.txt\")\n",
    "    logger.info(f\"   4. full_preprocessing.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec65de9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚ö° EXECUTING PREPROCESSING PIPELINE\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üöÄ RUNNING PREPROCESSING PIPELINE\n",
      "======================================================================\n",
      "\n",
      "--- Variant 1: Mandatory Preprocessing Only ---\n",
      "\n",
      "======================================================================\n",
      "üöÄ PROCESSING DATASET\n",
      "======================================================================\n",
      "Input: ../SSAC-UNPC\n",
      "Output: preprocessed_data\\mandatory_only.txt\n",
      "Apply optional steps: False\n",
      "Processing sample of 1,000,000 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000000/1000000 [01:52<00:00, 8926.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä PREPROCESSING STATISTICS\n",
      "======================================================================\n",
      "Input lines:  1,000,000\n",
      "Output lines: 966,104\n",
      "Lines kept:   96.61%\n",
      "\n",
      "--- Filtering Statistics ---\n",
      "Empty lines removed:     0\n",
      "Short lines removed:     33,896\n",
      "Long lines truncated:    8,074\n",
      "\n",
      "--- Normalization Statistics ---\n",
      "Diacritics removed:      86,519\n",
      "Alef normalized:         0\n",
      "Punctuation normalized:  14,389\n",
      "Numbers normalized:      82,420\n",
      "Latin removed:           76,163\n",
      "OOV chars removed:       389,971\n",
      "Consec. punct fixed:     141\n",
      "\n",
      "--- Variant 2: With Waw Separation ---\n",
      "\n",
      "======================================================================\n",
      "üöÄ PROCESSING DATASET\n",
      "======================================================================\n",
      "Input: ../SSAC-UNPC\n",
      "Output: preprocessed_data\\with_waw_separation.txt\n",
      "Apply optional steps: True\n",
      "Processing sample of 1,000,000 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000000/1000000 [02:05<00:00, 7971.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä PREPROCESSING STATISTICS\n",
      "======================================================================\n",
      "Input lines:  1,000,000\n",
      "Output lines: 966,534\n",
      "Lines kept:   96.65%\n",
      "\n",
      "--- Filtering Statistics ---\n",
      "Empty lines removed:     0\n",
      "Short lines removed:     33,466\n",
      "Long lines truncated:    9,707\n",
      "\n",
      "--- Normalization Statistics ---\n",
      "Diacritics removed:      86,519\n",
      "Alef normalized:         0\n",
      "Punctuation normalized:  14,389\n",
      "Numbers normalized:      82,420\n",
      "Latin removed:           76,163\n",
      "OOV chars removed:       389,971\n",
      "Consec. punct fixed:     141\n",
      "\n",
      "--- Variant 3: With Number Tokens ---\n",
      "\n",
      "======================================================================\n",
      "üöÄ PROCESSING DATASET\n",
      "======================================================================\n",
      "Input: ../SSAC-UNPC\n",
      "Output: preprocessed_data\\with_number_tokens.txt\n",
      "Apply optional steps: True\n",
      "Processing sample of 1,000,000 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000000/1000000 [02:02<00:00, 8152.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä PREPROCESSING STATISTICS\n",
      "======================================================================\n",
      "Input lines:  1,000,000\n",
      "Output lines: 961,832\n",
      "Lines kept:   96.18%\n",
      "\n",
      "--- Filtering Statistics ---\n",
      "Empty lines removed:     0\n",
      "Short lines removed:     38,168\n",
      "Long lines truncated:    7,487\n",
      "\n",
      "--- Normalization Statistics ---\n",
      "Diacritics removed:      86,519\n",
      "Alef normalized:         0\n",
      "Punctuation normalized:  14,389\n",
      "Numbers normalized:      82,420\n",
      "Latin removed:           76,163\n",
      "OOV chars removed:       389,971\n",
      "Consec. punct fixed:     141\n",
      "\n",
      "--- Variant 4: Full Preprocessing (All Options) ---\n",
      "\n",
      "======================================================================\n",
      "üöÄ PROCESSING DATASET\n",
      "======================================================================\n",
      "Input: ../SSAC-UNPC\n",
      "Output: preprocessed_data\\full_preprocessing.txt\n",
      "Apply optional steps: True\n",
      "Processing sample of 1,000,000 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000000/1000000 [02:16<00:00, 7339.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä PREPROCESSING STATISTICS\n",
      "======================================================================\n",
      "Input lines:  1,000,000\n",
      "Output lines: 962,309\n",
      "Lines kept:   96.23%\n",
      "\n",
      "--- Filtering Statistics ---\n",
      "Empty lines removed:     0\n",
      "Short lines removed:     37,691\n",
      "Long lines truncated:    9,014\n",
      "\n",
      "--- Normalization Statistics ---\n",
      "Diacritics removed:      86,519\n",
      "Alef normalized:         0\n",
      "Punctuation normalized:  14,389\n",
      "Numbers normalized:      82,420\n",
      "Latin removed:           76,163\n",
      "OOV chars removed:       389,971\n",
      "Consec. punct fixed:     141\n",
      "\n",
      "======================================================================\n",
      "‚úÖ PREPROCESSING COMPLETE\n",
      "======================================================================\n",
      "Output files saved to: preprocessed_data\n",
      "Files created:\n",
      "   1. mandatory_only.txt\n",
      "   2. with_waw_separation.txt\n",
      "   3. with_number_tokens.txt\n",
      "   4. full_preprocessing.txt\n",
      "\n",
      "======================================================================\n",
      "üîç POST-PREPROCESSING INSPECTION\n",
      "======================================================================\n",
      "Inspecting: preprocessed_data\\mandatory_only.txt\n",
      "\n",
      "--- Basic Statistics ---\n",
      "Total lines: 50,000\n",
      "Total words: 1,355,722\n",
      "Total chars: 8,304,590\n",
      "Avg words/line: 27.11\n",
      "Min words: 3\n",
      "Max words: 122\n",
      "\n",
      "--- Remaining Issues Check ---\n",
      "‚ö†Ô∏è  WARNING: Found 124 potential issues:\n",
      "   consecutive_punct: 124 lines\n",
      "\n",
      "--- Punctuation Distribution ---\n",
      "   'ÿå' (Arabic Comma): 69,042\n",
      "   '.' (Full Stop): 49,973\n",
      "   'ÿõ' (Arabic Semicolon): 6,760\n",
      "   ':' (Colon): 1,724\n",
      "   'ÿü' (Arabic Question Mark): 68\n",
      "   '!' (Exclamation Mark): 3\n",
      "\n",
      "--- Sample Lines ---\n",
      "   1. Ÿ£ ŸàÿπŸÖŸÑÿß ÿ®ÿ∑ŸÑÿ® ÿßŸÑÿ¨ŸÖÿπŸäÿ© ÿßŸÑÿπÿßŸÖÿ© ÿßŸÑŸàÿßÿ±ÿØ ŸÅŸä ÿßŸÑŸÅŸÇÿ±ÿ© Ÿ• ŸÖŸÜ ÿßŸÑŸÇÿ±ÿßÿ± ÿßŸÑŸÖÿ∞ŸÉŸàÿ± ÿßÿπŸÑÿßŸáÿå Ÿàÿ¨Ÿá ÿßŸÑÿßŸÖ...\n",
      "   2. )Ÿ°( ÿßŸÑŸàÿ´ÿßÿ¶ŸÇ ÿßŸÑÿ±ÿ≥ŸÖŸäÿ© ŸÑŸÑÿ¨ŸÖÿπŸäÿ© ÿßŸÑÿπÿßŸÖÿ©ÿå ÿßŸÑÿØŸàÿ±ÿ© ÿßŸÑÿ≥ÿßÿ®ÿπÿ© ŸàÿßŸÑÿßÿ±ÿ®ÿπŸàŸÜÿå ÿßŸÑŸÖŸÑÿ≠ŸÇ ÿ±ŸÇŸÖ Ÿ°Ÿ† (Ÿ§ŸßŸ°...\n",
      "   3. )Ÿ¢( Ÿàÿ™ÿ±ÿØ ÿßŸäÿ∂ÿß ÿßŸÑÿßÿ¥ÿßÿ±ÿßÿ™ ÿßŸÑŸä ŸÖÿ≥ÿßŸÑÿ© ÿßŸÜÿ¥ÿßÿ° ŸÇÿ∂ÿßÿ° ÿ¨ŸÜÿßÿ¶Ÿä ÿØŸàŸÑŸä ŸÅŸä ÿßŸÑŸàÿ´ŸäŸÇÿ©.\n",
      "   4. Ÿ°ÿå ÿßŸÑŸÖÿ≥ÿ™ŸÜÿ≥ÿÆ ŸÅŸäŸáÿß ÿßŸÑÿ™ÿπŸÑŸäŸÇÿßÿ™ ŸàÿßŸÑŸÖŸÑÿßÿ≠ÿ∏ÿßÿ™ ÿßŸÑŸÖŸÇÿØŸÖÿ© ŸÖŸÜ ÿßŸÑÿ≠ŸÉŸàŸÖÿßÿ™ ÿ®ÿ¥ÿßŸÜ ŸÖÿ¥ÿ±Ÿàÿπ ŸÖÿØŸàŸÜÿ© ÿßŸÑÿ¨ÿ±ÿß...\n",
      "   5. Ÿ° ÿ™ŸÇÿØŸÖ ÿßÿ≥ÿ™ÿ±ÿßŸÑŸäÿß ÿßŸÑÿ™ÿπŸÑŸäŸÇÿßÿ™ ÿßŸÑÿ™ÿßŸÑŸäÿ© ÿπŸÑŸä ÿ™ŸÇÿ±Ÿäÿ± ÿßŸÑŸÅÿ±ŸäŸÇ ÿßŸÑÿπÿßŸÖŸÑ ÿßŸÑŸÖÿπŸÜŸä ÿ®ŸÖÿ≥ÿßŸÑÿ© ÿßŸÜÿ¥ÿßÿ° ŸÇÿ∂...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXECUTE PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "# Configuration for execution\n",
    "EXECUTE_FULL_PIPELINE = True  # Set to True to run on full dataset\n",
    "SAMPLE_SIZE_FOR_TEST = 1_000_000  # Set to None for full dataset\n",
    "\n",
    "if EXECUTE_FULL_PIPELINE:\n",
    "    logger.section(\"‚ö° EXECUTING PREPROCESSING PIPELINE\")\n",
    "    \n",
    "    # For testing, use a sample\n",
    "    sample = SAMPLE_SIZE_FOR_TEST  # Change to None for full processing\n",
    "    \n",
    "    run_preprocessing_pipeline(\n",
    "        input_dir=config.input_dir,\n",
    "        output_dir=config.output_dir,\n",
    "        config=config,\n",
    "        create_variants=True,\n",
    "        sample_size=sample\n",
    "    )\n",
    "    \n",
    "    # Inspect the mandatory preprocessing output\n",
    "    mandatory_file = os.path.join(config.output_dir, \"mandatory_only.txt\")\n",
    "    if os.path.exists(mandatory_file):\n",
    "        post_stats = inspect_preprocessed_data(mandatory_file, sample_size=50000)\n",
    "else:\n",
    "    logger.info(\"Pipeline execution skipped. Set EXECUTE_FULL_PIPELINE = True to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d943608c",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9cef1e91",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìã PREPROCESSING SUMMARY & RECOMMENDATIONS\n",
      "======================================================================\n",
      "\n",
      "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "‚ïë                        PREPROCESSING PIPELINE SUMMARY                         ‚ïë\n",
      "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
      "‚ïë                                                                              ‚ïë\n",
      "‚ïë  MANDATORY PREPROCESSING STEPS (Always Applied):                             ‚ïë\n",
      "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                           ‚ïë\n",
      "‚ïë  ‚úÖ Remove diacritics (tashkeel)                                             ‚ïë\n",
      "‚ïë  ‚úÖ Normalize Alef variations (ÿ£ÿ•ÿ¢Ÿ± ‚Üí ÿß)                                      ‚ïë\n",
      "‚ïë  ‚úÖ Normalize Alef Maksura (Ÿâ ‚Üí Ÿä)                                           ‚ïë\n",
      "‚ïë  ‚úÖ Remove Tatweel (ŸÄ)                                                       ‚ïë\n",
      "‚ïë  ‚úÖ Unify punctuation to Arabic (,;? ‚Üí ÿåÿõÿü)                                  ‚ïë\n",
      "‚ïë  ‚úÖ Unify numbers to Arabic (0-9 ‚Üí Ÿ†-Ÿ©)                                      ‚ïë\n",
      "‚ïë  ‚úÖ Remove Latin letters                                                     ‚ïë\n",
      "‚ïë  ‚úÖ Remove OOV characters                                                    ‚ïë\n",
      "‚ïë  ‚úÖ Handle consecutive punctuation                                           ‚ïë\n",
      "‚ïë  ‚úÖ Normalize whitespace                                                     ‚ïë\n",
      "‚ïë  ‚úÖ Add punctuation spacing                                                  ‚ïë\n",
      "‚ïë  ‚úÖ Filter empty lines                                                       ‚ïë\n",
      "‚ïë  ‚úÖ Filter short sentences (<3 words)                                        ‚ïë\n",
      "‚ïë  ‚úÖ Truncate long sentences (>100 words)                                     ‚ïë\n",
      "‚ïë                                                                              ‚ïë\n",
      "‚ïë  OPTIONAL PREPROCESSING STEPS (For Experimentation):                         ‚ïë\n",
      "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                       ‚ïë\n",
      "‚ïë  ‚öôÔ∏è  Separate Waw conjunction (Ÿà + word ‚Üí Ÿà word)                            ‚ïë\n",
      "‚ïë  ‚öôÔ∏è  Replace numbers with token (<NUM>)                                      ‚ïë\n",
      "‚ïë  ‚öôÔ∏è  Remove document references (A/47/10)                                    ‚ïë\n",
      "‚ïë  ‚öôÔ∏è  Remove/mark stopwords                                                   ‚ïë\n",
      "‚ïë  ‚öôÔ∏è  Replace rare words (<UNK>)                                              ‚ïë\n",
      "‚ïë                                                                              ‚ïë\n",
      "‚ïë  OUTPUT FILES CREATED:                                                       ‚ïë\n",
      "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                       ‚ïë\n",
      "‚ïë  üìÅ preprocessed_data/                                                       ‚ïë\n",
      "‚ïë     ‚îú‚îÄ‚îÄ mandatory_only.txt      (Recommended for most experiments)          ‚ïë\n",
      "‚ïë     ‚îú‚îÄ‚îÄ with_waw_separation.txt (Test waw separation effect)                ‚ïë\n",
      "‚ïë     ‚îú‚îÄ‚îÄ with_number_tokens.txt  (Test number normalization effect)          ‚ïë\n",
      "‚ïë     ‚îî‚îÄ‚îÄ full_preprocessing.txt  (All preprocessing applied)                 ‚ïë\n",
      "‚ïë                                                                              ‚ïë\n",
      "‚ïë  RECOMMENDATIONS:                                                            ‚ïë\n",
      "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                            ‚ïë\n",
      "‚ïë  1. Start with mandatory_only.txt for baseline experiments                   ‚ïë\n",
      "‚ïë  2. Use with_waw_separation.txt if conjunction handling helps                ‚ïë\n",
      "‚ïë  3. Test with_number_tokens.txt if numbers cause vocabulary issues          ‚ïë\n",
      "‚ïë  4. Compare results to determine optimal preprocessing                       ‚ïë\n",
      "‚ïë                                                                              ‚ïë\n",
      "‚ïë  NEXT STEPS:                                                                 ‚ïë\n",
      "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                                 ‚ïë\n",
      "‚ïë  1. Tokenize preprocessed data with chosen tokenizer                        ‚ïë\n",
      "‚ïë  2. Create train/validation/test splits                                     ‚ïë\n",
      "‚ïë  3. Generate labels for sequence-to-sequence task                           ‚ïë\n",
      "‚ïë  4. Train and evaluate models                                               ‚ïë\n",
      "‚ïë                                                                              ‚ïë\n",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: SUMMARY AND RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "\n",
    "logger.section(\"üìã PREPROCESSING SUMMARY & RECOMMENDATIONS\")\n",
    "\n",
    "summary = \"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                        PREPROCESSING PIPELINE SUMMARY                         ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë  MANDATORY PREPROCESSING STEPS (Always Applied):                             ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                           ‚ïë\n",
    "‚ïë  ‚úÖ Remove diacritics (tashkeel)                                             ‚ïë\n",
    "‚ïë  ‚úÖ Normalize Alef variations (ÿ£ÿ•ÿ¢Ÿ± ‚Üí ÿß)                                      ‚ïë\n",
    "‚ïë  ‚úÖ Normalize Alef Maksura (Ÿâ ‚Üí Ÿä)                                           ‚ïë\n",
    "‚ïë  ‚úÖ Remove Tatweel (ŸÄ)                                                       ‚ïë\n",
    "‚ïë  ‚úÖ Unify punctuation to Arabic (,;? ‚Üí ÿåÿõÿü)                                  ‚ïë\n",
    "‚ïë  ‚úÖ Unify numbers to Arabic (0-9 ‚Üí Ÿ†-Ÿ©)                                      ‚ïë\n",
    "‚ïë  ‚úÖ Remove Latin letters                                                     ‚ïë\n",
    "‚ïë  ‚úÖ Remove OOV characters                                                    ‚ïë\n",
    "‚ïë  ‚úÖ Handle consecutive punctuation                                           ‚ïë\n",
    "‚ïë  ‚úÖ Normalize whitespace                                                     ‚ïë\n",
    "‚ïë  ‚úÖ Add punctuation spacing                                                  ‚ïë\n",
    "‚ïë  ‚úÖ Filter empty lines                                                       ‚ïë\n",
    "‚ïë  ‚úÖ Filter short sentences (<3 words)                                        ‚ïë\n",
    "‚ïë  ‚úÖ Truncate long sentences (>100 words)                                     ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë  OPTIONAL PREPROCESSING STEPS (For Experimentation):                         ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                       ‚ïë\n",
    "‚ïë  ‚öôÔ∏è  Separate Waw conjunction (Ÿà + word ‚Üí Ÿà word)                            ‚ïë\n",
    "‚ïë  ‚öôÔ∏è  Replace numbers with token (<NUM>)                                      ‚ïë\n",
    "‚ïë  ‚öôÔ∏è  Remove document references (A/47/10)                                    ‚ïë\n",
    "‚ïë  ‚öôÔ∏è  Remove/mark stopwords                                                   ‚ïë\n",
    "‚ïë  ‚öôÔ∏è  Replace rare words (<UNK>)                                              ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë  OUTPUT FILES CREATED:                                                       ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                       ‚ïë\n",
    "‚ïë  üìÅ preprocessed_data/                                                       ‚ïë\n",
    "‚ïë     ‚îú‚îÄ‚îÄ mandatory_only.txt      (Recommended for most experiments)          ‚ïë\n",
    "‚ïë     ‚îú‚îÄ‚îÄ with_waw_separation.txt (Test waw separation effect)                ‚ïë\n",
    "‚ïë     ‚îú‚îÄ‚îÄ with_number_tokens.txt  (Test number normalization effect)          ‚ïë\n",
    "‚ïë     ‚îî‚îÄ‚îÄ full_preprocessing.txt  (All preprocessing applied)                 ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë  RECOMMENDATIONS:                                                            ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                            ‚ïë\n",
    "‚ïë  1. Start with mandatory_only.txt for baseline experiments                   ‚ïë\n",
    "‚ïë  2. Use with_waw_separation.txt if conjunction handling helps                ‚ïë\n",
    "‚ïë  3. Test with_number_tokens.txt if numbers cause vocabulary issues          ‚ïë\n",
    "‚ïë  4. Compare results to determine optimal preprocessing                       ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë  NEXT STEPS:                                                                 ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                                 ‚ïë\n",
    "‚ïë  1. Tokenize preprocessed data with chosen tokenizer                        ‚ïë\n",
    "‚ïë  2. Create train/validation/test splits                                     ‚ïë\n",
    "‚ïë  3. Generate labels for sequence-to-sequence task                           ‚ïë\n",
    "‚ïë  4. Train and evaluate models                                               ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\"\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f03b721e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üè∑Ô∏è LABEL GENERATION EXAMPLE\n",
      "======================================================================\n",
      "Label Mapping:\n",
      "   0: No punctuation (O)\n",
      "   1: Period (.)\n",
      "   2: Comma (ÿå)\n",
      "   3: Question (ÿü)\n",
      "   4: Semicolon (ÿõ)\n",
      "   5: Colon (:)\n",
      "   6: Exclamation (!)\n",
      "\n",
      "Text: Ÿáÿ∞ÿß ŸÜÿµ ÿπÿ±ÿ®Ÿäÿå Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿπŸÑÿßŸÖÿßÿ™ ÿ™ÿ±ŸÇŸäŸÖ.\n",
      "Words:  ['Ÿáÿ∞ÿß', 'ŸÜÿµ', 'ÿπÿ±ÿ®Ÿäÿå', 'Ÿäÿ≠ÿ™ŸàŸä', 'ÿπŸÑŸâ', 'ÿπŸÑÿßŸÖÿßÿ™', 'ÿ™ÿ±ŸÇŸäŸÖ']\n",
      "Labels: [0, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      "Text: ŸÖÿß ŸáŸà ÿßŸÑÿ≥ÿ§ÿßŸÑÿü\n",
      "Words:  ['ŸÖÿß', 'ŸáŸà', 'ÿßŸÑÿ≥ÿ§ÿßŸÑÿü']\n",
      "Labels: [0, 0, 0]\n",
      "\n",
      "Text: ÿ£ŸàŸÑÿßŸãÿõ ÿ´ÿßŸÜŸäÿßŸãÿõ ÿ´ÿßŸÑÿ´ÿßŸã.\n",
      "Words:  ['ÿ£ŸàŸÑÿßŸãÿõ', 'ÿ´ÿßŸÜŸäÿßŸãÿõ', 'ÿ´ÿßŸÑÿ´ÿßŸã']\n",
      "Labels: [0, 0, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LABEL GENERATION UTILITY\n",
    "# ============================================================================\n",
    "\n",
    "def generate_labels_for_line(text: str) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"\n",
    "    Generate word and label sequences for a preprocessed line.\n",
    "    \n",
    "    This is the format needed for sequence-to-sequence training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Preprocessed text line\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[List[str], List[int]]\n",
    "        (words, labels) where labels indicate punctuation after each word\n",
    "        \n",
    "    Label Mapping:\n",
    "    - 0: No punctuation (O)\n",
    "    - 1: Period (.)\n",
    "    - 2: Arabic Comma (ÿå)\n",
    "    - 3: Question Mark (ÿü)\n",
    "    - 4: Semicolon (ÿõ)\n",
    "    - 5: Colon (:)\n",
    "    - 6: Exclamation (!)\n",
    "    \"\"\"\n",
    "    LABEL_MAP = {\n",
    "        'O': 0,    # No punctuation\n",
    "        '.': 1,    # Period\n",
    "        'ÿå': 2,    # Arabic Comma\n",
    "        'ÿü': 3,    # Question Mark\n",
    "        'ÿõ': 4,    # Semicolon\n",
    "        ':': 5,    # Colon\n",
    "        '!': 6,    # Exclamation\n",
    "    }\n",
    "    \n",
    "    words = []\n",
    "    labels = []\n",
    "    \n",
    "    # Pattern for Arabic words\n",
    "    word_pattern = re.compile(r'[\\u0600-\\u06FFŸ†-Ÿ©]+')\n",
    "    \n",
    "    # Find all words and their positions\n",
    "    for match in word_pattern.finditer(text):\n",
    "        word = match.group()\n",
    "        end_pos = match.end()\n",
    "        \n",
    "        # Look for punctuation immediately after\n",
    "        remaining = text[end_pos:].lstrip()\n",
    "        \n",
    "        if remaining and remaining[0] in LABEL_MAP:\n",
    "            label = LABEL_MAP[remaining[0]]\n",
    "        else:\n",
    "            label = LABEL_MAP['O']\n",
    "        \n",
    "        words.append(word)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return words, labels\n",
    "\n",
    "\n",
    "# Test label generation\n",
    "logger.section(\"üè∑Ô∏è LABEL GENERATION EXAMPLE\")\n",
    "\n",
    "test_lines = [\n",
    "    \"Ÿáÿ∞ÿß ŸÜÿµ ÿπÿ±ÿ®Ÿäÿå Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿπŸÑÿßŸÖÿßÿ™ ÿ™ÿ±ŸÇŸäŸÖ.\",\n",
    "    \"ŸÖÿß ŸáŸà ÿßŸÑÿ≥ÿ§ÿßŸÑÿü\",\n",
    "    \"ÿ£ŸàŸÑÿßŸãÿõ ÿ´ÿßŸÜŸäÿßŸãÿõ ÿ´ÿßŸÑÿ´ÿßŸã.\",\n",
    "]\n",
    "\n",
    "logger.info(\"Label Mapping:\")\n",
    "logger.info(\"   0: No punctuation (O)\")\n",
    "logger.info(\"   1: Period (.)\")\n",
    "logger.info(\"   2: Comma (ÿå)\")\n",
    "logger.info(\"   3: Question (ÿü)\")\n",
    "logger.info(\"   4: Semicolon (ÿõ)\")\n",
    "logger.info(\"   5: Colon (:)\")\n",
    "logger.info(\"   6: Exclamation (!)\")\n",
    "logger.info(\"\")\n",
    "\n",
    "for text in test_lines:\n",
    "    words, labels = generate_labels_for_line(text)\n",
    "    logger.info(f\"Text: {text}\")\n",
    "    logger.info(f\"Words:  {words}\")\n",
    "    logger.info(f\"Labels: {labels}\")\n",
    "    logger.info(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41053881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÖ PREPROCESSING NOTEBOOK COMPLETE\n",
      "======================================================================\n",
      "All preprocessing functions and pipeline have been defined.\n",
      "Run the pipeline with EXECUTE_FULL_PIPELINE = True to process the full dataset.\n"
     ]
    }
   ],
   "source": [
    "logger.section(\"‚úÖ PREPROCESSING NOTEBOOK COMPLETE\")\n",
    "logger.info(\"All preprocessing functions and pipeline have been defined.\")\n",
    "logger.info(\"Run the pipeline with EXECUTE_FULL_PIPELINE = True to process the full dataset.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
