# %% [markdown]
# # ๐ง ุฎุท ุฃูุงุจูุจ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ููุฌููุนุฉ ุจูุงูุงุช ุงูุชุฑููู ุงูุนุฑุจู
# ## ูุนุงูุฌุฉ ูููู SSAC-UNPC
# 
# ---
# 
# ### ๐ ุฌุฏูู ุงููุญุชููุงุช
# 
# 1. [ุงูููุฏูุฉ ูุงูุฅุนุฏุงุฏ](#1-ุงูููุฏูุฉ-ูุงูุฅุนุฏุงุฏ)
# 2. [ุงูุฌุฒุก 1: ูุญุต ุงููุดููุงุช (ูุจู ุงููุนุงูุฌุฉ)](#2-ุงูุฌุฒุก-1-ูุญุต-ุงููุดููุงุช-ูุจู-ุงููุนุงูุฌุฉ)
#    - 2.1 ูุดููุงุช ูุณุชูู ุงูุญุฑูู
#    - 2.2 ูุดููุงุช ุนูุงูุงุช ุงูุชุฑููู
#    - 2.3 ูุดููุงุช ูุณุชูู ุงูุฌูู
#    - 2.4 ูุดููุงุช ุงูุฃููุงุท ุงูุฎุงุตุฉ
# 3. [ุงูุฌุฒุก 2: ุฎุทูุงุช ุงููุนุงูุฌุฉ ุงูุฅูุฒุงููุฉ](#3-ุงูุฌุฒุก-2-ุฎุทูุงุช-ุงููุนุงูุฌุฉ-ุงูุฅูุฒุงููุฉ)
#    - 3.1 ุฅุฒุงูุฉ ุงูุชุดููู (ุงูุญุฑูุงุช)
#    - 3.2 ุชูุญูุฏ ุฃุดูุงู ุงูุฃูู
#    - 3.3 ุชูุญูุฏ ุงูุชุงุก ุงููุฑุจูุทุฉ ูุงูุฃูู ุงูููุตูุฑุฉ
#    - 3.4 ุฅุฒุงูุฉ ุงูุญุฑูู ุฎุงุฑุฌ ุงููุทุงู (OOV)
#    - 3.5 ุฅุฒุงูุฉ ุงูุญุฑูู ุงููุงุชูููุฉ
#    - 3.6 ุชูุญูุฏ ุงูุฃุฑูุงู (ุงูุฃุฑูุงู ุงูุนุฑุจูุฉ)
#    - 3.7 ุชูุญูุฏ ุนูุงูุงุช ุงูุชุฑููู (ุงูุชุฑููู ุงูุนุฑุจู)
#    - 3.8 ูุนุงูุฌุฉ ุงูุชุฑููู ุงููุชุชุงุจุน
#    - 3.9 ุชูุญูุฏ ุงููุณุงูุงุช ูุถุจุท ูุณุงูุงุช ุงูุชุฑููู
#    - 3.10 ุฅุฒุงูุฉ ุงูุฃุณุทุฑ ุงููุงุฑุบุฉ ูุงููุตูุฑุฉ ุฌุฏุงู
#    - 3.11 ูุนุงูุฌุฉ ุงูุฌูู ุงูุทูููุฉ
# 4. [ุงูุฌุฒุก 3: ุฎุทูุงุช ุงููุนุงูุฌุฉ ุงูุงุฎุชูุงุฑูุฉ (ููุชุฌุฑูุจ)](#4-ุงูุฌุฒุก-3-ุฎุทูุงุช-ุงููุนุงูุฌุฉ-ุงูุงุฎุชูุงุฑูุฉ)
#    - 4.1 ูุตู ูุงู ุงูุนุทู ุนู ุงููููุงุช
#    - 4.2 ุงุณุชุฑุงุชูุฌูุงุช ุงูุชุนุงูู ูุน ูููุงุช ุงูุชููู
#    - 4.3 ุงุณุชุจุฏุงู ุงูุฃุฑูุงู ุจุฑูุฒ ุฎุงุต
#    - 4.4 ูุนุงูุฌุฉ ุงููููุงุช ุงููุงุฏุฑุฉ
#    - 4.5 ุชุณููุฉ ุทูู ุงูุฌูู
#    - 4.6 ุฅุฒุงูุฉ/ุงุณุชุจุฏุงู ุงููุตุทูุญุงุช ุงูุฃุฌูุจูุฉ
# 5. [ุงูุฌุฒุก 4: ุฎุท ุฃูุงุจูุจ ุงููุนุงูุฌุฉ ุงููุงูู](#5-ุงูุฌุฒุก-4-ุฎุท-ุฃูุงุจูุจ-ุงููุนุงูุฌุฉ-ุงููุงูู)
# 6. [ุงูุฌุฒุก 5: ุงููุญุต ุจุนุฏ ุงููุนุงูุฌุฉ](#6-ุงูุฌุฒุก-5-ุงููุญุต-ุจุนุฏ-ุงููุนุงูุฌุฉ)
# 7. [ุงูุฌุฒุก 6: ุญูุธ ุงูุจูุงูุงุช ุงููุนุงูุฌุฉ](#7-ุงูุฌุฒุก-6-ุญูุธ-ุงูุจูุงูุงุช-ุงููุนุงูุฌุฉ)
# 8. [ุงูููุฎุต ูุงูุชูุตูุงุช](#8-ุงูููุฎุต-ูุงูุชูุตูุงุช)
# 
# ---

# %% [markdown]
# ## 1. ุงูููุฏูุฉ ูุงูุฅุนุฏุงุฏ
# 
# ### ๐ฏ ุงูุบุฑุถ ูู ูุฐุง ุงูุฏูุชุฑ
# 
# ูููุฐ ูุฐุง ุงูุฏูุชุฑ ุฎุท ุฃูุงุจูุจ ูุนุงูุฌุฉ ูุณุจูุฉ ุดุงูู ููููู SSAC-UNPC 
# ูู ูุฌููุนุฉ ุจูุงูุงุช ุงูุชุฑููู ุงูุนุฑุจู (APD). ุจูุงุกู ุนูู ูุชุงุฆุฌ ุชุญููู ุงูุจูุงูุงุช ุงูุงุณุชูุดุงูู (EDA)ุ ูุนุงูุฌ:
# 
# **ุงููุดููุงุช ุงูุชู ุชู ุชุญุฏูุฏูุง ูู ุงูุชุญููู:**
# 
# | ุงููุดููุฉ | ุงูุดุฏุฉ | ุงูุญู |
# |-------|----------|----------|
# | ูุฌูุฏ ุงูุชุดููู (0.27%) | ูุชูุณุทุฉ | ุฅุฒุงูุฉ ุฌููุน ุงูุญุฑูุงุช |
# | ุฎููุท ุชุฑููู ุนุฑุจู/ูุงุชููู | ุนุงููุฉ | ุงูุชูุญูุฏ ุฅูู ุงูุนุฑุจู |
# | ุฎููุท ุฃุฑูุงู ุนุฑุจูุฉ/ุบุฑุจูุฉ | ูุชูุณุทุฉ | ุงูุชูุญูุฏ ุฅูู ุงูุนุฑุจู |
# | ุฃุดูุงู ูุชุนุฏุฏุฉ ููุฃูู | ููุฎูุถุฉ | ุงูุชูุญูุฏ ุฅูู ุฃูู ูุฌุฑุฏุฉ |
# | ุญุฑูู ุฎุงุฑุฌ ุงููุทุงู | ููุฎูุถุฉ | ุฅุฒุงูุฉ ุฃู ุงุณุชุจุฏุงู |
# | ุญุฑูู ูุงุชูููุฉ ูู ุงููุต | ูุชูุณุทุฉ | ุฅุฒุงูุฉ |
# | ุฌูู ูุตูุฑุฉ ุฌุฏุงู (<3 ูููุงุช) | ูุชูุณุทุฉ | ุชุตููุฉ (ุญุฐู) |
# | ุฌูู ุทูููุฉ ุฌุฏุงู (>100 ูููุฉ) | ููุฎูุถุฉ | ูุต ุฃู ุชูุณูู |
# | ุชุฑููู ูุชุชุงุจุน | ููุฎูุถุฉ | ูุนุงูุฌุฉ ููุงุณุจุฉ |
# | ุชุฑููู ููุชุตู ุจุงููููุงุช | ูุชูุณุทุฉ | ุฅุถุงูุฉ ูุณุงูุงุช |
# 
# ### ๐ ุงููุชุงุฆุฌ ุงููุชููุนุฉ
# 
# ุจุนุฏ ุงููุนุงูุฌุฉ:
# - ูุต ุนุฑุจู ูุธูู ููุชุณู
# - ูุธุงู ุชุฑููู ููุญุฏ (ุนุฑุจู ููุท)
# - ูุธุงู ุฃุฑูุงู ููุญุฏ (ุนุฑุจู ููุท)
# - ุฎูู ูู ุงูุชุดููู ูุงูุนูุงูุงุช ุงูุฎุงุตุฉ
# - ูุณุงูุงุช ุตุญูุญุฉ ุญูู ุนูุงูุงุช ุงูุชุฑููู
# - ุชุตููุฉ ุงูุฌูู ุงููุตูุฑุฉ/ุงูุฅุดูุงููุฉ
# - ุฌุงูุฒูุฉ ููุชุฑููุฒ (Tokenization) ูุชุฏุฑูุจ ุงููููุฐุฌ

# %%
# ============================================================================
# ุงููุณู 1: ุงูุงุณุชูุฑุงุฏ ูุงูุฅุนุฏุงุฏ
# ============================================================================

# -----------------------------
# ููุชุจุงุช ุจุงูุซูู ุงูููุงุณูุฉ
# -----------------------------
import os                      # ูุนูููุงุช ุงููุธุงู (ุงููุณุงุฑุงุช)
import re                      # ููุชุนุงุจูุฑ ุงูููุทูุฉ (Regex)
import sys                     # ููุธุงุฆู ุงููุธุงู
import json                    # ููุชุนุงูู ูุน ูููุงุช JSON
import random                  # ููุชูููุฏ ุงูุนุดูุงุฆู
from pathlib import Path       # ููุชุนุงูู ุงูุญุฏูุซ ูุน ุงููุณุงุฑุงุช
from collections import Counter # ููุนุฏ ุงููุนุงู
from typing import List, Tuple, Dict, Optional, Generator, Callable # ูุฃููุงุท ุงูุจูุงูุงุช
from dataclasses import dataclass, field # ูุชุนุฑูู ููุงูู ุงูุจูุงูุงุช

# -----------------------------
# ููุชุจุงุช ุชุญููู ุงูุจูุงูุงุช
# -----------------------------
import numpy as np             # ููุนูููุงุช ุงูุญุณุงุจูุฉ

# -----------------------------
# ุดุฑูุท ุงูุชูุฏู
# -----------------------------
try:
    from tqdm import tqdm      # ุดุฑูุท ุชูุฏู ููุนูููุงุช ุงูุทูููุฉ
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("ููุงุญุธุฉ: ููุชุจุฉ tqdm ุบูุฑ ูุซุจุชุฉ. ููุชุซุจูุช: pip install tqdm")

# %%
# ============================================================================
# ุงููุณู 2: ุฅุนุฏุงุฏ ุงููุณุฌู (LOGGER)
# ============================================================================

class NotebookLogger:
    """
    ูุณุฌู ููุญุฏ ููุจุณุท ูุฏูุงุชุฑ ุฌูุจูุชุฑ.
    
    - ูุทุจุน ุงููุฎุฑุฌุงุช ูู ุงูุฏูุชุฑ
    - ูุถูู ุงูุณุฌูุงุช ุฅูู ููู
    - ุจุฏูู ุทูุงุจุน ุฒูููุฉ
    - ุจุฏูู ุชุฑููุณุงุช ุงูุฌูุณุฉ
    """

    def __init__(
        self,
        log_file: str | Path = "preprocessing.log",
        enable_console: bool = True,
        enable_file: bool = True,
    ):
        self.log_file = Path(log_file)
        self.enable_console = enable_console
        self.enable_file = enable_file

        if self.enable_file:
            self.log_file.parent.mkdir(parents=True, exist_ok=True)

    def _write(self, message: str):
        if self.enable_console:
            print(message, end="")

        if self.enable_file:
            with self.log_file.open("a", encoding="utf-8") as f:
                f.write(message)

    def info(self, message: str):
        self._write(f"{message}\n")

    def warn(self, message: str):
        self._write(f"โ๏ธ  ุชุญุฐูุฑ: {message}\n")

    def error(self, message: str):
        self._write(f"โ ุฎุทุฃ: {message}\n")

    def success(self, message: str):
        self._write(f"โ {message}\n")

    def section(self, title: str):
        block = (
            "\n" + "=" * 70 +
            f"\n{title}\n" +
            "=" * 70 + "\n"
        )
        self._write(block)

    def subsection(self, title: str):
        self._write(f"\n--- {title} ---\n")


# ุชููุฆุฉ ุงููุณุฌู
logger = NotebookLogger(log_file="logs/preprocessing.log")

# %%
# ============================================================================
# ุงููุณู 3: ุงูุฅุนุฏุงุฏุงุช (CONFIGURATION)
# ============================================================================

@dataclass
class PreprocessingConfig:
    """
    ุฅุนุฏุงุฏุงุช ุฎุท ุฃูุงุจูุจ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ.
    
    ููุตู ุจูู ุฎุทูุงุช ุงููุนุงูุฌุฉ ุงูุฅูุฒุงููุฉ ูุงูุงุฎุชูุงุฑูุฉ.
    """
    
    # -----------------------------
    # ูุณุงุฑุงุช ุงููููุงุช
    # -----------------------------
    input_dir: str = "../SSAC-UNPC"
    output_dir: str = "preprocessed_data"
    log_dir: str = "logs"
    
    # -----------------------------
    # ุงููุนุงูุฌุฉ ุงูุฅูุฒุงููุฉ (ุชุทุจู ุฏุงุฆูุงู)
    # -----------------------------
    remove_diacritics: bool = True       # ุฅุฒุงูุฉ ุงูุชุดููู
    normalize_alef: bool = True          # ุชูุญูุฏ ุงูุฃูู
    normalize_teh_marbuta: bool = True   # ุฉ (ุงุฎุชูุงุฑูุ ุงูุจุนุถ ูุญูููุง ูู ู)
    normalize_alef_maksura: bool = True  # ู -> ู
    remove_tatweel: bool = True          # ุฅุฒุงูุฉ ุงูุชุทููู (ู)
    remove_latin_letters: bool = True    # ุฅุฒุงูุฉ ุงูุญุฑูู ุงููุงุชูููุฉ
    remove_oov_chars: bool = True        # ุฅุฒุงูุฉ ุงูุญุฑูู ุฎุงุฑุฌ ุงููุทุงู
    unify_numbers_to_arabic: bool = True # ุชูุญูุฏ ุงูุฃุฑูุงู ููุนุฑุจูุฉ
    unify_punctuation_to_arabic: bool = True # ุชูุญูุฏ ุงูุชุฑููู ููุนุฑุจู
    handle_consecutive_punct: bool = True # ูุนุงูุฌุฉ ุงูุชุฑููู ุงููุชุชุงุจุน
    normalize_whitespace: bool = True    # ุชูุญูุฏ ุงููุณุงูุงุช
    add_punct_spacing: bool = True       # ุฅุถุงูุฉ ูุณุงูุงุช ููุชุฑููู
    
    # -----------------------------
    # ุชุตููุฉ ุงูุฌูู
    # -----------------------------
    min_words: int = 3
    max_words: int = 100
    remove_empty_lines: bool = True
    
    # -----------------------------
    # ุงููุนุงูุฌุฉ ุงูุงุฎุชูุงุฑูุฉ (ููุชุฌุฑูุจ)
    # -----------------------------
    separate_waw_conjunction: bool = False   # ูุตู ูุงู ุงูุนุทู
    remove_stopwords: bool = False           # ุฅุฒุงูุฉ ูููุงุช ุงูุชููู
    replace_numbers_with_token: bool = False # ุงุณุชุจุฏุงู ุงูุฃุฑูุงู ุจุฑูุฒ
    replace_rare_words: bool = False         # ุงุณุชุจุฏุงู ุงููููุงุช ุงููุงุฏุฑุฉ
    rare_word_threshold: int = 5             # ุญุฏ ุงููููุงุช ุงููุงุฏุฑุฉ
    remove_foreign_terms: bool = False       # ุฅุฒุงูุฉ ุงููุตุทูุญุงุช ุงูุฃุฌูุจูุฉ
    
    # -----------------------------
    # ูุนุงููุงุช ุงููุนุงูุฌุฉ
    # -----------------------------
    sample_size: Optional[int] = None  # None = ูุนุงูุฌุฉ ุงููู
    chunk_size: int = 100000           # ุนุฏุฏ ุงูุฃุณุทุฑ ููู ุฏูุนุฉ (ููููุงุกุฉ)
    random_seed: int = 42


# ุชููุฆุฉ ุงูุฅุนุฏุงุฏุงุช
config = PreprocessingConfig()

# ุฅูุดุงุก ูุฌูุฏุงุช ุงููุฎุฑุฌุงุช
os.makedirs(config.output_dir, exist_ok=True)
os.makedirs(config.log_dir, exist_ok=True)

logger.info("โ ุชู ุชููุฆุฉ ุงูุฅุนุฏุงุฏุงุช!")
logger.info(f"   ูุฌูุฏ ุงููุฏุฎูุงุช: {config.input_dir}")
logger.info(f"   ูุฌูุฏ ุงููุฎุฑุฌุงุช: {config.output_dir}")

# %%
# ============================================================================
# ุงููุณู 4: ุชุนุฑููุงุช ุงูุญุฑูู ุงูุนุฑุจูุฉ
# ============================================================================

# -----------------------------
# ุงูุญุฑูู ุงูุนุฑุจูุฉ ุงูุตุงูุญุฉ
# -----------------------------
# ุจูุงุกู ุนูู ูุชุงุฆุฌ ุงูุชุญููู: ุฌููุน ุงูุญุฑูู ุงูุนุฑุจูุฉ ุงูููุฌูุฏุฉ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช

ARABIC_LETTERS = set(
    'ุก ุข ุฃ ุค ุฅ ุฆ ุง ุจ ุฉ ุช ุซ ุฌ ุญ ุฎ ุฏ ุฐ ุฑ ุฒ ุณ ุด ุต ุถ ุท ุธ ุน ุบ ู ู ู ู ู ู ู ู ู ู'
    .split()
)

# ูุฌููุนุฉ ููุณุนุฉ ุชุดูู ุงูุญุฑูู ุงูุฃูู ุดููุนุงู (ุชุฃุซูุฑ ูุงุฑุณู/ุฃุฑุฏู ูู ุงูุฃุณูุงุก)
ARABIC_LETTERS_EXTENDED = ARABIC_LETTERS | set('ูพ ฺ ฺ ฺฏ ฺค')

# -----------------------------
# ุงูุชุดููู ุงูุนุฑุจู (ุงูุญุฑูุงุช)
# -----------------------------
ARABIC_DIACRITICS = {
    '\u064B': 'ูุชุญุชุงู',     # ู
    '\u064C': 'ุถูุชุงู',      # ู
    '\u064D': 'ูุณุฑุชุงู',     # ู
    '\u064E': 'ูุชุญุฉ',       # ู
    '\u064F': 'ุถูุฉ',        # ู
    '\u0650': 'ูุณุฑุฉ',       # ู
    '\u0651': 'ุดุฏุฉ',        # ู
    '\u0652': 'ุณููู',       # ู
}

DIACRITICS_PATTERN = re.compile(r'[\u064B-\u0652]')

# -----------------------------
# ุงูุฃุฑูุงู ุงูุนุฑุจูุฉ
# -----------------------------
ARABIC_NUMERALS = 'ููกูขูฃูคูฅูฆูงูจูฉ'
WESTERN_NUMERALS = '0123456789'

# ุฌุฏุงูู ุงูุชุญููู
WESTERN_TO_ARABIC_NUMS = str.maketrans(WESTERN_NUMERALS, ARABIC_NUMERALS)
ARABIC_TO_WESTERN_NUMS = str.maketrans(ARABIC_NUMERALS, WESTERN_NUMERALS)

# -----------------------------
# ุนูุงูุงุช ุงูุชุฑููู
# -----------------------------
# ุงูุชุฑููู ุงููุณุชูุฏู (ุงูุนุฑุจู)
ARABIC_PUNCTUATION = {
    'ุ': 'ูุงุตูุฉ ุนุฑุจูุฉ',
    'ุ': 'ูุงุตูุฉ ููููุทุฉ ุนุฑุจูุฉ',
    'ุ': 'ุนูุงูุฉ ุงุณุชููุงู ุนุฑุจูุฉ',
    '.': 'ููุทุฉ',
    ':': 'ููุทุชุงู ุฑุฃุณูุชุงู',
    '!': 'ุนูุงูุฉ ุชุนุฌุจ',
}

# ุงููุนุงุฏูุงุช ุงููุงุชูููุฉ ููุชูุญูุฏ
LATIN_TO_ARABIC_PUNCT = {
    ',': 'ุ',   # ูุงุตูุฉ ูุงุชูููุฉ -> ุนุฑุจูุฉ
    ';': 'ุ',   # ูุงุตูุฉ ููููุทุฉ ูุงุชูููุฉ -> ุนุฑุจูุฉ
    '?': 'ุ',   # ุนูุงูุฉ ุงุณุชููุงู ูุงุชูููุฉ -> ุนุฑุจูุฉ
}

# ุฌููุน ุนูุงูุงุช ุงูุชุฑููู ุงูุตุงูุญุฉ (ุจุนุฏ ุงูุชูุญูุฏ)
VALID_PUNCTUATION = set(ARABIC_PUNCTUATION.keys())

# ุนูุงูุงุช ููุงูุฉ ุงูุฌููุฉ
SENTENCE_TERMINALS = {'.', 'ุ', '!'}

# -----------------------------
# ุฃุดูุงู ุงูุฃูู
# -----------------------------
ALEF_VARIATIONS = {
    'ุฃ': 'ุง',  # ุฃูู ููุฒุฉ ููู
    'ุฅ': 'ุง',  # ุฃูู ููุฒุฉ ุชุญุช
    'ุข': 'ุง',  # ุฃูู ูุฏุฉ
    'ูฑ': 'ุง',  # ุฃูู ูุตู
}

# -----------------------------
# ูููุงุช ุงูุชููู ุงูุนุฑุจูุฉ
# -----------------------------
ARABIC_STOPWORDS = set([
    # ุญุฑูู ุงูุฌุฑ
    'ูู', 'ูู', 'ุนูู', 'ุฅูู', 'ุงูู', 'ุนู', 'ูุน', 'ุจูู', 'ุนูุฏ', 'ุญุชู', 'ููุฐ',
    'ุงูู', 'ูู', 'ุนูู',
    # ุฃุณูุงุก ุงูุฅุดุงุฑุฉ
    'ูุฐุง', 'ูุฐู', 'ุฐูู', 'ุชูู', 'ูุคูุงุก', 'ุฃููุฆู',
    # ุงูุฃุณูุงุก ุงูููุตููุฉ
    'ุงูุชู', 'ุงูุฐู', 'ุงููุฐุงู', 'ุงููุชุงู', 'ุงูุฐูู', 'ุงููุงุชู', 'ุงูููุงุชู',
    # ุฃุฏูุงุช ุงูุนุทู
    'ู', 'ุฃู', 'ุงู', 'ุซู', 'ููู', 'ุจู', 'ุฅุฐุง', 'ูู', 'ุฅุฐ', 'ู',
    # ุฃุฏูุงุช ุฃุฎุฑู
    'ุฃู', 'ุงู', 'ุฅู', 'ูุฏ', 'ูุง', 'ูุง', 'ูู', 'ูู', 'ู', 'ุจ', 'ู',
    # ุงูุถูุงุฆุฑ
    'ูู', 'ูู', 'ูู', 'ูู', 'ุฃูุง', 'ูุญู', 'ุฃูุช', 'ุฃูุชู', 'ุงูุง', 'ุงูุช',
    # ุงูุฃูุนุงู ุงููุณุงุนุฏุฉ
    'ูุงู', 'ูุงูุช', 'ูููู', 'ุชููู', 'ูุงููุง', 'ููุณ', 'ููุณุช',
    # ุฃุฎุฑู
    'ูู', 'ุจุนุถ', 'ุฃู', 'ุงู', 'ุบูุฑ', 'ุจุนุฏ', 'ูุจู', 'ุญูุซ', 'ุนูุฏูุง',
    'ุญูู', 'ุฏูู', 'ุถุฏ', 'ุฎูุงู', 'ุนุจุฑ', 'ูุญู', 'ููู', 'ุชุญุช',
    # ูููุงุช ูุธูููุฉ ุดุงุฆุนุฉ
    'ููู', 'ููู', 'ูุนูู', 'ูุงูู', 'ููุน', 'ููู', 'ููู', 'ููุฐุง', 'ููุฐู',
    'ูุฅู', 'ูุงู', 'ูุฅู', 'ูุงู', 'ูุฃู', 'ูุงู', 'ููุง', 'ููุง', 'ุฅูู', 'ุงูู',
    'ุฅููุง', 'ุงููุง', 'ุฃูู', 'ุฃููุง', 'ุฐุงุช', 'ููุง', 'ูู', 'ููู', 'ุจูุง', 'ุจู',
    'ูููุง', 'ููู', 'ูููุง', 'ููู', 'ุนููุง', 'ุนูู', 'ุฅูููุง', 'ุฅููู',
    'ุนูููุง', 'ุนููู', 'ูุนูุง', 'ูุนู', 'ุจูููุง', 'ุจูููู',
])

# -----------------------------
# ุฃููุงุท ูุงู ุงูุนุทู
# -----------------------------
# ุงูุญุฏ ุงูุฃุฏูู ูุทูู ุงููููุฉ ููุตู ุงููุงู
WAW_CONJUNCTION_MIN_LENGTH = 3  # ุงูุตู ููุท ุฅุฐุง ุชุจูู 3 ุญุฑูู ุฃู ุฃูุซุฑ

logger.info("โ ุชู ุชุญููู ุชุนุฑููุงุช ุงูุญุฑูู ุงูุนุฑุจูุฉ!")
logger.info(f"   - ุงูุญุฑูู ุงูุนุฑุจูุฉ: {len(ARABIC_LETTERS)}")
logger.info(f"   - ุงูุชุดููู: {len(ARABIC_DIACRITICS)}")
logger.info(f"   - ูููุงุช ุงูุชููู: {len(ARABIC_STOPWORDS)}")
logger.info(f"   - ุนูุงูุงุช ุงูุชุฑููู ุงูุตุงูุญุฉ: {len(VALID_PUNCTUATION)}")

# %%
# ============================================================================
# ุงููุณู 5: ุฃุฏูุงุช ุชุญููู ุงูุจูุงูุงุช
# ============================================================================

def iter_dataset_lines(dataset_dir: str, encoding: str = "utf-8") -> Generator[str, None, None]:
    """
    ุชูุฑุงุฑ (Iterate) ุนูู ุฌููุน ูููุงุช ูุฌููุนุฉ ุงูุจูุงูุงุช ูุชุฏูู ุฃุณุทุฑ ูุงุญุฏ.
    
    ุชุทุจู ูุฐู ุงูุฏุงูุฉ ุงูุชุญููู ุงููุณูู (Lazy Loading) ููุชุนุงูู ูุน ุงูุจูุงูุงุช ุงููุจูุฑุฉ
    ุงูุชู ูุง ูููู ุชุญููููุง ูู ุงูุฐุงูุฑุฉ ุฏูุนุฉ ูุงุญุฏุฉ.
    
    ุงููุนุงููุงุช:
    -----------
    dataset_dir : str
        ูุณุงุฑ ุงููุฌูุฏ ุงููุญุชูู ุนูู ูููุงุช .txt
    encoding : str
        ุชุฑููุฒ ุงูููู (ุงูุงูุชุฑุงุถู: utf-8)
        
    ุชูุชุฌ (Yields):
    -------
    str
        ุฌููุฉ/ุณุทุฑ ูุงุญุฏ ูู ูู ูุฑุฉ (ุจุฏูู ูุญุฑู ุงูุณุทุฑ ุงูุฌุฏูุฏ)
    """
    # ุงูุญุตูู ุนูู ุฌููุน ูููุงุช ุงููุตูุต ูุฑุชุจุฉ
    txt_files = sorted(Path(dataset_dir).glob("*.txt"))
    
    if not txt_files:
        logger.error(f"ูู ูุชู ุงูุนุซูุฑ ุนูู ูููุงุช .txt ูู {dataset_dir}")
        return
    
    for file_path in txt_files:
        try:
            with open(file_path, "r", encoding=encoding) as f:
                for line in f:
                    yield line.rstrip("\n")
        except Exception as e:
            logger.error(f"ุฎุทุฃ ูู ูุฑุงุกุฉ {file_path}: {e}")
            continue


def count_total_lines(dataset_dir: str) -> int:
    """
    ุนุฏ ุฅุฌูุงูู ุงูุฃุณุทุฑ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช (ูุดุฑูุท ุงูุชูุฏู).
    
    ุงููุนุงููุงุช:
    -----------
    dataset_dir : str
        ูุณุงุฑ ูุฌูุฏ ุงูุจูุงูุงุช
        
    ุชุฑุฌุน:
    --------
    int
        ุฅุฌูุงูู ุนุฏุฏ ุงูุฃุณุทุฑ
    """
    total = 0
    for file_path in sorted(Path(dataset_dir).glob("*.txt")):
        with open(file_path, "r", encoding="utf-8") as f:
            total += sum(1 for _ in f)
    return total


def get_sample_lines(dataset_dir: str, n: int = 1000, seed: int = 42) -> List[str]:
    """
    ุงูุญุตูู ุนูู ุนููุฉ ุนุดูุงุฆูุฉ ูู ุงูุฃุณุทุฑ ูููุญุต.
    
    ุงููุนุงููุงุช:
    -----------
    dataset_dir : str
        ูุณุงุฑ ูุฌูุฏ ุงูุจูุงูุงุช
    n : int
        ุนุฏุฏ ุงูุนููุงุช
    seed : int
        ุงูุจุฐุฑุฉ ุงูุนุดูุงุฆูุฉ
        
    ุชุฑุฌุน:
    --------
    List[str]
        ูุงุฆูุฉ ุฃุณุทุฑ ุงูุนููุฉ
    """
    random.seed(seed)
    
    # ุฌูุน ุงูุฃุณุทุฑ ูู ุงูุจุฏุงูุฉ ูุฃุฎุฐ ุงูุนููุงุช
    lines = []
    for i, line in enumerate(iter_dataset_lines(dataset_dir)):
        if i >= n * 10:  # ุฌูุน ุฃูุซุฑ ูู ุงููุทููุจ ููุงุฎุชูุงุฑ ุงูุนุดูุงุฆู
            break
        if line.strip():
            lines.append(line)
    
    return random.sample(lines, min(n, len(lines)))


logger.info("โ ุฃุฏูุงุช ุชุญููู ุงูุจูุงูุงุช ุฌุงูุฒุฉ!")

# %% [markdown]
# ---
# ## 2. ุงูุฌุฒุก 1: ูุญุต ุงููุดููุงุช (ูุจู ุงููุนุงูุฌุฉ)
# 
# ูุจู ุชุทุจูู ุฃู ูุนุงูุฌุฉุ ูุญุชุงุฌ ูุชุญุฏูุฏ ูููุงุณ ุฌููุน ุงููุดููุงุช ูู ุงูุจูุงูุงุช ุงูุฎุงู ุจุดูู ูููุฌู.
# ูุณุงุนุฏูุง ูุฐุง ูู:
# 
# 1. ููู ุญุฌู ูู ูุดููุฉ
# 2. ุชุฑุชูุจ ุฃููููุงุช ุฎุทูุงุช ุงููุนุงูุฌุฉ
# 3. ุงูุชุญูู ูู ุฃู ุงููุนุงูุฌุฉ ุฃุตูุญุช ุงููุดููุงุช

# %% [markdown]
# ### 2.1 ูุดููุงุช ูุณุชูู ุงูุญุฑูู

# %%
# ============================================================================
# ุงููุญุต 2.1: ูุดููุงุช ูุณุชูู ุงูุญุฑูู
# ============================================================================

def inspect_character_issues(dataset_dir: str, sample_size: int = 500000) -> Dict:
    """
    ูุญุต ูุดููุงุช ูุณุชูู ุงูุญุฑูู ูู ูุฌููุนุฉ ุงูุจูุงูุงุช.
    
    ูุชุญูู ูู:
    - ุงูุชุดููู (ุงูุญุฑูุงุช)
    - ุงูุญุฑูู ุฎุงุฑุฌ ุงููุทุงู (OOV)
    - ุงูุญุฑูู ุงููุงุชูููุฉ
    - ุงูุญุฑูู ุงูุฎุงุตุฉ
    - ุฃุดูุงู ุงูุฃูู
    - ุงูุชุทููู (ู)
    
    ุงููุนุงููุงุช:
    -----------
    dataset_dir : str
        ูุณุงุฑ ูุฌูุฏ ุงูุจูุงูุงุช
    sample_size : int
        ุนุฏุฏ ุงูุฃุณุทุฑ ูููุญุต
        
    ุชุฑุฌุน:
    --------
    Dict
        ูุงููุณ ูุญุชูู ุนูู ุฅุญุตุงุฆูุงุช ุงููุดููุงุช
    """
    logger.section("๐ ูุญุต ูุดููุงุช ูุณุชูู ุงูุญุฑูู")
    logger.info(f"ุฌุงุฑู ูุญุต {sample_size:,} ุณุทุฑ...")
    
    # ุชููุฆุฉ ุงูุนุฏุงุฏุงุช
    stats = {
        'total_chars': 0,
        'total_lines': 0,
        'diacritics': Counter(),
        'latin_letters': Counter(),
        'oov_chars': Counter(),
        'alef_variations': Counter(),
        'tatweel_count': 0,
        'lines_with_diacritics': 0,
        'lines_with_latin': 0,
        'lines_with_oov': 0,
    }
    
    # ุชุนุฑูู ูุฌููุนุฉ ุงูุญุฑูู ุงูุตุงูุญุฉ
    valid_chars = set()
    valid_chars.update(ARABIC_LETTERS_EXTENDED)
    valid_chars.update(ARABIC_NUMERALS)
    valid_chars.update(WESTERN_NUMERALS)
    valid_chars.update(VALID_PUNCTUATION)
    valid_chars.update(LATIN_TO_ARABIC_PUNCT.keys())
    valid_chars.update(' \t\n')  # ุงููุณุงูุงุช
    valid_chars.update('()[]{}ยซยป""\'-โโ')  # ุงูุฃููุงุณ ูุนูุงูุงุช ุงูุชูุตูุต
    valid_chars.update(ARABIC_DIACRITICS.keys())  # ุงูุชุดููู (ููุนุฏ)
    
    # ููุท ุงูุญุฑูู ุงููุงุชูููุฉ
    latin_pattern = re.compile(r'[A-Za-z]')
    
    # ุฅูุดุงุก ุงูููุฑุฑ (Iterator)
    iterator = iter_dataset_lines(dataset_dir)
    if TQDM_AVAILABLE:
        iterator = tqdm(iterator, total=sample_size, desc="ูุญุต ุงูุญุฑูู")
    
    for i, line in enumerate(iterator):
        if i >= sample_size:
            break
        
        stats['total_lines'] += 1
        stats['total_chars'] += len(line)
        
        has_diacritics = False
        has_latin = False
        has_oov = False
        
        for char in line:
            # ุงูุชุญูู ูู ุงูุชุดููู
            if char in ARABIC_DIACRITICS:
                stats['diacritics'][char] += 1
                has_diacritics = True
            
            # ุงูุชุญูู ูู ุงูุญุฑูู ุงููุงุชูููุฉ
            if latin_pattern.match(char):
                stats['latin_letters'][char] += 1
                has_latin = True
            
            # ุงูุชุญูู ูู ุฃุดูุงู ุงูุฃูู
            if char in ALEF_VARIATIONS:
                stats['alef_variations'][char] += 1
            
            # ุงูุชุญูู ูู ุงูุชุทููู
            if char == '\u0640':
                stats['tatweel_count'] += 1
            
            # ุงูุชุญูู ูู ุงูุญุฑูู ุฎุงุฑุฌ ุงููุทุงู
            if char not in valid_chars and not latin_pattern.match(char):
                stats['oov_chars'][char] += 1
                has_oov = True
        
        if has_diacritics:
            stats['lines_with_diacritics'] += 1
        if has_latin:
            stats['lines_with_latin'] += 1
        if has_oov:
            stats['lines_with_oov'] += 1
    
    # ุนุฑุถ ุงููุชุงุฆุฌ
    logger.subsection("ุงูุชุดููู (ุงูุญุฑูุงุช)")
    total_diacritics = sum(stats['diacritics'].values())
    logger.info(f"ุฅุฌูุงูู ุงูุชุดููู ุงูููุฌูุฏ: {total_diacritics:,}")
    logger.info(f"ุฃุณุทุฑ ุชุญุชูู ุนูู ุชุดููู: {stats['lines_with_diacritics']:,} ({stats['lines_with_diacritics']/stats['total_lines']*100:.2f}%)")
    
    if stats['diacritics']:
        logger.info("ุชูุฒูุน ุงูุชุดููู:")
        for char, count in stats['diacritics'].most_common():
            name = ARABIC_DIACRITICS.get(char, 'ุบูุฑ ูุนุฑูู')
            logger.info(f"   {repr(char)} ({name}): {count:,}")
    
    logger.subsection("ุงูุญุฑูู ุงููุงุชูููุฉ")
    total_latin = sum(stats['latin_letters'].values())
    logger.info(f"ุฅุฌูุงูู ุงูุญุฑูู ุงููุงุชูููุฉ: {total_latin:,}")
    logger.info(f"ุฃุณุทุฑ ุชุญุชูู ุนูู ูุงุชููู: {stats['lines_with_latin']:,} ({stats['lines_with_latin']/stats['total_lines']*100:.2f}%)")
    
    if stats['latin_letters']:
        logger.info("ุฃูุซุฑ ุงูุญุฑูู ุงููุงุชูููุฉ:")
        for char, count in stats['latin_letters'].most_common(10):
            logger.info(f"   '{char}': {count:,}")
    
    logger.subsection("ุฃุดูุงู ุงูุฃูู")
    total_alef_var = sum(stats['alef_variations'].values())
    logger.info(f"ุฅุฌูุงูู ุฃุดูุงู ุงูุฃูู: {total_alef_var:,}")
    
    if stats['alef_variations']:
        for char, count in stats['alef_variations'].most_common():
            logger.info(f"   '{char}': {count:,}")
    
    logger.subsection("ุงูุชุทููู (ุงููุดูุฏุฉ)")
    logger.info(f"ุนุฏุฏ ูุฑุงุช ุงูุชุทููู: {stats['tatweel_count']:,}")
    
    logger.subsection("ุญุฑูู ุฎุงุฑุฌ ุงููุทุงู (OOV)")
    total_oov = sum(stats['oov_chars'].values())
    logger.info(f"ุฅุฌูุงูู ุงูุญุฑูู ุฎุงุฑุฌ ุงููุทุงู: {total_oov:,}")
    logger.info(f"ุฃุณุทุฑ ุชุญุชูู ุนูู OOV: {stats['lines_with_oov']:,} ({stats['lines_with_oov']/stats['total_lines']*100:.2f}%)")
    
    if stats['oov_chars']:
        logger.info("ุฃูู ุงูุญุฑูู ุฎุงุฑุฌ ุงููุทุงู:")
        for char, count in stats['oov_chars'].most_common(20):
            try:
                char_name = f"U+{ord(char):04X}"
            except:
                char_name = "Unknown"
            logger.info(f"   {repr(char)} ({char_name}): {count:,}")
    
    return stats


# ุชุดุบูู ูุญุต ุงูุญุฑูู
char_issues = inspect_character_issues(config.input_dir, sample_size=500000)

# %% [markdown]
# ### 2.2 ูุดููุงุช ุนูุงูุงุช ุงูุชุฑููู

# %%
# ============================================================================
# ุงููุญุต 2.2: ูุดููุงุช ุนูุงูุงุช ุงูุชุฑููู
# ============================================================================

def inspect_punctuation_issues(dataset_dir: str, sample_size: int = 500000) -> Dict:
    """
    ูุญุต ุงููุดููุงุช ุงููุชุนููุฉ ุจุนูุงูุงุช ุงูุชุฑููู.
    
    ูุชุญูู ูู:
    - ุฎููุท ุงูุชุฑููู ุงูุนุฑุจู/ุงููุงุชููู
    - ุงูุชุฑููู ุงููุชุชุงุจุน (ูุซู ุุุ ุฃู ..)
    - ููุต ุงููุณุงูุงุช ุญูู ุงูุชุฑููู
    - ุนูุงูุงุช ุชุฑููู ุบูุฑ ุตุงูุญุฉ
    
    ุงููุนุงููุงุช:
    -----------
    dataset_dir : str
        ูุณุงุฑ ูุฌูุฏ ุงูุจูุงูุงุช
    sample_size : int
        ุนุฏุฏ ุงูุฃุณุทุฑ ูููุญุต
        
    ุชุฑุฌุน:
    --------
    Dict
        ูุงููุณ ูุญุชูู ุนูู ุฅุญุตุงุฆูุงุช ุงููุดููุงุช
    """
    logger.section("๐ ูุญุต ูุดููุงุช ุงูุชุฑููู")
    logger.info(f"ุฌุงุฑู ูุญุต {sample_size:,} ุณุทุฑ...")
    
    stats = {
        'total_lines': 0,
        'arabic_punct': Counter(),
        'latin_punct': Counter(),
        'other_punct': Counter(),
        'consecutive_punct': [],  # ูุชุฎุฒูู ุฃูุซูุฉ
        'consecutive_punct_count': 0,
        'attached_punct_count': 0,
        'lines_with_mixed_punct': 0,
    }
    
    # ุฌููุน ุนูุงูุงุช ุงูุชุฑููู ูููุดู
    all_punct = set(ARABIC_PUNCTUATION.keys()) | set(LATIN_TO_ARABIC_PUNCT.keys())
    
    # ููุท ุงูุชุฑููู ุงููุชุชุงุจุน
    consecutive_pattern = re.compile(r'[ุุุ.,:;?!]{2,}')
    
    # ููุท ุงูุชุฑููู ุงูููุชุตู (ุจุฏูู ูุณุงูุฉ ูุจูู/ุจุนุฏู)
    # ูููุฉ ุนุฑุจูุฉ ูุชุจูุนุฉ ูุจุงุดุฑุฉ ุจุชุฑููู ุจุฏูู ูุณุงูุฉ
    attached_pattern = re.compile(r'[\u0600-\u06FF][ุุุ.:!][\u0600-\u06FF]')
    
    iterator = iter_dataset_lines(dataset_dir)
    if TQDM_AVAILABLE:
        iterator = tqdm(iterator, total=sample_size, desc="ูุญุต ุงูุชุฑููู")
    
    for i, line in enumerate(iterator):
        if i >= sample_size:
            break
        
        stats['total_lines'] += 1
        
        has_arabic_punct = False
        has_latin_punct = False
        
        # ุนุฏ ุฃููุงุน ุงูุชุฑููู
        for char in line:
            if char in ARABIC_PUNCTUATION:
                stats['arabic_punct'][char] += 1
                has_arabic_punct = True
            elif char in LATIN_TO_ARABIC_PUNCT:
                stats['latin_punct'][char] += 1
                has_latin_punct = True
            elif char in '()[]{}ยซยป""\'':
                stats['other_punct'][char] += 1
        
        # ุงูุชุญูู ูู ุงูุชุฑููู ุงููุฎุชูุท
        if has_arabic_punct and has_latin_punct:
            stats['lines_with_mixed_punct'] += 1
        
        # ุงูุชุญูู ูู ุงูุชุฑููู ุงููุชุชุงุจุน
        consecutive_matches = consecutive_pattern.findall(line)
        if consecutive_matches:
            stats['consecutive_punct_count'] += len(consecutive_matches)
            if len(stats['consecutive_punct']) < 20:  # ุชุฎุฒูู ุฃูุซูุฉ
                for match in consecutive_matches:
                    stats['consecutive_punct'].append((match, line[:100]))
        
        # ุงูุชุญูู ูู ุงูุชุฑููู ุงูููุชุตู
        attached_matches = attached_pattern.findall(line)
        if attached_matches:
            stats['attached_punct_count'] += len(attached_matches)
    
    # ุนุฑุถ ุงููุชุงุฆุฌ
    logger.subsection("ุงูุชุฑููู ุงูุนุฑุจู")
    for char, count in stats['arabic_punct'].most_common():
        name = ARABIC_PUNCTUATION.get(char, 'ุบูุฑ ูุนุฑูู')
        logger.info(f"   '{char}' ({name}): {count:,}")
    
    logger.subsection("ุงูุชุฑููู ุงููุงุชููู (ูุญุชุงุฌ ุชูุญูุฏ)")
    for char, count in stats['latin_punct'].most_common():
        logger.info(f"   '{char}': {count:,} -> ูุฌุจ ุฃู ูุตุจุญ '{LATIN_TO_ARABIC_PUNCT.get(char, char)}'")
    
    logger.subsection("ุฃุณุทุฑ ุจุชุฑููู ูุฎุชูุท")
    logger.info(f"ุฃุณุทุฑ ุชุญูู ุฎููุทุงู ูู ุงูุชุฑููู ุงูุนุฑุจู ูุงููุงุชููู: {stats['lines_with_mixed_punct']:,}")
    logger.info(f"ุงููุณุจุฉ: {stats['lines_with_mixed_punct']/stats['total_lines']*100:.2f}%")
    
    logger.subsection("ุงูุชุฑููู ุงููุชุชุงุจุน")
    logger.info(f"ุฅุฌูุงูู ุงูุญุงูุงุช: {stats['consecutive_punct_count']:,}")
    if stats['consecutive_punct']:
        logger.info("ุฃูุซูุฉ:")
        for match, context in stats['consecutive_punct'][:5]:
            logger.info(f"   '{match}' ูู: {context[:60]}...")
    
    logger.subsection("ุงูุชุฑููู ุงูููุชุตู")
    logger.info(f"ุญุงูุงุช ููุชูุฏ ูููุง ุงูุชุฑููู ููุณุงูุงุช ููุงุณุจุฉ: {stats['attached_punct_count']:,}")
    
    return stats


# ุชุดุบูู ูุญุต ุงูุชุฑููู
punct_issues = inspect_punctuation_issues(config.input_dir, sample_size=500000)

# %% [markdown]
# ### 2.3 ูุดููุงุช ูุณุชูู ุงูุฌูู

# %%
# ============================================================================
# ุงููุญุต 2.3: ูุดููุงุช ูุณุชูู ุงูุฌูู
# ============================================================================

def inspect_sentence_issues(dataset_dir: str, sample_size: int = 1000000) -> Dict:
    """
    ูุญุต ูุดููุงุช ูุณุชูู ุงูุฌูู.
    
    ูุชุญูู ูู:
    - ุงูุฃุณุทุฑ ุงููุงุฑุบุฉ
    - ุงูุฌูู ุงููุตูุฑุฉ ุฌุฏุงู
    - ุงูุฌูู ุงูุทูููุฉ ุฌุฏุงู
    - ุบูุงุจ ุนูุงูุงุช ููุงูุฉ ุงูุฌููุฉ
    - ุฌูู ูุชุนุฏุฏุฉ ูู ุณุทุฑ ูุงุญุฏ
    
    ุงููุนุงููุงุช:
    -----------
    dataset_dir : str
        ูุณุงุฑ ูุฌูุฏ ุงูุจูุงูุงุช
    sample_size : int
        ุนุฏุฏ ุงูุฃุณุทุฑ ูููุญุต
        
    ุชุฑุฌุน:
    --------
    Dict
        ูุงููุณ ูุญุชูู ุนูู ุฅุญุตุงุฆูุงุช ุงููุดููุงุช
    """
    logger.section("๐ ูุญุต ูุดููุงุช ูุณุชูู ุงูุฌูู")
    logger.info(f"ุฌุงุฑู ูุญุต {sample_size:,} ุณุทุฑ...")
    
    stats = {
        'total_lines': 0,
        'empty_lines': 0,
        'very_short': [],  # < 3 ูููุงุช
        'very_long': [],   # > 100 ูููุฉ
        'word_counts': [],
        'missing_terminal': 0,
        'wrong_terminal': Counter(),
        'multiple_terminals': 0,
    }
    
    iterator = iter_dataset_lines(dataset_dir)
    if TQDM_AVAILABLE:
        iterator = tqdm(iterator, total=sample_size, desc="ูุญุต ุงูุฌูู")
    
    for i, line in enumerate(iterator):
        if i >= sample_size:
            break
        
        stats['total_lines'] += 1
        
        # ุงูุชุญูู ูู ุงููุฑุงุบ
        stripped = line.strip()
        if not stripped:
            stats['empty_lines'] += 1
            continue
        
        # ุนุฏ ุงููููุงุช
        words = stripped.split()
        word_count = len(words)
        stats['word_counts'].append(word_count)
        
        # ุงูุชุญูู ูู ุงููุตุฑ ุงูุดุฏูุฏ
        if word_count < 3:
            if len(stats['very_short']) < 20:  # ุชุฎุฒูู ุฃูุซูุฉ
                stats['very_short'].append(stripped)
        
        # ุงูุชุญูู ูู ุงูุทูู ุงูุดุฏูุฏ
        if word_count > 100:
            if len(stats['very_long']) < 10:
                stats['very_long'].append((word_count, stripped[:100] + "..."))
        
        # ุงูุชุญูู ูู ุนูุงูุฉ ุงูููุงูุฉ
        if stripped:
            last_char = stripped[-1]
            if last_char not in SENTENCE_TERMINALS:
                stats['missing_terminal'] += 1
                stats['wrong_terminal'][last_char] += 1
        
        # ุงูุชุญูู ูู ูุฌูุฏ ุนูุงูุงุช ููุงูุฉ ูุชุนุฏุฏุฉ ุฏุงุฎู ุงูุณุทุฑ
        terminal_count = sum(1 for c in stripped[:-1] if c in SENTENCE_TERMINALS)
        if terminal_count > 0:
            stats['multiple_terminals'] += 1
    
    # ุนุฑุถ ุงููุชุงุฆุฌ
    logger.subsection("ุงูุฃุณุทุฑ ุงููุงุฑุบุฉ")
    logger.info(f"ุฃุณุทุฑ ูุงุฑุบุฉ: {stats['empty_lines']:,} ({stats['empty_lines']/stats['total_lines']*100:.4f}%)")
    
    logger.subsection("ุฅุญุตุงุฆูุงุช ุทูู ุงูุฌูู")
    if stats['word_counts']:
        word_arr = np.array(stats['word_counts'])
        logger.info(f"ุงููุชูุณุท: {np.mean(word_arr):.2f} ูููุฉ")
        logger.info(f"ุงููุณูุท: {np.median(word_arr):.2f} ูููุฉ")
        logger.info(f"ุงูุฃุฏูู: {np.min(word_arr)} ูููุงุช")
        logger.info(f"ุงูุฃูุตู: {np.max(word_arr)} ูููุงุช")
        logger.info(f"ุงูุงูุญุฑุงู ุงููุนูุงุฑู: {np.std(word_arr):.2f} ูููุฉ")
    
    logger.subsection("ุงูุฌูู ุงููุตูุฑุฉ ุฌุฏุงู (<3 ูููุงุช)")
    short_count = sum(1 for w in stats['word_counts'] if w < 3)
    logger.info(f"ุงูุนุฏุฏ: {short_count:,} ({short_count/stats['total_lines']*100:.2f}%)")
    if stats['very_short']:
        logger.info("ุฃูุซูุฉ:")
        for example in stats['very_short'][:5]:
            logger.info(f"   '{example}'")
    
    logger.subsection("ุงูุฌูู ุงูุทูููุฉ ุฌุฏุงู (>100 ูููุฉ)")
    long_count = sum(1 for w in stats['word_counts'] if w > 100)
    logger.info(f"ุงูุนุฏุฏ: {long_count:,} ({long_count/stats['total_lines']*100:.2f}%)")
    if stats['very_long']:
        logger.info("ุฃูุซูุฉ:")
        for wc, example in stats['very_long'][:3]:
            logger.info(f"   [{wc} ูููุฉ] {example}")
    
    logger.subsection("ูุดููุงุช ุนูุงูุงุช ุงูููุงูุฉ")
    logger.info(f"ุฃุณุทุฑ ุชูุชูุฏ ูุนูุงูุฉ ููุงูุฉ ููุงุณูุฉ: {stats['missing_terminal']:,}")
    if stats['wrong_terminal']:
        logger.info("ููุงูุงุช ุบูุฑ ููุงุณูุฉ ูุฌุฏุช:")
        for char, count in stats['wrong_terminal'].most_common(10):
            logger.info(f"   '{char}': {count:,}")
    
    logger.subsection("ุนูุงูุงุช ููุงูุฉ ูุชุนุฏุฏุฉ")
    logger.info(f"ุฃุณุทุฑ ุชุญุชูู ุนูู ุนูุงูุงุช ููุงูุฉ ูู ูุณุท ุงูุฌููุฉ: {stats['multiple_terminals']:,}")
    
    return stats


# ุชุดุบูู ูุญุต ุงูุฌูู
sentence_issues = inspect_sentence_issues(config.input_dir, sample_size=1000000)

# %% [markdown]
# ### 2.4 ูุดููุงุช ุงูุฃููุงุท ุงูุฎุงุตุฉ

# %%
# ============================================================================
# ุงููุญุต 2.4: ูุดููุงุช ุงูุฃููุงุท ุงูุฎุงุตุฉ
# ============================================================================

def inspect_special_patterns(dataset_dir: str, sample_size: int = 500000) -> Dict:
    """
    ูุญุต ุงูุฃููุงุท ุงูุฎุงุตุฉ ุงูุชู ูุฏ ุชุญุชุงุฌ ููุนุงูุฌุฉ.
    
    ูุชุญูู ูู:
    - ูุงู ุงูุนุทู ุงููุชุตูุฉ ุจุงููููุงุช
    - ุงูุฃุฑูุงู (ุงูุนุฑุจูุฉ ูุงูุบุฑุจูุฉ)
    - ูุฑุงุฌุน ุงููุณุชูุฏุงุช (ูุซู A/47/10)
    - ุฃููุงุท ุงูุฑูุงุจุท ุฃู ุงูุจุฑูุฏ ุงูุฅููุชุฑููู
    - ุงูุญุฑูู ุงูููุฑุฑุฉ
    
    ุงููุนุงููุงุช:
    -----------
    dataset_dir : str
        ูุณุงุฑ ูุฌูุฏ ุงูุจูุงูุงุช
    sample_size : int
        ุนุฏุฏ ุงูุฃุณุทุฑ ูููุญุต
        
    ุชุฑุฌุน:
    --------
    Dict
        ูุงููุณ ูุญุชูู ุนูู ุฅุญุตุงุฆูุงุช ุงูุฃููุงุท
    """
    logger.section("๐ ูุญุต ุงูุฃููุงุท ุงูุฎุงุตุฉ")
    logger.info(f"ุฌุงุฑู ูุญุต {sample_size:,} ุณุทุฑ...")
    
    stats = {
        'total_lines': 0,
        'waw_attached': Counter(),  # ูููุงุช ุชุจุฏุฃ ุจู ู
        'arabic_numbers': 0,
        'western_numbers': 0,
        'mixed_number_lines': 0,
        'doc_references': [],
        'repeated_chars': [],
        'foreign_words': Counter(),
    }
    
    # ุงูุฃููุงุท (Patterns)
    waw_word_pattern = re.compile(r'\bู[\u0600-\u06FF]{2,}\b')
    arabic_num_pattern = re.compile(r'[ู-ูฉ]+')
    western_num_pattern = re.compile(r'[0-9]+')
    doc_ref_pattern = re.compile(r'[A-Z]/[0-9]+(?:/[0-9A-Z]+)*')
    repeated_pattern = re.compile(r'(.)\1{3,}')  # ููุณ ุงูุญุฑู 4 ูุฑุงุช ุฃู ุฃูุซุฑ
    foreign_word_pattern = re.compile(r'\b[A-Za-z]{3,}\b')  # 3+ ุญุฑูู ูุงุชูููุฉ
    
    iterator = iter_dataset_lines(dataset_dir)
    if TQDM_AVAILABLE:
        iterator = tqdm(iterator, total=sample_size, desc="ูุญุต ุงูุฃููุงุท")
    
    for i, line in enumerate(iterator):
        if i >= sample_size:
            break
        
        stats['total_lines'] += 1
        
        # ุงููููุงุช ุงููุชุตูุฉ ุจุงููุงู
        waw_matches = waw_word_pattern.findall(line)
        for match in waw_matches:
            stats['waw_attached'][match] += 1
        
        # ุฃูุธูุฉ ุงูุฃุฑูุงู
        has_arabic = bool(arabic_num_pattern.search(line))
        has_western = bool(western_num_pattern.search(line))
        
        if has_arabic:
            stats['arabic_numbers'] += len(arabic_num_pattern.findall(line))
        if has_western:
            stats['western_numbers'] += len(western_num_pattern.findall(line))
        if has_arabic and has_western:
            stats['mixed_number_lines'] += 1
        
        # ูุฑุงุฌุน ุงููุณุชูุฏุงุช
        doc_refs = doc_ref_pattern.findall(line)
        if doc_refs and len(stats['doc_references']) < 20:
            stats['doc_references'].extend(doc_refs[:2])
        
        # ุงูุญุฑูู ุงูููุฑุฑุฉ
        repeated = repeated_pattern.findall(line)
        if repeated and len(stats['repeated_chars']) < 10:
            stats['repeated_chars'].append(line[:80])
        
        # ุงููููุงุช ุงูุฃุฌูุจูุฉ
        foreign = foreign_word_pattern.findall(line)
        for word in foreign:
            stats['foreign_words'][word] += 1
    
    # ุนุฑุถ ุงููุชุงุฆุฌ
    logger.subsection("ุฃููุงุท ูุงู ุงูุนุทู")
    logger.info(f"ุฅุฌูุงูู ุงููููุงุช ุงูุชู ุชุจุฏุฃ ุจู ู: {sum(stats['waw_attached'].values()):,}")
    logger.info("ุฃูุซุฑ ุงููููุงุช ุดููุนุงู ูุน ุงููุงู:")
    for word, count in stats['waw_attached'].most_common(15):
        logger.info(f"   '{word}': {count:,}")
    
    logger.subsection("ุฃูุธูุฉ ุงูุฃุฑูุงู")
    logger.info(f"ุฃุฑูุงู ุนุฑุจูุฉ ูุฌุฏุช: {stats['arabic_numbers']:,}")
    logger.info(f"ุฃุฑูุงู ุบุฑุจูุฉ ูุฌุฏุช: {stats['western_numbers']:,}")
    logger.info(f"ุฃุณุทุฑ ุจุฃุฑูุงู ูุฎุชูุทุฉ: {stats['mixed_number_lines']:,}")
    
    logger.subsection("ูุฑุงุฌุน ุงููุณุชูุฏุงุช")
    logger.info(f"ุฃูุซูุฉ: {stats['doc_references'][:10]}")
    
    logger.subsection("ุญุฑูู ููุฑุฑุฉ")
    if stats['repeated_chars']:
        logger.info("ุฃุณุทุฑ ุจุญุฑูู ููุฑุฑุฉ:")
        for example in stats['repeated_chars'][:3]:
            logger.info(f"   {example}")
    else:
        logger.info("ูู ูุชู ุงูุนุซูุฑ ุนูู ุฃููุงุท ุญุฑูู ููุฑุฑุฉ ูููุฉ")
    
    logger.subsection("ูููุงุช ุฃุฌูุจูุฉ")
    logger.info(f"ูููุงุช ุฃุฌูุจูุฉ ูุฑูุฏุฉ: {len(stats['foreign_words']):,}")
    logger.info("ุฃูุซุฑ ุงููููุงุช ุงูุฃุฌูุจูุฉ ุดููุนุงู:")
    for word, count in stats['foreign_words'].most_common(15):
        logger.info(f"   '{word}': {count:,}")
    
    return stats


# ุชุดุบูู ูุญุต ุงูุฃููุงุท ุงูุฎุงุตุฉ
special_issues = inspect_special_patterns(config.input_dir, sample_size=500000)

# %% [markdown]
# ---
# ## 3. ุงูุฌุฒุก 2: ุฎุทูุงุช ุงููุนุงูุฌุฉ ุงูุฅูุฒุงููุฉ
# 
# ูุฐู ุงูุฎุทูุงุช **ุชูุทุจู ุฏุงุฆูุงู** ูุฃููุง ุชุนุงูุฌ ูุดููุงุช ุฃุณุงุณูุฉ ูู ุฌูุฏุฉ ุงูุจูุงูุงุช
# ูุงูุชู ุณุชุคุซุฑ ุณูุจุงู ุนูู ุชุฏุฑูุจ ุงููููุฐุฌ.

# %% [markdown]
# ### 3.1 ุฅุฒุงูุฉ ุงูุชุดููู (ุงูุญุฑูุงุช)

# %%
# ============================================================================
# ุงููุนุงูุฌุฉ 3.1: ุฅุฒุงูุฉ ุงูุชุดููู
# ============================================================================

def remove_diacritics(text: str) -> str:
    """
    ุฅุฒุงูุฉ ุงูุชุดููู (ุงูุญุฑูุงุช) ูู ุงููุต ุงูุนุฑุจู.
    
    ุงูุชุดููู ุงูุฐู ูุชู ุฅุฒุงูุชู:
    - ูุชุญุชุงู (ู)ุ ุถูุชุงู (ู)ุ ูุณุฑุชุงู (ู)
    - ูุชุญุฉ (ู)ุ ุถูุฉ (ู)ุ ูุณุฑุฉ (ู)
    - ุดุฏุฉ (ู)ุ ุณููู (ู)
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู ูุน ุงุญุชูุงู ูุฌูุฏ ุชุดููู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ุจุฏูู ุชุดููู
        
    ูุซุงู:
    --------
    >>> remove_diacritics("ุงููุนูุฑูุจููููุฉ")
    'ุงูุนุฑุจูุฉ'
    """
    return DIACRITICS_PATTERN.sub('', text)


# ุงุฎุชุจุงุฑ ุงูุฏุงูุฉ
logger.section("๐ง ุงููุนุงูุฌุฉ ุงููุณุจูุฉ: ุฅุฒุงูุฉ ุงูุชุดููู")

test_cases = [
    "ุงููุนูุฑูุจููููุฉ",
    "ููุญููููุฏ",
    "ุงูุฃููููู ุงูููุชููุญูุฏูุฉ",
    "ูุต ุจุฏูู ุชุดููู",
]

logger.info("ุญุงูุงุช ุงูุงุฎุชุจุงุฑ:")
for test in test_cases:
    result = remove_diacritics(test)
    logger.info(f"   '{test}' -> '{result}'")

# %% [markdown]
# ### 3.2 ุชูุญูุฏ ุฃุดูุงู ุงูุฃูู

# %%
# ============================================================================
# ุงููุนุงูุฌุฉ 3.2: ุชูุญูุฏ ุฃุดูุงู ุงูุฃูู
# ============================================================================

# ุชุฌููุน ุงูููุท ููููุงุกุฉ
ALEF_PATTERN = re.compile(r'[ุฃุฅุขูฑ]')

def normalize_alef(text: str) -> str:
    """
    ุชูุญูุฏ ุฌููุน ุฃุดูุงู ุงูุฃูู ุฅูู ุฃูู ูุฌุฑุฏุฉ (ุง).
    
    ุงูุชุญูููุงุช:
    - ุฃ (ููุฒุฉ ููู) -> ุง
    - ุฅ (ููุฒุฉ ุชุญุช) -> ุง
    - ุข (ูุฏุฉ) -> ุง
    - ูฑ (ูุตู) -> ุง
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ูุน ุฃูู ููุญุฏุฉ
        
    ูุซุงู:
    --------
    >>> normalize_alef("ุฃุญูุฏ ุฅุจุฑุงููู ุขุฏู")
    'ุงุญูุฏ ุงุจุฑุงููู ุงุฏู'
    """
    return ALEF_PATTERN.sub('ุง', text)


# ุงุฎุชุจุงุฑ ุงูุฏุงูุฉ
logger.section("๐ง ุงููุนุงูุฌุฉ ุงููุณุจูุฉ: ุชูุญูุฏ ุงูุฃูู")

test_cases = [
    "ุฃุญูุฏ",
    "ุฅุจุฑุงููู",
    "ุขุฏู",
    "ุงูุฃูู",
    "ุงูุฅูุณุงู",
]

logger.info("ุญุงูุงุช ุงูุงุฎุชุจุงุฑ:")
for test in test_cases:
    result = normalize_alef(test)
    logger.info(f"   '{test}' -> '{result}'")

# %% [markdown]
# ### 3.3 ุชูุญูุฏ ุงูุชุงุก ุงููุฑุจูุทุฉ ูุงูุฃูู ุงูููุตูุฑุฉ

# %%
# ============================================================================
# ุงููุนุงูุฌุฉ 3.3: ุชูุญูุฏ ุงูุชุงุก ุงููุฑุจูุทุฉ ูุงูุฃูู ุงูููุตูุฑุฉ
# ============================================================================

def normalize_teh_marbuta(text: str, to_heh: bool = False) -> str:
    """
    ูุนุงูุฌุฉ ุงูุชุงุก ุงููุฑุจูุทุฉ (ุฉ).
    
    ุฎูุงุฑุงุช:
    - ุฅุจูุงุคูุง ููุง ูู (ุงูุงูุชุฑุงุถู ููุฐู ุงููููุฉ)
    - ุชุญููููุง ุฅูู ูุงุก (ู) - ุจุนุถ ุชุทุจููุงุช NLP ุชูุนู ุฐูู
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
    to_heh : bool
        ุฅุฐุง ูุงู Trueุ ุญูู ุฉ ุฅูู ู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ุงููุนุงูุฌ
    """
    if to_heh:
        return text.replace('ุฉ', 'ู')
    return text


def normalize_alef_maksura(text: str) -> str:
    """
    ุชูุญูุฏ ุงูุฃูู ุงูููุตูุฑุฉ (ู) ุฅูู ูุงุก (ู).
    
    ููุงุญุธุฉ: ูู ุจุนุถ ุงูุณูุงูุงุชุ ูุชู ุงูุชูููุฒ ุจููููุง. ูุชูุจุค ุงูุชุฑูููุ
    ุงูุชูุญูุฏ ูุญุณู ุงูุงุชุณุงู.
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ูุน ู -> ู
        
    ูุซุงู:
    --------
    >>> normalize_alef_maksura("ุนูู")
    'ุนูู'
    """
    return text.replace('ู', 'ู')


# ุงุฎุชุจุงุฑ ุงูุฏูุงู
logger.section("๐ง ุงููุนุงูุฌุฉ ุงููุณุจูุฉ: ุชูุญูุฏ ุงูุชุงุก ุงููุฑุจูุทุฉ ูุงูุฃูู ุงูููุตูุฑุฉ")

test_cases = [
    ("ูุฏุฑุณุฉ", "ุชุงุก ูุฑุจูุทุฉ"),
    ("ุนูู", "ุฃูู ููุตูุฑุฉ"),
    ("ูุณุชุดูู", "ุฃูู ููุตูุฑุฉ"),
    ("ุงููุงูุฑุฉ", "ุชุงุก ูุฑุจูุทุฉ"),
]

logger.info("ุญุงูุงุช ุงูุงุฎุชุจุงุฑ:")
for test, note in test_cases:
    result_tm = normalize_teh_marbuta(test, to_heh=False)
    result_am = normalize_alef_maksura(test)
    logger.info(f"   '{test}' ({note})")
    logger.info(f"      ุชุงุก ูุฑุจูุทุฉ (ุฅุจูุงุก): '{result_tm}'")
    logger.info(f"      ุฃูู ููุตูุฑุฉ -> ูุงุก: '{result_am}'")

# %% [markdown]
# ### 3.4 ุฅุฒุงูุฉ ุงูุญุฑูู ุฎุงุฑุฌ ุงููุทุงู (OOV)

# %%
# ============================================================================
# ุงููุนุงูุฌุฉ 3.4: ุฅุฒุงูุฉ ุงูุญุฑูู ุฎุงุฑุฌ ุงููุทุงู (OOV)
# ============================================================================

def build_valid_charset() -> set:
    """
    ุจูุงุก ูุฌููุนุฉ ุงูุญุฑูู ุงูุตุงูุญุฉ ููููุฉ ุงูุชุฑููู ุงูุนุฑุจู.
    
    ุชุดูู:
    - ุงูุญุฑูู ุงูุนุฑุจูุฉ (ุงูุฃุณุงุณูุฉ + ุงูููุณุนุฉ)
    - ุงูุฃุฑูุงู ุงูุนุฑุจูุฉ
    - ุนูุงูุงุช ุงูุชุฑููู ุงูุตุงูุญุฉ
    - ุงููุณุงูุงุช
    
    ุชุฑุฌุน:
    --------
    set
        ูุฌููุนุฉ ุงูุญุฑูู ุงูุตุงูุญุฉ
    """
    valid = set()
    
    # ุงูุญุฑูู ุงูุนุฑุจูุฉ (ุฃุณุงุณูุฉ + ููุณุนุฉ)
    valid.update('ุกุขุฃุคุฅุฆุงุจุฉุชุซุฌุญุฎุฏุฐุฑุฒุณุดุตุถุทุธุนุบููููููููู')
    valid.update('ููพฺฺฺฏฺค')  # ููุณุนุฉ
    
    # ุงูุฃุฑูุงู ุงูุนุฑุจูุฉ
    valid.update('ููกูขูฃูคูฅูฆูงูจูฉ')
    
    # ุนูุงูุงุช ุงูุชุฑููู ุงูุตุงูุญุฉ
    valid.update('ุุุ.:!')
    
    # ุญุฑูู ูููููุฉ ุฃุณุงุณูุฉ
    valid.update(' ')  # ูุณุงูุฉ
    
    # ุฅุจูุงุก ุจุนุถ ุงูุฃููุงุณ ููููููุฉ (ุณุชุนุงูุฌ ูุงุญูุงู ุฅุฐุง ูุฒู)
    valid.update('()[]')
    
    return valid


VALID_CHARSET = build_valid_charset()


def remove_oov_characters(text: str, valid_chars: set = None, replacement: str = '') -> str:
    """
    ุฅุฒุงูุฉ ุงูุญุฑูู ุบูุฑ ุงูููุฌูุฏุฉ ูู ูุฌููุนุฉ ุงูุญุฑูู ุงูุตุงูุญุฉ.
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
    valid_chars : set
        ูุฌููุนุฉ ุงูุญุฑูู ุงูุตุงูุญุฉ (ูุณุชุฎุฏู VALID_CHARSET ุฅุฐุง None)
    replacement : str
        ุงูุญุฑู ุงูุจุฏูู ููุญุฑูู ุงููุญุฐููุฉ (ุงูุงูุชุฑุงุถู: ุฅุฒุงูุฉ)
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ุจุนุฏ ุฅุฒุงูุฉ ุงูุญุฑูู ุบูุฑ ุงูุตุงูุญุฉ
    """
    if valid_chars is None:
        valid_chars = VALID_CHARSET
    
    result = []
    for char in text:
        if char in valid_chars:
            result.append(char)
        elif replacement:
            result.append(replacement)
    
    return ''.join(result)


# ุงุฎุชุจุงุฑ ุงูุฏุงูุฉ
logger.section("๐ง ุงููุนุงูุฌุฉ ุงููุณุจูุฉ: ุฅุฒุงูุฉ ุงูุญุฑูู ุฎุงุฑุฌ ุงููุทุงู")

test_cases = [
    "ุงููุต ุงูุนุฑุจู ูุน English text",
    "ุฑูู: ูกูขูฃ ู 456",
    "ูุน ุฑููุฒ ุฎุงุตุฉ: @#$%",
    "ยซูุต ุจูู ุฃููุงุณยป",
]

logger.info(f"ุญุฌู ูุฌููุนุฉ ุงูุญุฑูู ุงูุตุงูุญุฉ: {len(VALID_CHARSET)}")
logger.info("ุญุงูุงุช ุงูุงุฎุชุจุงุฑ:")
for test in test_cases:
    result = remove_oov_characters(test)
    logger.info(f"   '{test}'")
    logger.info(f"   -> '{result}'")

# %% [markdown]
# ### 3.5 ุฅุฒุงูุฉ ุงูุญุฑูู ุงููุงุชูููุฉ

# %%
# ============================================================================
# ุงููุนุงูุฌุฉ 3.5: ุฅุฒุงูุฉ ุงูุญุฑูู ุงููุงุชูููุฉ
# ============================================================================

LATIN_PATTERN = re.compile(r'[A-Za-z]+')

def remove_latin_letters(text: str, replacement: str = '') -> str:
    """
    ุฅุฒุงูุฉ ุงูุญุฑูู ุงููุงุชูููุฉ ูู ุงููุต.
    
    ูุนุงูุฌ:
    - ุงููููุงุช ุงูุฅูุฌููุฒูุฉ ุงูููุฑุฏุฉ
    - ูุฑุงุฌุน ุงููุณุชูุฏุงุช (A/47/10 -> /47/10)
    - ุงููุต ุงููุฎุชูุท
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
    replacement : str
        ุจุฏูู ููุชุชุงุจุนุงุช ุงููุงุชูููุฉ (ุงูุงูุชุฑุงุถู: ุฅุฒุงูุฉ)
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ุจุฏูู ุญุฑูู ูุงุชูููุฉ
    """
    return LATIN_PATTERN.sub(replacement, text)


# ุงุฎุชุจุงุฑ ุงูุฏุงูุฉ
logger.section("๐ง ุงููุนุงูุฌุฉ ุงููุณุจูุฉ: ุฅุฒุงูุฉ ุงูุญุฑูู ุงููุงุชูููุฉ")

test_cases = [
    "ุงูุฃูู ุงููุชุญุฏุฉ United Nations",
    "ุงููุซููุฉ A/47/10",
    "ุจุฑูุงูุฌ UNDP ููุชูููุฉ",
    "add. 1",
]

logger.info("ุญุงูุงุช ุงูุงุฎุชุจุงุฑ:")
for test in test_cases:
    result = remove_latin_letters(test)
    logger.info(f"   '{test}' -> '{result}'")

# %% [markdown]
# ### 3.6 ุชูุญูุฏ ุงูุฃุฑูุงู (ุงูุฃุฑูุงู ุงูุนุฑุจูุฉ)

# %%
# ============================================================================
# ุงููุนุงูุฌุฉ 3.6: ุชูุญูุฏ ุงูุฃุฑูุงู ููุนุฑุจูุฉ
# ============================================================================

def unify_numbers_to_arabic(text: str) -> str:
    """
    ุชุญููู ุฌููุน ุงูุฃุฑูุงู ุงูุบุฑุจูุฉ (0-9) ุฅูู ุฃุฑูุงู ุนุฑุจูุฉ (ู-ูฉ).
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ุจุฃุฑูุงู ุนุฑุจูุฉ ููุท
        
    ูุซุงู:
    --------
    >>> unify_numbers_to_arabic("ุนุงู 2024")
    'ุนุงู ูขููขูค'
    """
    return text.translate(WESTERN_TO_ARABIC_NUMS)


def unify_numbers_to_western(text: str) -> str:
    """
    ุชุญููู ุฌููุน ุงูุฃุฑูุงู ุงูุนุฑุจูุฉ (ู-ูฉ) ุฅูู ุฃุฑูุงู ุบุฑุจูุฉ (0-9).
    
    ููุฌ ุจุฏูู - ุจุนุถ ุงูููุงุฐุฌ ุชูุถู ุงูุฃุฑูุงู ุงูุบุฑุจูุฉ.
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ุจุฃุฑูุงู ุบุฑุจูุฉ ููุท
    """
    return text.translate(ARABIC_TO_WESTERN_NUMS)


# ุงุฎุชุจุงุฑ ุงูุฏุงูุฉ
logger.section("๐ง ุงููุนุงูุฌุฉ ุงููุณุจูุฉ: ุชูุญูุฏ ุงูุฃุฑูุงู")

test_cases = [
    "ุนุงู 2024",
    "ุฑูู ูกูขูฃ",
    "ูุจูุบ 100 ุฏููุงุฑ",
    "ูฅูู + 500 = ูกููู",
]

logger.info("ุญุงูุงุช ุงูุงุฎุชุจุงุฑ (-> ุฃุฑูุงู ุนุฑุจูุฉ):")
for test in test_cases:
    result = unify_numbers_to_arabic(test)
    logger.info(f"   '{test}' -> '{result}'")

# %% [markdown]
# ### 3.7 ุชูุญูุฏ ุนูุงูุงุช ุงูุชุฑููู (ุงูุชุฑููู ุงูุนุฑุจู)

# %%
# ============================================================================
# ุงููุนุงูุฌุฉ 3.7: ุชูุญูุฏ ุงูุชุฑููู ููุนุฑุจู
# ============================================================================

def unify_punctuation_to_arabic(text: str) -> str:
    """
    ุชุญููู ุนูุงูุงุช ุงูุชุฑููู ุงููุงุชูููุฉ ุฅูู ูุธูุฑุงุชูุง ุงูุนุฑุจูุฉ.
    
    ุงูุชุญูููุงุช:
    - , -> ุ  (ูุงุตูุฉ)
    - ; -> ุ  (ูุงุตูุฉ ููููุทุฉ)
    - ? -> ุ  (ุนูุงูุฉ ุงุณุชููุงู)
    
    ููุงุญุธุฉ: ุงูููุทุฉ (.)ุ ุงูููุทุชุงู (:)ุ ูุงูุชุนุฌุจ (!) ุชุจูู ููุง ูู
    ูุฃููุง ูุดุชุฑูุฉ ูู ุงููุธุงููู.
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ุจุชุฑููู ุนุฑุจู
    """
    for latin, arabic in LATIN_TO_ARABIC_PUNCT.items():
        text = text.replace(latin, arabic)
    return text


# ุงุฎุชุจุงุฑ ุงูุฏุงูุฉ
logger.section("๐ง ุงููุนุงูุฌุฉ ุงููุณุจูุฉ: ุชูุญูุฏ ุงูุชุฑููู")

test_cases = [
    "ุฃููุงู, ุซุงููุงู, ุซุงูุซุงู",
    "ูู ูุฐุง ุตุญูุญ?",
    "ููุงุญุธุฉ; ูุฐุง ููู",
    "ุงููุต ูุญุชูู ุนูู , ู ; ู ?",
]

logger.info("ุญุงูุงุช ุงูุงุฎุชุจุงุฑ:")
for test in test_cases:
    result = unify_punctuation_to_arabic(test)
    logger.info(f"   '{test}'")
    logger.info(f"   -> '{result}'")

# %% [markdown]
# ### 3.8 ูุนุงูุฌุฉ ุงูุชุฑููู ุงููุชุชุงุจุน

# %%
# ============================================================================
# ุงููุนุงูุฌุฉ 3.8: ูุนุงูุฌุฉ ุงูุชุฑููู ุงููุชุชุงุจุน
# ============================================================================

def handle_consecutive_punctuation(text: str, keep_first: bool = True) -> str:
    """
    ูุนุงูุฌุฉ ุนูุงูุงุช ุงูุชุฑููู ุงููุชุชุงุจุนุฉ.
    
    ุงูุงุณุชุฑุงุชูุฌูุงุช:
    - keep_first: ุฅุจูุงุก ุงูุนูุงูุฉ ุงูุฃููู ูุญุฐู ุงูุจุงูู
    - keep_last: ุฅุจูุงุก ุงูุนูุงูุฉ ุงูุฃุฎูุฑุฉ ูุญุฐู ุงูุจุงูู
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
    keep_first : bool
        ุฅุฐุง Trueุ ุงุญุชูุธ ุจุงูุฃููู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ุจุนุฏ ูุนุงูุฌุฉ ุงูุชุชุงุจุน
    """
    # ููุท ูุทุงุจู 2 ุฃู ุฃูุซุฑ ูู ุนูุงูุงุช ุงูุชุฑููู ุงููุชุชุงุจุนุฉ
    punct_chars = r'ุุุ.,:;?!'
    pattern = re.compile(f'([{punct_chars}])([{punct_chars}]+)')
    
    if keep_first:
        # ุฅุจูุงุก ุงูุฃููู
        return pattern.sub(r'\1', text)
    else:
        # ุฅุจูุงุก ุงูุฃุฎูุฑุฉ
        def keep_last_match(m):
            return m.group(0)[-1]
        return pattern.sub(keep_last_match, text)


def handle_consecutive_punctuation_smart(text: str) -> str:
    """
    ูุนุงูุฌุฉ ุฐููุฉ ููุชุฑููู ุงููุชุชุงุจุน.
    
    ุชุณุชุฎุฏู ุงูุฃููููุฉ: . > ุ > ! > ุ > ุ > :
    ุชุญุชูุธ ุจุงูุนูุงูุฉ ุฐุงุช ุงูุฃููููุฉ ุงูุฃุนูู.
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ุจุนุฏ ุงุฎุชุฒุงู ุงูุชุฑููู
    """
    # ุชุฑุชูุจ ุงูุฃููููุฉ (ุงูุฃุนูู ุฃููุงู)
    priority = {'.': 6, 'ุ': 5, '?': 5, '!': 4, 'ุ': 3, ';': 3, 'ุ': 2, ',': 2, ':': 1}
    
    punct_chars = r'ุุุ.,:;?!'
    pattern = re.compile(f'[{punct_chars}]{{2,}}')
    
    def replace_func(match):
        sequence = match.group(0)
        # ุฅูุฌุงุฏ ุงูุนูุงูุฉ ุฐุงุช ุงูุฃููููุฉ ุงูุฃุนูู
        best_char = sequence[0]
        best_priority = priority.get(best_char, 0)
        
        for char in sequence:
            char_priority = priority.get(char, 0)
            if char_priority > best_priority:
                best_char = char
                best_priority = char_priority
        
        return best_char
    
    return pattern.sub(replace_func, text)


# ุงุฎุชุจุงุฑ ุงูุฏุงูุฉ
logger.section("๐ง ุงููุนุงูุฌุฉ ุงููุณุจูุฉ: ูุนุงูุฌุฉ ุงูุชุฑููู ุงููุชุชุงุจุน")

test_cases = [
    "ูุงุฐุงุุุ",
    "ูุฐุง ุตุญูุญ..",
    "ุฃููุงูุุ",
    "ุงูุชูู.ุ",
    "ููุงูุฉ ุงููุต.,",
]

logger.info("ุญุงูุงุช ุงูุงุฎุชุจุงุฑ:")
for test in test_cases:
    result_first = handle_consecutive_punctuation(test, keep_first=True)
    result_smart = handle_consecutive_punctuation_smart(test)
    logger.info(f"   '{test}'")
    logger.info(f"      ุฅุจูุงุก ุงูุฃููู: '{result_first}'")
    logger.info(f"      ุฐูู:          '{result_smart}'")

# %% [markdown]
# ### 3.9 ุชูุญูุฏ ุงููุณุงูุงุช ูุถุจุท ูุณุงูุงุช ุงูุชุฑููู

# %%
# ============================================================================
# ุงููุนุงูุฌุฉ 3.9: ุชูุญูุฏ ุงููุณุงูุงุช ูุถุจุท ูุณุงูุงุช ุงูุชุฑููู
# ============================================================================

def normalize_whitespace(text: str) -> str:
    """
    ุชูุญูุฏ ุงููุณุงูุงุช ูู ุงููุต.
    
    - ุงุณุชุจุฏุงู ุงููุณุงูุงุช ุงููุชุนุฏุฏุฉ ุจูุณุงูุฉ ูุงุญุฏุฉ
    - ุฅุฒุงูุฉ ุงููุณุงูุงุช ูู ุงูุจุฏุงูุฉ ูุงูููุงูุฉ
    - ุงุณุชุจุฏุงู ุนูุงูุงุช ุงูุฌุฏููุฉ (tabs) ุจูุณุงูุงุช
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ุจูุณุงูุงุช ููุญุฏุฉ
    """
    # ุงุณุชุจุฏุงู tabs ุจูุณุงูุงุช
    text = text.replace('\t', ' ')
    
    # ุงุณุชุจุฏุงู ุงููุณุงูุงุช ุงููุชุนุฏุฏุฉ
    text = re.sub(r' +', ' ', text)
    
    # ุฅุฒุงูุฉ ูุณุงูุงุช ุงูุจุฏุงูุฉ ูุงูููุงูุฉ
    text = text.strip()
    
    return text


def add_punctuation_spacing(text: str) -> str:
    """
    ุถูุงู ูุฌูุฏ ูุณุงูุงุช ุตุญูุญุฉ ุญูู ุนูุงูุงุช ุงูุชุฑููู.
    
    ุงูููุงุนุฏ:
    - ูุณุงูุฉ ุจุนุฏ ุนูุงูุฉ ุงูุชุฑููู (ุฅุฐุง ุชุจุนูุง ุญุฑู/ุฑูู)
    - ูุง ูุณุงูุฉ ูุจู ุนูุงูุฉ ุงูุชุฑููู
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ุจูุณุงูุงุช ุชุฑููู ุตุญูุญุฉ
    """
    punct_marks = 'ุุุ.:!'
    
    # ุฅุฒุงูุฉ ุงููุณุงูุฉ ูุจู ุงูุชุฑููู
    for p in punct_marks:
        text = re.sub(rf'\s+{re.escape(p)}', p, text)
    
    # ุฅุถุงูุฉ ูุณุงูุฉ ุจุนุฏ ุงูุชุฑููู ุฅุฐุง ุชุจุนู ุญุฑู ุฃู ุฑูู ุนุฑุจู
    for p in punct_marks:
        # ุจุนุฏ ุงูุชุฑูููุ ุฅุฐุง ุชุจุนู ุญุฑู ุนุฑุจู ุจุฏูู ูุณุงูุฉุ ุฃุถู ูุณุงูุฉ
        text = re.sub(
            rf'{re.escape(p)}([\u0600-\u06FFู-ูฉ])',
            rf'{p} \1',
            text
        )
    
    # ุชูุธูู ุงููุณุงูุงุช ุงููุชุนุฏุฏุฉ ุงูุชู ูุฏ ุชููู ูุดุฃุช
    text = re.sub(r' +', ' ', text)
    
    return text


# ุงุฎุชุจุงุฑ ุงูุฏูุงู
logger.section("๐ง ุงููุนุงูุฌุฉ ุงููุณุจูุฉ: ุชูุญูุฏ ุงููุณุงูุงุช ูุถุจุท ุงูุชุฑููู")

test_cases = [
    "ุงููุต   ูุน   ูุณุงูุงุช    ูุซูุฑุฉ",
    "ูููุฉุูููุฉ",
    "ูุต .ูุน ูุณุงูุฉ ูุจู ุงูููุทุฉ",
    "ุณุคุงูุุฌูุงุจ",
    "ุฃููุงู ุ ุซุงููุงู ุ ุซุงูุซุงู",
]

logger.info("ุญุงูุงุช ุงูุงุฎุชุจุงุฑ:")
for test in test_cases:
    result_ws = normalize_whitespace(test)
    result_spacing = add_punctuation_spacing(result_ws)
    logger.info(f"   '{test}'")
    logger.info(f"      ุจุนุฏ ุชูุญูุฏ ุงููุณุงูุงุช: '{result_ws}'")
    logger.info(f"      ุจุนุฏ ุถุจุท ุงูุชุฑููู:    '{result_spacing}'")

# %% [markdown]
# ### 3.10 ุฅุฒุงูุฉ ุงูุฃุณุทุฑ ุงููุงุฑุบุฉ ูุงููุตูุฑุฉ ุฌุฏุงู

# %%
# ============================================================================
# ุงููุนุงูุฌุฉ 3.10: ุฅุฒุงูุฉ ุงูุฃุณุทุฑ ุงููุงุฑุบุฉ ูุงููุตูุฑุฉ
# ============================================================================

def is_valid_sentence(text: str, min_words: int = 3) -> bool:
    """
    ุงูุชุญูู ููุง ุฅุฐุง ูุงูุช ุงูุฌููุฉ ุชุณุชููู ุงูุญุฏ ุงูุฃุฏูู ูู ุงููุชุทูุจุงุช.
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู (ูุฌุจ ุฃู ูููู ูุนุงูุฌุงู ูุณุจูุงู)
    min_words : int
        ุงูุญุฏ ุงูุฃุฏูู ูููููุงุช
        
    ุชุฑุฌุน:
    --------
    bool
        True ุฅุฐุง ูุงูุช ุงูุฌููุฉ ุตุงูุญุฉ
    """
    # ุฅุฒุงูุฉ ุงููุณุงูุงุช
    text = text.strip()
    
    # ุงูุชุญูู ูู ุงููุฑุงุบ
    if not text:
        return False
    
    # ุนุฏ ุงููููุงุช ุงูุนุฑุจูุฉ ููุท
    arabic_words = re.findall(r'[\u0600-\u06FF]+', text)
    
    return len(arabic_words) >= min_words


def filter_sentence(text: str, min_words: int = 3) -> Optional[str]:
    """
    ุชุตููุฉ ูุฅุฑุฌุงุน ุงูุฌููุฉ ุฅุฐุง ูุงูุช ุตุงูุญุฉุ ูุฅูุง None.
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
    min_words : int
        ุงูุญุฏ ุงูุฃุฏูู ูููููุงุช
        
    ุชุฑุฌุน:
    --------
    Optional[str]
        ุงูุฌููุฉ ุฅุฐุง ูุงูุช ุตุงูุญุฉุ ูุฅูุง None
    """
    if is_valid_sentence(text, min_words):
        return text
    return None


# ุงุฎุชุจุงุฑ ุงูุฏุงูุฉ
logger.section("๐ง ุงููุนุงูุฌุฉ ุงููุณุจูุฉ: ุชุตููุฉ ุงูุฌูู ุงููุตูุฑุฉ/ุงููุงุฑุบุฉ")

test_cases = [
    "",
    "ูุนู",
    "ุฃ",
    "ูุนู ูุง",
    "ูุฐุง ูุต ุตุญูุญ ูููุจูู",
    "   ",
    "1.",
    "ูุต ูุตูุฑ ุฌุฏุง",
]

logger.info(f"ุงูุญุฏ ุงูุฃุฏูู ูููููุงุช: {config.min_words}")
logger.info("ุญุงูุงุช ุงูุงุฎุชุจุงุฑ:")
for test in test_cases:
    is_valid = is_valid_sentence(test, min_words=config.min_words)
    status = "โ ุตุงูุญ" if is_valid else "โ ุบูุฑ ุตุงูุญ"
    logger.info(f"   '{test}' -> {status}")

# %% [markdown]
# ### 3.11 ูุนุงูุฌุฉ ุงูุฌูู ุงูุทูููุฉ

# %%
# ============================================================================
# ุงููุนุงูุฌุฉ 3.11: ูุนุงูุฌุฉ ุงูุฌูู ุงูุทูููุฉ
# ============================================================================

def truncate_sentence(text: str, max_words: int = 100) -> str:
    """
    ูุต ุงูุฌููุฉ ูุชูุงุณุจ ุงูุญุฏ ุงูุฃูุตู ูููููุงุช.
    
    ุงูุงุณุชุฑุงุชูุฌูุฉ: ุงููุต ุนูุฏ ุญุฏูุฏ ุงููููุงุชุ ูุญุงููุฉ ุงูุฅููุงุก ุจุนูุงูุฉ ุชุฑููู.
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
    max_words : int
        ุงูุญุฏ ุงูุฃูุตู ูููููุงุช
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ุงูููุตูุต
    """
    words = text.split()
    
    if len(words) <= max_words:
        return text
    
    # ุงููุต ุฅูู max_words
    truncated_words = words[:max_words]
    truncated = ' '.join(truncated_words)
    
    # ุถูุงู ุงูููุงูุฉ ุจุนูุงูุฉ ุชุฑููู
    if truncated and truncated[-1] not in SENTENCE_TERMINALS:
        truncated += '.'
    
    return truncated


def split_long_sentence(text: str, max_words: int = 100) -> List[str]:
    """
    ุชูุณูู ุงูุฌููุฉ ุงูุทูููุฉ ุฅูู ุฌูู ูุชุนุฏุฏุฉ ุนูุฏ ุงูุญุฏูุฏ ุงูุทุจูุนูุฉ.
    
    ุงูุงุณุชุฑุงุชูุฌูุฉ: ุงูุชูุณูู ุนูุฏ ุนูุงูุงุช ุงูุชุฑููู (ุุ) ุงูุชู ุชุฎูู ููุงุตู ุทุจูุนูุฉ.
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
    max_words : int
        ุงูุญุฏ ุงูุฃูุตู ูููููุงุช ููู ููุทุน
        
    ุชุฑุฌุน:
    --------
    List[str]
        ูุงุฆูุฉ ุจููุงุทุน ุงูุฌููุฉ
    """
    words = text.split()
    
    if len(words) <= max_words:
        return [text]
    
    # ุฅูุฌุงุฏ ููุงุท ุงููุตู ุงููุญุชููุฉ (ุจุนุฏ ุ ุฃู ุ)
    segments = []
    current_segment = []
    word_count = 0
    
    for word in words:
        current_segment.append(word)
        word_count += 1
        
        # ุชุญูู ุฅุฐุง ูุงูุช ุงููููุฉ ุชูุชูู ุจูุงุตูุฉ ูุชุฌุงูุฒูุง ุงูููุชุตู
        if word_count >= max_words // 2:
            if word.endswith('ุ') or word.endswith('ุ'):
                # ุฅูุดุงุก ููุทุน
                segment_text = ' '.join(current_segment)
                segments.append(segment_text)
                current_segment = []
                word_count = 0
        
        # ุฅุฌุจุงุฑ ุงูุชูุณูู ุฅุฐุง ูุตููุง ููุญุฏ ุงูุฃูุตู
        if word_count >= max_words:
            segment_text = ' '.join(current_segment)
            if not segment_text.endswith(('.', 'ุ', '!')):
                segment_text += '.'
            segments.append(segment_text)
            current_segment = []
            word_count = 0
    
    # ุฅุถุงูุฉ ุงููุชุจูู
    if current_segment:
        segment_text = ' '.join(current_segment)
        segments.append(segment_text)
    
    return segments


# ุงุฎุชุจุงุฑ ุงูุฏูุงู
logger.section("๐ง ุงููุนุงูุฌุฉ ุงููุณุจูุฉ: ูุนุงูุฌุฉ ุงูุฌูู ุงูุทูููุฉ")

# ุฅูุดุงุก ุฌููุฉ ุทูููุฉ ููุงุฎุชุจุงุฑ
long_sentence = " ".join(["ูููุฉ"] * 150)
logger.info(f"ุงูุทูู ุงูุฃุตูู: {len(long_sentence.split())} ูููุฉ")

truncated = truncate_sentence(long_sentence, max_words=100)
logger.info(f"ุจุนุฏ ุงููุต: {len(truncated.split())} ูููุฉ")
logger.info(f"ููุชูู ุจู: '{truncated[-10:]}'")

# ุงุฎุชุจุงุฑ ุงูููุงุตู ุงูุทุจูุนูุฉ
long_with_punct = "ูุฐุง ูุต ุทููู ูุญุชูู ุนูู ููุฑุงุช ูุชุนุฏุฏุฉุ ููู ููุฑุฉ ุชุญุชูู ุนูู ูุนูููุงุช ูููุฉุ " * 20
segments = split_long_sentence(long_with_punct, max_words=50)
logger.info(f"\nุชู ุงูุชูุณูู ุฅูู {len(segments)} ููุทุน:")
for i, seg in enumerate(segments[:3]):
    logger.info(f"   ุงูููุทุน {i+1}: {len(seg.split())} ูููุฉ")

# %% [markdown]
# ---
# ## 4. ุงูุฌุฒุก 3: ุฎุทูุงุช ุงููุนุงูุฌุฉ ุงูุงุฎุชูุงุฑูุฉ (ููุชุฌุฑูุจ)
# 
# ูุฐู ุงูุฎุทูุงุช **ุงุฎุชูุงุฑูุฉ** ููููู ุชูุนูููุง ููุชุฌุฑูุจ.
# ูุฏ ุชุญุณู ุฃู ุชุถุฑ ุจุฃุฏุงุก ุงููููุฐุฌ ุญุณุจ ุงููููุฉ.

# %% [markdown]
# ### 4.1 ูุตู ูุงู ุงูุนุทู ุนู ุงููููุงุช

# %%
# ============================================================================
# ุงุฎุชูุงุฑู 4.1: ูุตู ูุงู ุงูุนุทู
# ============================================================================

def separate_waw_conjunction(text: str, min_remaining_length: int = 3) -> str:
    """
    ูุตู ูุงู ุงูุนุทู (ู) ุนู ุจุฏุงูุฉ ุงููููุงุช.
    
    ูู ุงูุนุฑุจูุฉุ ุชุชุตู ุงููุงู ุจุงููููุฉ ุงูุชุงููุฉ ูุณุงุจูุฉ. ูุตููุง ูุฏ ูุณุงุนุฏ ูู:
    - ุชุฑููุฒ (Tokenization) ุฃูุถู
    - ุญุฏูุฏ ูููุงุช ูุชุณูุฉ
    - ุชุญุณูู ุชูุจุค ุงูุชุฑููู ูุจู ุงูุนุทู
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
    min_remaining_length : int
        ุงูุตู ููุท ุฅุฐุง ุชุจูู ูู ุงููููุฉ ูุฐุง ุงูุนุฏุฏ ูู ุงูุญุฑูู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ูุน ูุงู ุนุทู ููุตููุฉ
        
    ูุซุงู:
    --------
    >>> separate_waw_conjunction("ููุงู ุงูุฑุฌู ูุฐูุจ")
    'ู ูุงู ุงูุฑุฌู ู ุฐูุจ'
    """
    # ูููุงุช ุญูุซ ุงููุงู ุฌุฒุก ูู ุงูุฌุฐุฑ (ูุฌุจ ุนุฏู ูุตููุง)
    waw_root_words = {
        'ููุช', 'ูุฌู', 'ูุถุน', 'ูุตู', 'ููุน', 'ูุฒู', 'ููุฏ', 'ูุฑู', 'ูุทู',
        'ูุณุท', 'ูุญุฏุฉ', 'ูุฒูุฑ', 'ูุฒุงุฑุฉ', 'ููุงูุฉ', 'ููุฏ', 'ูุงูุฏ', 'ูุงูุฏุฉ',
        'ูุซููุฉ', 'ูุซุงุฆู', 'ูุงูุน', 'ูุงุฌุจ', 'ููุงุฉ', 'ููุงูุฉ', 'ูููู',
        'ูุงุญุฏ', 'ูุงุญุฏุฉ', 'ูุณููุฉ', 'ูุณุงุฆู', 'ูุฑุดุฉ', 'ูุธููุฉ', 'ูุธุงุฆู',
    }
    
    def should_separate(word: str) -> bool:
        """ุชุญูู ููุง ุฅุฐุง ูุงู ูุฌุจ ูุตู ุงููุงู ุนู ูุฐู ุงููููุฉ."""
        if not word.startswith('ู'):
            return False
        
        if len(word) < min_remaining_length + 1:  # +1 ูููุงู
            return False
        
        # ุงูุชุญูู ููุง ุฅุฐุง ูุงูุช ุงููููุฉ (ูุน ุงููุงู) ูููุฉ ุฌุฐุฑูุฉ
        if word in waw_root_words:
            return False
        
        # ุงูุชุญูู ููุง ุฅุฐุง ูุงูุช ุงููููุฉ ุจุฏูู ูุงู ููุฌูุฏุฉ ููููุฉ ุตุงูุญุฉ
        remaining = word[1:]
        
        # ุจุงุฏุฆุงุช ุดุงุฆุนุฉ ุชุดูุฑ ุฅูู ุฃู ุงููุงู ููุนุทู
        conjunction_indicators = [
            'ุงู',   # ู + ุงู (ุงูุชุนุฑูู)
            'ูู', 'ูู', 'ูู',  # ุถูุงุฆุฑ
            'ูุฏ', 'ูู', 'ูู',  # ุฃุฏูุงุช
            'ูุงู', 'ูููู',     # ุฃูุนุงู
            'ุฃู', 'ุฅู',        # ุฃุฏูุงุช
        ]
        
        for indicator in conjunction_indicators:
            if remaining.startswith(indicator):
                return True
        
        # ุฅุฐุง ุจุฏุฃุช ุงููููุฉ ุงููุชุจููุฉ ุจู ุงู ุงูุชุนุฑููุ ููู ุบุงูุจุงู ุนุทู
        if remaining.startswith('ุงู'):
            return True
        
        return len(remaining) >= min_remaining_length
    
    words = text.split()
    result = []
    
    for word in words:
        if should_separate(word):
            result.append('ู')
            result.append(word[1:])
        else:
            result.append(word)
    
    return ' '.join(result)


# ุงุฎุชุจุงุฑ ุงูุฏุงูุฉ
logger.section("๐ง ุงุฎุชูุงุฑู: ูุตู ูุงู ุงูุนุทู")

test_cases = [
    "ููุงู ุงูุฑุฌู",
    "ูุงูุฃูู ุงููุชุญุฏุฉ",
    "ููู ูุฐุง ุงูุตุฏุฏ",
    "ููุช ุงูุงุฌุชูุงุน",  # ูุง ูุฌุจ ูุตููุง (ููุช ูููุฉ ุฌุฐุฑูุฉ)
    "ูุซููุฉ ูููุฉ",   # ูุง ูุฌุจ ูุตููุง
    "ูุงูุชูููุฉ ุงููุณุชุฏุงูุฉ",
    "ููู ูุนูู",
]

logger.info("ุญุงูุงุช ุงูุงุฎุชุจุงุฑ:")
for test in test_cases:
    result = separate_waw_conjunction(test)
    logger.info(f"   '{test}'")
    logger.info(f"   -> '{result}'")

# %% [markdown]
# ### 4.2 ุงุณุชุฑุงุชูุฌูุงุช ุงูุชุนุงูู ูุน ูููุงุช ุงูุชููู

# %%
# ============================================================================
# ุงุฎุชูุงุฑู 4.2: ุงูุชุนุงูู ูุน ูููุงุช ุงูุชููู
# ============================================================================

def mark_stopwords(text: str, marker: str = '<SW>') -> str:
    """
    ุชูููุฒ ูููุงุช ุงูุชููู ุจุฑูุฒ ุฎุงุต (ููุชุญููู/ุงูุชุฌุฑูุจ).
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
    marker : str
        ุงูุฑูุฒ ููุฅุถุงูุฉ ุจุนุฏ ูููุงุช ุงูุชููู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ูุน ูููุงุช ุชููู ูููุฒุฉ
    """
    words = text.split()
    result = []
    
    for word in words:
        # ุฅุฒุงูุฉ ุงูุชุฑููู ูููุญุต
        clean_word = re.sub(r'[ุุุ.:!]', '', word)
        
        if clean_word in ARABIC_STOPWORDS:
            result.append(word + marker)
        else:
            result.append(word)
    
    return ' '.join(result)


def remove_stopwords(text: str, keep_structure: bool = True) -> str:
    """
    ุฅุฒุงูุฉ ูููุงุช ุงูุชููู ูู ุงููุต.
    
    ุชุญุฐูุฑ: ูุฐุง ูุฏ ูุถุฑ ุจุชูุจุค ุงูุชุฑููู ูุฃู ูููุงุช ุงูุชููู ุชููุฑ ุณูุงูุงู ูููููุงู ูููุงู.
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
    keep_structure : bool
        ุฅุฐุง Trueุ ุงุญุชูุธ ุจุงูุชุฑููู ุญุชู ูู ูุงู ููุชุตูุงู ุจูููุงุช ุชููู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ุจุฏูู ูููุงุช ุชููู
    """
    words = text.split()
    result = []
    
    for word in words:
        # ูุตู ุงููููุฉ ุนู ุงูุชุฑููู ุงููุงุญู
        punct = ''
        clean_word = word
        
        if word and word[-1] in 'ุุุ.:!':
            punct = word[-1]
            clean_word = word[:-1]
        
        if clean_word not in ARABIC_STOPWORDS:
            result.append(word)
        elif keep_structure and punct:
            # ุงูุงุญุชูุงุธ ุจุงูุชุฑููู ุญุชู ูู ูุงูุช ุงููููุฉ ูููุฉ ุชููู
            if result:
                result[-1] += punct
    
    return ' '.join(result)


# ุงุฎุชุจุงุฑ ุงูุฏุงูุฉ
logger.section("๐ง ุงุฎุชูุงุฑู: ุงูุชุนุงูู ูุน ูููุงุช ุงูุชููู")

test_cases = [
    "ููู ูุฐุง ุงูุตุฏุฏ",
    "ูู ุฃุฌู ุงูุชูููุฉ",
    "ุงูุฃูู ุงููุชุญุฏุฉ ูู ููุธูุฉ ุฏูููุฉ",
]

logger.info("ุญุงูุงุช ุงูุงุฎุชุจุงุฑ:")
for test in test_cases:
    marked = mark_stopwords(test)
    removed = remove_stopwords(test)
    logger.info(f"   ุงูุฃุตู:   '{test}'")
    logger.info(f"   ูููุฒุฉ:   '{marked}'")
    logger.info(f"   ูุญุฐููุฉ:  '{removed}'")

# %% [markdown]
# ### 4.3 ุงุณุชุจุฏุงู ุงูุฃุฑูุงู ุจุฑูุฒ ุฎุงุต

# %%
# ============================================================================
# ุงุฎุชูุงุฑู 4.3: ุงุณุชุจุฏุงู ุงูุฃุฑูุงู ุจุฑูุฒ
# ============================================================================

def replace_numbers_with_token(text: str, token: str = '<NUM>') -> str:
    """
    ุงุณุชุจุฏุงู ุฌููุน ุงูุฃุฑูุงู ุจุฑูุฒ ุฎุงุต.
    
    ูุฐุง ูุณุงุนุฏ ูู:
    - ุชูููู ุญุฌู ุงูููุฑุฏุงุช
    - ุชุฑููุฒ ุงููููุฐุฌ ุนูู ุงููููู ุจุฏูุงู ูู ุฃุฑูุงู ูุญุฏุฏุฉ
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
    token : str
        ุงูุฑูุฒ ูุงุณุชุจุฏุงู ุงูุฃุฑูุงู ุจู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ุจุนุฏ ุงุณุชุจุฏุงู ุงูุฃุฑูุงู
    """
    # ููุท ููุฃุฑูุงู ุงูุนุฑุจูุฉ ุฃู ุงูุบุฑุจูุฉ
    number_pattern = re.compile(r'[ู-ูฉ0-9]+')
    return number_pattern.sub(token, text)


def normalize_number_format(text: str) -> str:
    """
    ุชูุญูุฏ ุชูุณูู ุงูุฃุฑูุงู (ูุซู ููุงุตู ุงูุขูุงู).
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ุจุชูุณูู ุฃุฑูุงู ููุญุฏ
    """
    # ุฅุฒุงูุฉ ููุงุตู ุงูุขูุงู (ูู ูู , ู ูฌ)
    text = re.sub(r'(\d),(\d)', r'\1\2', text)
    text = re.sub(r'([\u0660-\u0669])ูฌ([\u0660-\u0669])', r'\1\2', text)
    
    return text


# ุงุฎุชุจุงุฑ ุงูุฏุงูุฉ
logger.section("๐ง ุงุฎุชูุงุฑู: ุงุณุชุจุฏุงู ุงูุฃุฑูุงู ุจุฑูุฒ")

test_cases = [
    "ูู ุนุงู ูขููขูค",
    "ูุจูุบ ูกููููู ุฏููุงุฑ",
    "ูู ูฅ ุฅูู ูกู ุณููุงุช",
    "ุงููุฑุงุฑ ุฑูู ูคูง/ูกูขูฃ",
]

logger.info("ุญุงูุงุช ุงูุงุฎุชุจุงุฑ:")
for test in test_cases:
    result = replace_numbers_with_token(test)
    logger.info(f"   '{test}'")
    logger.info(f"   -> '{result}'")

# %% [markdown]
# ### 4.4 ูุนุงูุฌุฉ ุงููููุงุช ุงููุงุฏุฑุฉ

# %%
# ============================================================================
# ุงุฎุชูุงุฑู 4.4: ูุนุงูุฌุฉ ุงููููุงุช ุงููุงุฏุฑุฉ
# ============================================================================

def build_vocabulary(dataset_dir: str, sample_size: int = 1000000) -> Counter:
    """
    ุจูุงุก ูุงููุณ ุจุชูุฑุงุฑ ุงููููุงุช.
    
    ุงููุนุงููุงุช:
    -----------
    dataset_dir : str
        ูุณุงุฑ ุงูุจูุงูุงุช
    sample_size : int
        ุนุฏุฏ ุงูุฃุณุทุฑ ูููุนุงูุฌุฉ
        
    ุชุฑุฌุน:
    --------
    Counter
        ุนุฏุงุฏ ุชูุฑุงุฑ ุงููููุงุช
    """
    vocab = Counter()
    arabic_word_pattern = re.compile(r'[\u0600-\u06FF]+')
    
    for i, line in enumerate(iter_dataset_lines(dataset_dir)):
        if i >= sample_size:
            break
        
        words = arabic_word_pattern.findall(line)
        vocab.update(words)
    
    return vocab


def replace_rare_words(text: str, vocab: Counter, threshold: int = 5, 
                       token: str = '<UNK>') -> str:
    """
    ุงุณุชุจุฏุงู ุงููููุงุช ุงููุงุฏุฑุฉ (ุฃูู ูู ุญุฏ ุงูุชูุฑุงุฑ) ุจุฑูุฒ ุฎุงุต.
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
    vocab : Counter
        ุงููุงููุณ ูุน ุงูุชูุฑุงุฑุงุช
    threshold : int
        ุงูุญุฏ ุงูุฃุฏูู ููุชูุฑุงุฑ ูุฅุจูุงุก ุงููููุฉ
    token : str
        ุฑูุฒ ุงูุงุณุชุจุฏุงู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ูุน ุงุณุชุจุฏุงู ุงููููุงุช ุงููุงุฏุฑุฉ
    """
    arabic_word_pattern = re.compile(r'[\u0600-\u06FF]+')
    
    def replace_if_rare(match):
        word = match.group(0)
        if vocab.get(word, 0) < threshold:
            return token
        return word
    
    return arabic_word_pattern.sub(replace_if_rare, text)


# ููุงุญุธุฉ: ุจูุงุก ุงููุงููุณ ูููู ุญุณุงุจูุงูุ ุณููุถุญ ุจูุญุงูุงุฉ
logger.section("๐ง ุงุฎุชูุงุฑู: ูุนุงูุฌุฉ ุงููููุงุช ุงููุงุฏุฑุฉ")

logger.info("ุจูุงุก ุงููุงููุณ ุนูููุฉ ููููุฉ ุญุณุงุจูุงู.")
logger.info("ูู ุงูุฅูุชุงุฌุ ูุฌุจ ุจูุงุก ุงููุงููุณ ูุฑุฉ ูุงุญุฏุฉ ูุญูุธู.")
logger.info("\nูุซุงู ุงูุงุณุชุฎุฏุงู:")
logger.info("   vocab = build_vocabulary(dataset_dir, sample_size=1000000)")
logger.info("   text = replace_rare_words(text, vocab, threshold=5)")

# %% [markdown]
# ### 4.5 ุชุณููุฉ ุทูู ุงูุฌูู

# %%
# ============================================================================
# ุงุฎุชูุงุฑู 4.5: ุชุณููุฉ ุทูู ุงูุฌูู
# ============================================================================

def pad_sentence(text: str, target_length: int, pad_token: str = '<PAD>') -> str:
    """
    ุญุดู ุงูุฌููุฉ ูุชุตู ููุทูู ุงููุณุชูุฏู.
    
    ููุงุญุธุฉ: ูุฐุง ูุชู ุนุงุฏุฉ ุฃุซูุงุก ุงูู batchingุ ููุณ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ.
    ูุฏุฑุฌ ููุง ููุงูุชูุงู.
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
    target_length : int
        ุนุฏุฏ ุงููููุงุช ุงููุณุชูุฏู
    pad_token : str
        ุฑูุฒ ุงูุญุดู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ุงููุญุดู
    """
    words = text.split()
    
    if len(words) >= target_length:
        return text
    
    padding = [pad_token] * (target_length - len(words))
    return text + ' ' + ' '.join(padding)


def split_into_chunks(text: str, chunk_size: int = 50, overlap: int = 10) -> List[str]:
    """
    ุชูุณูู ุงููุต ุงูุทููู ุฅูู ุฃุฌุฒุงุก ูุชุฏุงุฎูุฉ.
    
    ูููุฏ ูููุณุชูุฏุงุช ุงูุทูููุฉ ุฌุฏุงู ุญูุซ ูุฌุจ ุงูุญูุงุธ ุนูู ุงูุณูุงู.
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
    chunk_size : int
        ุงููููุงุช ุงููุณุชูุฏูุฉ ููู ุฌุฒุก
    overlap : int
        ุนุฏุฏ ูููุงุช ุงูุชุฏุงุฎู ุจูู ุงูุฃุฌุฒุงุก
        
    ุชุฑุฌุน:
    --------
    List[str]
        ูุงุฆูุฉ ุฃุฌุฒุงุก ุงููุต
    """
    words = text.split()
    
    if len(words) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(words):
        end = min(start + chunk_size, len(words))
        chunk_words = words[start:end]
        chunks.append(' '.join(chunk_words))
        
        # ุชุญุฑูู ุงูุจุฏุงูุฉ ูุน ุงูุชุฏุงุฎู
        start = end - overlap
        if start >= len(words) - overlap:
            break
    
    return chunks


# ุงุฎุชุจุงุฑ ุงูุฏูุงู
logger.section("๐ง ุงุฎุชูุงุฑู: ุชุณููุฉ ุทูู ุงูุฌูู")

test_text = "ูุฐุง ูุต ุทููู ูุญุชูู ุนูู ุงูุนุฏูุฏ ูู ุงููููุงุช ุงูุชู ุชุญุชุงุฌ ุฅูู ูุนุงูุฌุฉ"
logger.info(f"ุงูุฃุตู: {len(test_text.split())} ูููุฉ")

chunks = split_into_chunks(test_text, chunk_size=5, overlap=2)
logger.info(f"ุชู ุงูุชูุณูู ุฅูู {len(chunks)} ุฃุฌุฒุงุก:")
for i, chunk in enumerate(chunks):
    logger.info(f"   ุงูุฌุฒุก {i+1}: '{chunk}'")

# %% [markdown]
# ### 4.6 ุฅุฒุงูุฉ/ุงุณุชุจุฏุงู ุงููุตุทูุญุงุช ุงูุฃุฌูุจูุฉ

# %%
# ============================================================================
# ุงุฎุชูุงุฑู 4.6: ุงูุชุนุงูู ูุน ุงููุตุทูุญุงุช ุงูุฃุฌูุจูุฉ
# ============================================================================

def remove_document_references(text: str) -> str:
    """
    ุฅุฒุงูุฉ ูุฑุงุฌุน ูุณุชูุฏุงุช ุงูุฃูู ุงููุชุญุฏุฉ (ูุซู A/47/10, S/RES/1234).
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ุจุฏูู ูุฑุงุฌุน ุงููุณุชูุฏุงุช
    """
    # ููุท ูุฑุงุฌุน ุงููุณุชูุฏุงุช
    doc_pattern = re.compile(r'[A-Z]/[A-Z0-9]+(?:/[A-Z0-9]+)*')
    return doc_pattern.sub('', text)


def replace_foreign_with_token(text: str, token: str = '<FOREIGN>') -> str:
    """
    ุงุณุชุจุฏุงู ุงููุตุทูุญุงุช ุงูุฃุฌูุจูุฉ (ุบูุฑ ุงูุนุฑุจูุฉ) ุจุฑูุฒ ุฎุงุต.
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุงููุต ุงููุฏุฎู
    token : str
        ุฑูุฒ ุงูุงุณุชุจุฏุงู
        
    ุชุฑุฌุน:
    --------
    str
        ุงููุต ูุน ุงุณุชุจุฏุงู ุงููุตุทูุญุงุช ุงูุฃุฌูุจูุฉ
    """
    # ููุท ูููููุงุช ุงููุงุชูููุฉ (3+ ุญุฑูู)
    foreign_pattern = re.compile(r'\b[A-Za-z]{3,}\b')
    return foreign_pattern.sub(token, text)


# ุงุฎุชุจุงุฑ ุงูุฏูุงู
logger.section("๐ง ุงุฎุชูุงุฑู: ุงูุชุนุงูู ูุน ุงููุตุทูุญุงุช ุงูุฃุฌูุจูุฉ")

test_cases = [
    "ุงููุซููุฉ A/47/10 ุงููุคุฑุฎุฉ",
    "ุจุฑูุงูุฌ UNDP ููุชูููุฉ",
    "ูุฑุงุฑ ูุฌูุณ ุงูุฃูู S/RES/1234",
]

logger.info("ุญุงูุงุช ุงูุงุฎุชุจุงุฑ:")
for test in test_cases:
    no_refs = remove_document_references(test)
    replaced = replace_foreign_with_token(test)
    logger.info(f"   ุงูุฃุตู:      '{test}'")
    logger.info(f"   ุฅุฒุงูุฉ ูุฑุงุฌุน:'{no_refs}'")
    logger.info(f"   ุงุณุชุจุฏุงู:    '{replaced}'")

# %% [markdown]
# ---
# ## 5. ุงูุฌุฒุก 4: ุฎุท ุฃูุงุจูุจ ุงููุนุงูุฌุฉ ุงููุงูู

# %%
# ============================================================================
# ุงููุณู 5: ุฎุท ุฃูุงุจูุจ ุงููุนุงูุฌุฉ ุงููุงูู
# ============================================================================

@dataclass
class PreprocessingStats:
    """ุฅุญุตุงุฆูุงุช ุชู ุฌูุนูุง ุฃุซูุงุก ุงููุนุงูุฌุฉ."""
    total_input_lines: int = 0
    total_output_lines: int = 0
    empty_lines_removed: int = 0
    short_lines_removed: int = 0
    long_lines_truncated: int = 0
    diacritics_removed: int = 0
    alef_normalized: int = 0
    punct_normalized: int = 0
    numbers_normalized: int = 0
    latin_removed: int = 0
    oov_removed: int = 0
    consecutive_punct_fixed: int = 0
    whitespace_fixed: int = 0


class ArabicTextPreprocessor:
    """
    ุฎุท ุฃูุงุจูุจ ูุนุงูุฌุฉ ูุงูู ููุฌููุนุฉ ุจูุงูุงุช ุงูุชุฑููู ุงูุนุฑุจู.
    
    ูููุฑ ูุฐุง ุงูููุงุณ ุฎุท ุฃูุงุจูุจ ูุงุจู ููุชููุฆุฉ ูููู ุงุณุชุฎุฏุงูู ูุน
    ูู ูู ุฎุทูุงุช ุงููุนุงูุฌุฉ ุงูุฅูุฒุงููุฉ ูุงูุงุฎุชูุงุฑูุฉ.
    
    ุงูุณูุงุช:
    -----------
    config : PreprocessingConfig
        ูุงุฆู ุงูุฅุนุฏุงุฏุงุช ุงููุญุชูู ุนูู ุฌููุน ุงูุฎูุงุฑุงุช
    stats : PreprocessingStats
        ุงูุฅุญุตุงุฆูุงุช ุงููุฌูุนุฉ ุฃุซูุงุก ุงููุนุงูุฌุฉ
    """
    
    def __init__(self, config: PreprocessingConfig):
        """
        ุชููุฆุฉ ุงููุนุงูุฌ ุจุงูุฅุนุฏุงุฏุงุช.
        
        ุงููุนุงููุงุช:
        -----------
        config : PreprocessingConfig
            ูุงุฆู ุงูุฅุนุฏุงุฏุงุช
        """
        self.config = config
        self.stats = PreprocessingStats()
        self.vocab = None  # ููุนุงูุฌุฉ ุงููููุงุช ุงููุงุฏุฑุฉ
        
        # ุชุฌููุน ุงูุฃููุงุท ููููุงุกุฉ
        self._compile_patterns()
    
    def _compile_patterns(self):
        """ุชุฌููุน ุฃููุงุท Regex ููููุงุกุฉ."""
        self.diacritics_pattern = re.compile(r'[\u064B-\u0652]')
        self.alef_pattern = re.compile(r'[ุฃุฅุขูฑ]')
        self.latin_pattern = re.compile(r'[A-Za-z]+')
        self.number_western = re.compile(r'[0-9]')
        self.consecutive_punct = re.compile(r'[ุุุ.,:;?!]{2,}')
        self.multi_space = re.compile(r' +')
        self.arabic_word = re.compile(r'[\u0600-\u06FF]+')
    
    def preprocess_line(self, text: str, apply_optional: bool = False) -> Optional[str]:
        """
        ุชุทุจูู ุฎุท ุฃูุงุจูุจ ุงููุนุงูุฌุฉ ุงููุงูู ุนูู ุณุทุฑ ูุงุญุฏ.
        
        ุงููุนุงููุงุช:
        -----------
        text : str
            ุณุทุฑ ุงููุต ุงููุฏุฎู
        apply_optional : bool
            ุชุทุจูู ุงูุฎุทูุงุช ุงูุงุฎุชูุงุฑูุฉ ุฃู ูุง
            
        ุชุฑุฌุน:
        --------
        Optional[str]
            ุงููุต ุงููุนุงูุฌุ ุฃู None ุฅุฐุง ุชู ุชุตููุฉ ุงูุณุทุฑ
        """
        original = text
        
        # ุชุชุจุน ุงูุชุบููุฑุงุช ููุฅุญุตุงุฆูุงุช
        had_diacritics = bool(self.diacritics_pattern.search(text))
        had_alef_var = bool(self.alef_pattern.search(text))
        had_latin = bool(self.latin_pattern.search(text))
        had_consec_punct = bool(self.consecutive_punct.search(text))
        
        # ============================================
        # ุงููุนุงูุฌุฉ ุงูุฅูุฒุงููุฉ
        # ============================================
        
        # 1. ุฅุฒุงูุฉ ุงูุชุดููู
        if self.config.remove_diacritics:
            text = self.diacritics_pattern.sub('', text)
            if had_diacritics:
                self.stats.diacritics_removed += 1
        
        # 2. ุชูุญูุฏ ุงูุฃูู
        if self.config.normalize_alef:
            text = self.alef_pattern.sub('ุง', text)
            if had_alef_var:
                self.stats.alef_normalized += 1
        
        # 3. ุชูุญูุฏ ุงูุฃูู ุงูููุตูุฑุฉ (ู -> ู)
        if self.config.normalize_alef_maksura:
            text = text.replace('ู', 'ู')
        
        # 4. ุฅุฒุงูุฉ ุงูุชุทููู
        if self.config.remove_tatweel:
            text = text.replace('\u0640', '')
        
        # 5. ุชูุญูุฏ ุงูุชุฑููู ููุนุฑุจู
        if self.config.unify_punctuation_to_arabic:
            for latin, arabic in LATIN_TO_ARABIC_PUNCT.items():
                if latin in text:
                    text = text.replace(latin, arabic)
                    self.stats.punct_normalized += 1
        
        # 6. ุชูุญูุฏ ุงูุฃุฑูุงู ููุนุฑุจูุฉ
        if self.config.unify_numbers_to_arabic:
            if self.number_western.search(text):
                text = text.translate(WESTERN_TO_ARABIC_NUMS)
                self.stats.numbers_normalized += 1
        
        # 7. ุฅุฒุงูุฉ ุงูุญุฑูู ุงููุงุชูููุฉ
        if self.config.remove_latin_letters:
            if had_latin:
                text = self.latin_pattern.sub('', text)
                self.stats.latin_removed += 1
        
        # 8. ุฅุฒุงูุฉ ุงูุญุฑูู ุฎุงุฑุฌ ุงููุทุงู
        if self.config.remove_oov_chars:
            original_len = len(text)
            text = remove_oov_characters(text)
            if len(text) < original_len:
                self.stats.oov_removed += 1
        
        # 9. ูุนุงูุฌุฉ ุงูุชุฑููู ุงููุชุชุงุจุน
        if self.config.handle_consecutive_punct:
            if had_consec_punct:
                text = handle_consecutive_punctuation_smart(text)
                self.stats.consecutive_punct_fixed += 1
        
        # 10. ุชูุญูุฏ ุงููุณุงูุงุช
        if self.config.normalize_whitespace:
            text = self.multi_space.sub(' ', text)
            text = text.strip()
            self.stats.whitespace_fixed += 1
        
        # 11. ุฅุถุงูุฉ ูุณุงูุงุช ุงูุชุฑููู
        if self.config.add_punct_spacing:
            text = add_punctuation_spacing(text)
        
        # ============================================
        # ุงููุนุงูุฌุฉ ุงูุงุฎุชูุงุฑูุฉ
        # ============================================
        
        if apply_optional:
            # ูุตู ูุงู ุงูุนุทู
            if self.config.separate_waw_conjunction:
                text = separate_waw_conjunction(text)
            
            # ุงุณุชุจุฏุงู ุงูุฃุฑูุงู ุจุฑูุฒ
            if self.config.replace_numbers_with_token:
                text = replace_numbers_with_token(text, '<NUM>')
            
            # ุฅุฒุงูุฉ ุงููุตุทูุญุงุช ุงูุฃุฌูุจูุฉ
            if self.config.remove_foreign_terms:
                text = remove_document_references(text)
        
        # ============================================
        # ุงูุชุตููุฉ
        # ============================================
        
        # ุชูุธูู ูุณุงูุงุช ููุงุฆู
        text = self.multi_space.sub(' ', text).strip()
        
        # ุงูุชุญูู ูู ุงููุฑุงุบ
        if self.config.remove_empty_lines and not text:
            self.stats.empty_lines_removed += 1
            return None
        
        # ุงูุชุญูู ูู ุนุฏุฏ ุงููููุงุช
        word_count = len(self.arabic_word.findall(text))
        
        # ุชุตููุฉ ุงูุฌูู ุงููุตูุฑุฉ
        if word_count < self.config.min_words:
            self.stats.short_lines_removed += 1
            return None
        
        # ูุนุงูุฌุฉ ุงูุฌูู ุงูุทูููุฉ
        if word_count > self.config.max_words:
            text = truncate_sentence(text, self.config.max_words)
            self.stats.long_lines_truncated += 1
        
        return text
    
    def process_dataset(self, input_dir: str, output_file: str, 
                        apply_optional: bool = False,
                        sample_size: Optional[int] = None) -> PreprocessingStats:
        """
        ูุนุงูุฌุฉ ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุงููุงูู ูุญูุธูุง ูู ููู.
        
        ุงููุนุงููุงุช:
        -----------
        input_dir : str
            ูุณุงุฑ ูุฌูุฏ ุงููุฏุฎูุงุช
        output_file : str
            ูุณุงุฑ ููู ุงููุฎุฑุฌุงุช
        apply_optional : bool
            ุชุทุจูู ุงููุนุงูุฌุฉ ุงูุงุฎุชูุงุฑูุฉ ุฃู ูุง
        sample_size : Optional[int]
            ุญุฏ ุงููุนุงูุฌุฉ ูุนุฏุฏ ูุนูู ูู ุงูุฃุณุทุฑ (None = ุงููู)
            
        ุชุฑุฌุน:
        --------
        PreprocessingStats
            ุฅุญุตุงุฆูุงุช ุงููุนุงูุฌุฉ
        """
        logger.section("๐ ูุนุงูุฌุฉ ูุฌููุนุฉ ุงูุจูุงูุงุช")
        logger.info(f"ุงููุฏุฎูุงุช: {input_dir}")
        logger.info(f"ุงููุฎุฑุฌุงุช: {output_file}")
        logger.info(f"ุชุทุจูู ุงูุงุฎุชูุงุฑู: {apply_optional}")
        
        # ุฅุนุงุฏุฉ ุชุนููู ุงูุฅุญุตุงุฆูุงุช
        self.stats = PreprocessingStats()
        
        # ุนุฏ ุฅุฌูุงูู ุงูุฃุณุทุฑ ูุดุฑูุท ุงูุชูุฏู
        if sample_size is None:
            total_lines = count_total_lines(input_dir)
            logger.info(f"ุฅุฌูุงูู ุงูุฃุณุทุฑ ูููุนุงูุฌุฉ: {total_lines:,}")
        else:
            total_lines = sample_size
            logger.info(f"ูุนุงูุฌุฉ ุนููุฉ ูู {total_lines:,} ุณุทุฑ")
        
        # ุฅูุดุงุก ูุฌูุฏ ุงููุฎุฑุฌุงุช ุฅุฐุง ูุฒู ุงูุฃูุฑ
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # ุงููุนุงูุฌุฉ ูุงููุชุงุจุฉ
        iterator = iter_dataset_lines(input_dir)
        if TQDM_AVAILABLE:
            iterator = tqdm(iterator, total=total_lines, desc="ุฌุงุฑู ุงููุนุงูุฌุฉ")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for i, line in enumerate(iterator):
                if sample_size and i >= sample_size:
                    break
                
                self.stats.total_input_lines += 1
                
                # ูุนุงูุฌุฉ ุงูุณุทุฑ
                processed = self.preprocess_line(line, apply_optional)
                
                # ุงููุชุงุจุฉ ุฅุฐุง ูู ูุชู ุชุตููุชู
                if processed:
                    f.write(processed + '\n')
                    self.stats.total_output_lines += 1
        
        # ุชุณุฌูู ุงูุฅุญุตุงุฆูุงุช
        self._log_statistics()
        
        return self.stats
    
    def _log_statistics(self):
        """ุชุณุฌูู ุฅุญุตุงุฆูุงุช ุงููุนุงูุฌุฉ."""
        logger.section("๐ ุฅุญุตุงุฆูุงุช ุงููุนุงูุฌุฉ")
        
        logger.info(f"ุฃุณุทุฑ ุงููุฏุฎูุงุช:  {self.stats.total_input_lines:,}")
        logger.info(f"ุฃุณุทุฑ ุงููุฎุฑุฌุงุช:  {self.stats.total_output_lines:,}")
        
        kept_pct = self.stats.total_output_lines / max(self.stats.total_input_lines, 1) * 100
        logger.info(f"ูุณุจุฉ ุงูุฅุจูุงุก:   {kept_pct:.2f}%")
        
        logger.subsection("ุฅุญุตุงุฆูุงุช ุงูุชุตููุฉ")
        logger.info(f"ุฃุณุทุฑ ูุงุฑุบุฉ ูุญุฐููุฉ:     {self.stats.empty_lines_removed:,}")
        logger.info(f"ุฃุณุทุฑ ูุตูุฑุฉ ูุญุฐููุฉ:     {self.stats.short_lines_removed:,}")
        logger.info(f"ุฃุณุทุฑ ุทูููุฉ ููุตูุตุฉ:     {self.stats.long_lines_truncated:,}")
        
        logger.subsection("ุฅุญุตุงุฆูุงุช ุงูุชุทุจูุน")
        logger.info(f"ุชุดููู ูุญุฐูู:           {self.stats.diacritics_removed:,}")
        logger.info(f"ุฃูู ููุญุฏุฉ:             {self.stats.alef_normalized:,}")
        logger.info(f"ุชุฑููู ููุญุฏ:            {self.stats.punct_normalized:,}")
        logger.info(f"ุฃุฑูุงู ููุญุฏุฉ:           {self.stats.numbers_normalized:,}")
        logger.info(f"ูุงุชููู ูุญุฐูู:          {self.stats.latin_removed:,}")
        logger.info(f"OOV ูุญุฐูู:             {self.stats.oov_removed:,}")
        logger.info(f"ุชุฑููู ูุชุชุงุจุน ูุตูุญ:     {self.stats.consecutive_punct_fixed:,}")


# ุฅูุดุงุก ุงููุนุงูุฌ
preprocessor = ArabicTextPreprocessor(config)
logger.success("ุชู ุชููุฆุฉ ุงููุนุงูุฌ!")

# %% [markdown]
# ### ุงุฎุชุจุงุฑ ุฎุท ุงูุฃูุงุจูุจ ุงููุงูู

# %%
# ============================================================================
# ุงุฎุชุจุงุฑ ุฎุท ุงูุฃูุงุจูุจ ุงููุงูู
# ============================================================================

logger.section("๐งช ุงุฎุชุจุงุฑ ุฎุท ุงูุฃูุงุจูุจ ุงููุงูู")

# ุญุงูุงุช ุงุฎุชุจุงุฑ ุชุบุทู ูุดููุงุช ูุชููุนุฉ
test_cases = [
    # ุชุดููู
    "ุงูุฃููููู ุงูููุชููุญูุฏูุฉ ููููุธููููุฉ ุฏููููููููุฉ.",
    
    # ุชุฑููู ูุฎุชูุท
    "ุฃููุงู, ุซุงููุงู; ุซุงูุซุงู?",
    
    # ุฃุดูุงู ุงูุฃูู
    "ุฃุญูุฏ ูุฅุจุฑุงููู ูุขุฏู",
    
    # ุฃุฑูุงู
    "ูู ุนุงู 2024 ูุตู ุงูุนุฏุฏ ุฅูู 100",
    
    # ูุต ูุงุชููู
    "ุงููุซููุฉ A/47/10 ูุงููุฑุงุฑ UNDP/2024",
    
    # ุชุฑููู ูุชุชุงุจุน
    "ูุงุฐุงุุุ ูุฐุง ุตุญูุญ...",
    
    # ูุดููุงุช ูุณุงูุงุช
    "ุงููุต   ูุน    ูุณุงูุงุช   ูุซูุฑุฉ",
    
    # ุฌููุฉ ูุตูุฑุฉ (ูุฌุจ ุฃู ุชุญุฐู)
    "ูุนู",
    
    # ูุดููุงุช ูุฌูุนุฉ
    "ููููุงูู ุฃุญูุฏ: ูุฐุง ููููููู ุฌูุฏููุงู,, ูุงููู!!",
]

logger.info("ูุนุงูุฌุฉ ุญุงูุงุช ุงูุงุฎุชุจุงุฑ:\n")

for i, test in enumerate(test_cases, 1):
    result = preprocessor.preprocess_line(test, apply_optional=False)
    logger.info(f"ุงูุงุฎุชุจุงุฑ {i}:")
    logger.info(f"   ุงูุฏุฎู:  '{test}'")
    if result:
        logger.info(f"   ุงูุฎุฑุฌ:  '{result}'")
    else:
        logger.info(f"   ุงูุฎุฑุฌ:  [ุชูุช ุงูุชุตููุฉ]")
    logger.info("")

# %% [markdown]
# ---
# ## 6. ุงูุฌุฒุก 5: ุงููุญุต ุจุนุฏ ุงููุนุงูุฌุฉ

# %%
# ============================================================================
# ุงููุณู 6: ุงููุญุต ุจุนุฏ ุงููุนุงูุฌุฉ
# ============================================================================

def inspect_preprocessed_data(file_path: str, sample_size: int = 100000) -> Dict:
    """
    ูุญุต ุงูุจูุงูุงุช ุงููุนุงูุฌุฉ ููุชุญูู ูู ุงูุฌูุฏุฉ.
    
    ุงููุนุงููุงุช:
    -----------
    file_path : str
        ูุณุงุฑ ุงูููู ุงููุนุงูุฌ
    sample_size : int
        ุนุฏุฏ ุงูุฃุณุทุฑ ูููุญุต
        
    ุชุฑุฌุน:
    --------
    Dict
        ูุชุงุฆุฌ ุงููุญุต
    """
    logger.section("๐ ูุญุต ูุง ุจุนุฏ ุงููุนุงูุฌุฉ")
    logger.info(f"ุฌุงุฑู ูุญุต: {file_path}")
    
    stats = {
        'total_lines': 0,
        'total_words': 0,
        'total_chars': 0,
        'word_counts': [],
        'remaining_issues': {
            'diacritics': 0,
            'latin_letters': 0,
            'western_numbers': 0,
            'latin_punct': 0,
            'consecutive_punct': 0,
            'alef_variations': 0,
        },
        'punctuation_dist': Counter(),
        'sample_lines': [],
    }
    
    if not os.path.exists(file_path):
        logger.error(f"ุงูููู ุบูุฑ ููุฌูุฏ: {file_path}")
        return stats
    
    # ุฃููุงุท ูููุญุต
    diacritics_pattern = re.compile(r'[\u064B-\u0652]')
    latin_pattern = re.compile(r'[A-Za-z]')
    western_num_pattern = re.compile(r'[0-9]')
    consecutive_punct_pattern = re.compile(r'[ุุุ.,:;?!]{2,}')
    alef_var_pattern = re.compile(r'[ุฃุฅุขูฑ]')
    arabic_word_pattern = re.compile(r'[\u0600-\u06FF]+')
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= sample_size:
                break
            
            line = line.rstrip('\n')
            stats['total_lines'] += 1
            stats['total_chars'] += len(line)
            
            words = arabic_word_pattern.findall(line)
            stats['total_words'] += len(words)
            stats['word_counts'].append(len(words))
            
            # ุชุฎุฒูู ุนููุฉ ุฃุณุทุฑ
            if len(stats['sample_lines']) < 10:
                stats['sample_lines'].append(line)
            
            # ุงูุชุญูู ูู ุงููุดููุงุช ุงููุชุจููุฉ
            if diacritics_pattern.search(line):
                stats['remaining_issues']['diacritics'] += 1
            if latin_pattern.search(line):
                stats['remaining_issues']['latin_letters'] += 1
            if western_num_pattern.search(line):
                stats['remaining_issues']['western_numbers'] += 1
            if consecutive_punct_pattern.search(line):
                stats['remaining_issues']['consecutive_punct'] += 1
            if alef_var_pattern.search(line):
                stats['remaining_issues']['alef_variations'] += 1
            
            # ุนุฏ ุงูุชุฑููู
            for char in line:
                if char in VALID_PUNCTUATION:
                    stats['punctuation_dist'][char] += 1
    
    # ุนุฑุถ ุงููุชุงุฆุฌ
    logger.subsection("ุฅุญุตุงุฆูุงุช ุฃุณุงุณูุฉ")
    logger.info(f"ุฅุฌูุงูู ุงูุฃุณุทุฑ: {stats['total_lines']:,}")
    logger.info(f"ุฅุฌูุงูู ุงููููุงุช: {stats['total_words']:,}")
    logger.info(f"ุฅุฌูุงูู ุงูุญุฑูู: {stats['total_chars']:,}")
    
    if stats['word_counts']:
        word_arr = np.array(stats['word_counts'])
        logger.info(f"ูุชูุณุท ุงููููุงุช/ุณุทุฑ: {np.mean(word_arr):.2f}")
        logger.info(f"ุงูุญุฏ ุงูุฃุฏูู: {np.min(word_arr)}")
        logger.info(f"ุงูุญุฏ ุงูุฃูุตู: {np.max(word_arr)}")
    
    logger.subsection("ูุญุต ุงููุดููุงุช ุงููุชุจููุฉ")
    total_issues = sum(stats['remaining_issues'].values())
    if total_issues == 0:
        logger.success("ูู ูุชู ุงูุนุซูุฑ ุนูู ูุดููุงุช! ุงูุจูุงูุงุช ูุธููุฉ.")
    else:
        logger.warn(f"ุชู ุงูุนุซูุฑ ุนูู {total_issues} ูุดููุฉ ูุญุชููุฉ:")
        for issue, count in stats['remaining_issues'].items():
            if count > 0:
                logger.info(f"   {issue}: {count:,} ุณุทุฑ")
    
    logger.subsection("ุชูุฒูุน ุงูุชุฑููู")
    for char, count in stats['punctuation_dist'].most_common():
        name = ARABIC_PUNCTUATION.get(char, 'ุบูุฑ ูุนุฑูู')
        logger.info(f"   '{char}' ({name}): {count:,}")
    
    logger.subsection("ุนููุฉ ุฃุณุทุฑ")
    for i, line in enumerate(stats['sample_lines'][:5], 1):
        display = line[:80] + "..." if len(line) > 80 else line
        logger.info(f"   {i}. {display}")
    
    return stats


# ุณูุชู ุชุดุบูู ูุฐุง ุจุนุฏ ูุนุงูุฌุฉ ุงูุจูุงูุงุช

# %% [markdown]
# ---
# ## 7. ุงูุฌุฒุก 6: ุญูุธ ุงูุจูุงูุงุช ุงููุนุงูุฌุฉ

# %%
# ============================================================================
# ุงููุณู 7: ูุนุงูุฌุฉ ูุญูุธ ุงูุจูุงูุงุช
# ============================================================================

def run_preprocessing_pipeline(
    input_dir: str,
    output_dir: str,
    config: PreprocessingConfig,
    create_variants: bool = True,
    sample_size: Optional[int] = None
):
    """
    ุชุดุบูู ุฎุท ุฃูุงุจูุจ ุงููุนุงูุฌุฉ ุงููุงูู ูุญูุธ ุงููุชุงุฆุฌ.
    
    ููุดุฆ ุนุฏุฉ ูุชุบูุฑุงุช ูู ุงููุฎุฑุฌุงุช:
    1. mandatory_only.txt - ูุนุงูุฌุฉ ุฅูุฒุงููุฉ ููุท
    2. with_waw_separation.txt - + ูุตู ุงููุงู
    3. with_number_tokens.txt - + ุงุณุชุจุฏุงู ุงูุฃุฑูุงู
    4. full_preprocessing.txt - ุฌููุน ุงูุฎุทูุงุช
    
    ุงููุนุงููุงุช:
    -----------
    input_dir : str
        ูุณุงุฑ ูุฌูุฏ ุงููุฏุฎูุงุช
    output_dir : str
        ูุณุงุฑ ูุฌูุฏ ุงููุฎุฑุฌุงุช
    config : PreprocessingConfig
        ูุงุฆู ุงูุฅุนุฏุงุฏุงุช
    create_variants : bool
        ุฅูุดุงุก ูุชุบูุฑุงุช ูุชุนุฏุฏุฉ ุฃู ูุง
    sample_size : Optional[int]
        ุญุฏ ุงููุนุงูุฌุฉ (None = ูุงูู)
    """
    logger.section("๐ ุชุดุบูู ุฎุท ุฃูุงุจูุจ ุงููุนุงูุฌุฉ")
    
    # ุฅูุดุงุก ูุฌูุฏ ุงููุฎุฑุฌุงุช
    os.makedirs(output_dir, exist_ok=True)
    
    # ============================================
    # ุงููุชุบูุฑ 1: ุงููุนุงูุฌุฉ ุงูุฅูุฒุงููุฉ ููุท
    # ============================================
    logger.subsection("ุงููุชุบูุฑ 1: ุงููุนุงูุฌุฉ ุงูุฅูุฒุงููุฉ ููุท")
    
    output_file_mandatory = os.path.join(output_dir, "mandatory_only.txt")
    
    # ุถูุงู ุฅููุงู ุงูุฎุทูุงุช ุงูุงุฎุชูุงุฑูุฉ
    config.separate_waw_conjunction = False
    config.replace_numbers_with_token = False
    config.remove_foreign_terms = False
    
    preprocessor = ArabicTextPreprocessor(config)
    stats_mandatory = preprocessor.process_dataset(
        input_dir, 
        output_file_mandatory,
        apply_optional=False,
        sample_size=sample_size
    )
    
    # ุญูุธ ุงูุฅุญุตุงุฆูุงุช
    stats_file = os.path.join(output_dir, "stats_mandatory.json")
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump({
            'total_input': stats_mandatory.total_input_lines,
            'total_output': stats_mandatory.total_output_lines,
            'empty_removed': stats_mandatory.empty_lines_removed,
            'short_removed': stats_mandatory.short_lines_removed,
            'long_truncated': stats_mandatory.long_lines_truncated,
        }, f, indent=2)
    
    if not create_variants:
        return
    
    # ============================================
    # ุงููุชุบูุฑ 2: ูุน ูุตู ุงููุงู
    # ============================================
    logger.subsection("ุงููุชุบูุฑ 2: ูุน ูุตู ุงููุงู")
    
    config.separate_waw_conjunction = True
    preprocessor = ArabicTextPreprocessor(config)
    
    output_file_waw = os.path.join(output_dir, "with_waw_separation.txt")
    stats_waw = preprocessor.process_dataset(
        input_dir,
        output_file_waw,
        apply_optional=True,
        sample_size=sample_size
    )
    
    config.separate_waw_conjunction = False  # ุฅุนุงุฏุฉ ุถุจุท
    
    # ============================================
    # ุงููุชุบูุฑ 3: ูุน ุฑููุฒ ุงูุฃุฑูุงู
    # ============================================
    logger.subsection("ุงููุชุบูุฑ 3: ูุน ุฑููุฒ ุงูุฃุฑูุงู")
    
    config.replace_numbers_with_token = True
    preprocessor = ArabicTextPreprocessor(config)
    
    output_file_nums = os.path.join(output_dir, "with_number_tokens.txt")
    stats_nums = preprocessor.process_dataset(
        input_dir,
        output_file_nums,
        apply_optional=True,
        sample_size=sample_size
    )
    
    config.replace_numbers_with_token = False  # ุฅุนุงุฏุฉ ุถุจุท
    
    # ============================================
    # ุงููุชุบูุฑ 4: ุงููุนุงูุฌุฉ ุงููุงููุฉ
    # ============================================
    logger.subsection("ุงููุชุบูุฑ 4: ุงููุนุงูุฌุฉ ุงููุงููุฉ (ูู ุงูุฎูุงุฑุงุช)")
    
    config.separate_waw_conjunction = True
    config.replace_numbers_with_token = True
    config.remove_foreign_terms = True
    
    preprocessor = ArabicTextPreprocessor(config)
    
    output_file_full = os.path.join(output_dir, "full_preprocessing.txt")
    stats_full = preprocessor.process_dataset(
        input_dir,
        output_file_full,
        apply_optional=True,
        sample_size=sample_size
    )
    
    logger.section("โ ุชูุช ุงููุนุงูุฌุฉ")
    logger.info(f"ุชู ุญูุธ ุงููููุงุช ูู: {output_dir}")
    logger.info("ุงููููุงุช ุงูุชู ุชู ุฅูุดุงุคูุง:")
    logger.info(f"   1. mandatory_only.txt")
    logger.info(f"   2. with_waw_separation.txt")
    logger.info(f"   3. with_number_tokens.txt")
    logger.info(f"   4. full_preprocessing.txt")


# %%
# ============================================================================
# ุชูููุฐ ุงููุนุงูุฌุฉ
# ============================================================================

# ุฅุนุฏุงุฏุงุช ุงูุชูููุฐ
EXECUTE_FULL_PIPELINE = True  # ุงุฌุนููุง True ูุชูููุฐ ุงููุนุงูุฌุฉ
SAMPLE_SIZE_FOR_TEST = 100000  # ุงุฌุนููุง None ูููุนุงูุฌุฉ ุงููุงููุฉ

if EXECUTE_FULL_PIPELINE:
    logger.section("โก ุชูููุฐ ุฎุท ุฃูุงุจูุจ ุงููุนุงูุฌุฉ")
    
    # ููุงุฎุชุจุงุฑุ ุงุณุชุฎุฏู ุนููุฉ
    sample = SAMPLE_SIZE_FOR_TEST  # ุบูุฑูุง ูู None ูููุนุงูุฌุฉ ุงููุงููุฉ
    
    run_preprocessing_pipeline(
        input_dir=config.input_dir,
        output_dir=config.output_dir,
        config=config,
        create_variants=True,
        sample_size=sample
    )
    
    # ูุญุต ุงููุงุชุฌ ุงูุฅูุฒุงูู
    mandatory_file = os.path.join(config.output_dir, "mandatory_only.txt")
    if os.path.exists(mandatory_file):
        post_stats = inspect_preprocessed_data(mandatory_file, sample_size=50000)
else:
    logger.info("ุชู ุชุฎุทู ุงูุชูููุฐ. ุงุฌุนู EXECUTE_FULL_PIPELINE = True ููุชูููุฐ.")

# %% [markdown]
# ---
# ## 8. ุงูููุฎุต ูุงูุชูุตูุงุช

# %%
# ============================================================================
# ุงููุณู 8: ุงูููุฎุต ูุงูุชูุตูุงุช
# ============================================================================

logger.section("๐ ููุฎุต ุงููุนุงูุฌุฉ ูุงูุชูุตูุงุช")

summary = """
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ                        ููุฎุต ุฎุท ุฃูุงุจูุจ ุงููุนุงูุฌุฉ                                 โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโฃ
โ                                                                              โ
โ  ุฎุทูุงุช ุงููุนุงูุฌุฉ ุงูุฅูุฒุงููุฉ (ุชุทุจู ุฏุงุฆูุงู):                                     โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                                      โ
โ  โ ุฅุฒุงูุฉ ุงูุชุดููู (ุงูุญุฑูุงุช)                                                  โ
โ  โ ุชูุญูุฏ ุฃุดูุงู ุงูุฃูู (ุฃุฅุขูฑ -> ุง)                                            โ
โ  โ ุชูุญูุฏ ุงูุฃูู ุงูููุตูุฑุฉ (ู -> ู)                                            โ
โ  โ ุฅุฒุงูุฉ ุงูุชุทููู (ู)                                                        โ
โ  โ ุชูุญูุฏ ุงูุชุฑููู ููุนุฑุจู (,;? -> ุุุ)                                        โ
โ  โ ุชูุญูุฏ ุงูุฃุฑูุงู ููุนุฑุจูุฉ (0-9 -> ู-ูฉ)                                       โ
โ  โ ุฅุฒุงูุฉ ุงูุญุฑูู ุงููุงุชูููุฉ                                                   โ
โ  โ ุฅุฒุงูุฉ ุงูุญุฑูู ุฎุงุฑุฌ ุงููุทุงู (OOV)                                           โ
โ  โ ูุนุงูุฌุฉ ุงูุชุฑููู ุงููุชุชุงุจุน                                                  โ
โ  โ ุชูุญูุฏ ุงููุณุงูุงุช                                                           โ
โ  โ ุฅุถุงูุฉ ูุณุงูุงุช ุงูุชุฑููู                                                     โ
โ  โ ุชุตููุฉ ุงูุฃุณุทุฑ ุงููุงุฑุบุฉ                                                     โ
โ  โ ุชุตููุฉ ุงูุฌูู ุงููุตูุฑุฉ (<3 ูููุงุช)                                           โ
โ  โ ูุต ุงูุฌูู ุงูุทูููุฉ (>100 ูููุฉ)                                             โ
โ                                                                              โ
โ  ุฎุทูุงุช ุงููุนุงูุฌุฉ ุงูุงุฎุชูุงุฑูุฉ (ููุชุฌุฑูุจ):                                        โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                                         โ
โ  โ๏ธ  ูุตู ูุงู ุงูุนุทู (ู + ูููุฉ -> ู ูููุฉ)                                      โ
โ  โ๏ธ  ุงุณุชุจุฏุงู ุงูุฃุฑูุงู ุจุฑูุฒ (<NUM>)                                            โ
โ  โ๏ธ  ุฅุฒุงูุฉ ูุฑุงุฌุน ุงููุณุชูุฏุงุช (A/47/10)                                         โ
โ  โ๏ธ  ุฅุฒุงูุฉ/ุชูููุฒ ูููุงุช ุงูุชููู                                                โ
โ  โ๏ธ  ุงุณุชุจุฏุงู ุงููููุงุช ุงููุงุฏุฑุฉ (<UNK>)                                         โ
โ                                                                              โ
โ  ุงููููุงุช ุงููุงุชุฌุฉ:                                                            โ
โ  โโโโโโโโโโโโโโโ                                                             โ
โ  ๐ preprocessed_data/                                                       โ
โ     โโโ mandatory_only.txt      (ููุตู ุจู ููุนุธู ุงูุชุฌุงุฑุจ)                      โ
โ     โโโ with_waw_separation.txt (ูุงุฎุชุจุงุฑ ุชุฃุซูุฑ ูุตู ุงููุงู)                    โ
โ     โโโ with_number_tokens.txt  (ูุงุฎุชุจุงุฑ ุชุฃุซูุฑ ุงุณุชุจุฏุงู ุงูุฃุฑูุงู)              โ
โ     โโโ full_preprocessing.txt  (ูู ุงูุฎุทูุงุช ูุทุจูุฉ)                           โ
โ                                                                              โ
โ  ุงูุชูุตูุงุช:                                                                   โ
โ  โโโโโโโโโ                                                                   โ
โ  1. ุงุจุฏุฃ ุจู mandatory_only.txt ููุชุฌุงุฑุจ ุงูุฃุณุงุณูุฉ                              โ
โ  2. ุงุณุชุฎุฏู with_waw_separation.txt ุฅุฐุง ุณุงุนุฏ ูุตู ุงููุงู                        โ
โ  3. ุฌุฑุจ with_number_tokens.txt ุฅุฐุง ุณุจุจุช ุงูุฃุฑูุงู ูุดููุงุช                       โ
โ  4. ูุงุฑู ุงููุชุงุฆุฌ ูุชุญุฏูุฏ ุงููุนุงูุฌุฉ ุงูุฃูุซู                                      โ
โ                                                                              โ
โ  ุงูุฎุทูุงุช ุงูุชุงููุฉ:                                                            โ
โ  โโโโโโโโโโโโโโโ                                                             โ
โ  1. ุชุฑููุฒ ุงูุจูุงูุงุช ุงููุนุงูุฌุฉ (Tokenization)                                   โ
โ  2. ุฅูุดุงุก ุชูุณููุงุช ุงูุชุฏุฑูุจ/ุงูุชุญูู/ุงูุงุฎุชุจุงุฑ                                    โ
โ  3. ุชูููุฏ ุงูุชุตูููุงุช (Labels) ููููุฉ ุงูุชุณูุณู-ุฅูู-ุชุณูุณู                         โ
โ  4. ุชุฏุฑูุจ ูุชูููู ุงูููุงุฐุฌ                                                     โ
โ                                                                              โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
"""

print(summary)

# %%
# ============================================================================
# ุฃุฏุงุฉ ุชูููุฏ ุงูุชุตูููุงุช (Label Generation)
# ============================================================================

def generate_labels_for_line(text: str) -> Tuple[List[str], List[int]]:
    """
    ุชูููุฏ ุชุณูุณูุงุช ุงููููุงุช ูุงูุชุตูููุงุช ูุณุทุฑ ูุนุงูุฌ.
    
    ูุฐุง ูู ุงูุชูุณูู ุงููุทููุจ ูุชุฏุฑูุจ ุงูุชุณูุณู-ุฅูู-ุชุณูุณู.
    
    ุงููุนุงููุงุช:
    -----------
    text : str
        ุณุทุฑ ูุต ูุนุงูุฌ ูุณุจูุงู
        
    ุชุฑุฌุน:
    --------
    Tuple[List[str], List[int]]
        (ูููุงุชุ ุชุตูููุงุช) ุญูุซ ุชุดูุฑ ุงูุชุตูููุงุช ููุชุฑููู ุจุนุฏ ูู ูููุฉ
        
    ุฎุฑูุทุฉ ุงูุชุตูููุงุช:
    - 0: ูุง ุชุฑููู (O)
    - 1: ููุทุฉ (.)
    - 2: ูุงุตูุฉ ุนุฑุจูุฉ (ุ)
    - 3: ุนูุงูุฉ ุงุณุชููุงู (ุ)
    - 4: ูุงุตูุฉ ููููุทุฉ (ุ)
    - 5: ููุทุชุงู ุฑุฃุณูุชุงู (:)
    - 6: ุนูุงูุฉ ุชุนุฌุจ (!)
    """
    LABEL_MAP = {
        'O': 0,    # ูุง ุชุฑููู
        '.': 1,    # ููุทุฉ
        'ุ': 2,    # ูุงุตูุฉ ุนุฑุจูุฉ
        'ุ': 3,    # ุนูุงูุฉ ุงุณุชููุงู
        'ุ': 4,    # ูุงุตูุฉ ููููุทุฉ
        ':': 5,    # ููุทุชุงู ุฑุฃุณูุชุงู
        '!': 6,    # ุนูุงูุฉ ุชุนุฌุจ
    }
    
    words = []
    labels = []
    
    # ููุท ูููููุงุช ุงูุนุฑุจูุฉ
    word_pattern = re.compile(r'[\u0600-\u06FFู-ูฉ]+')
    
    # ุงูุนุซูุฑ ุนูู ุงููููุงุช ูููุงูุนูุง
    for match in word_pattern.finditer(text):
        word = match.group()
        end_pos = match.end()
        
        # ุงูุจุญุซ ุนู ุชุฑููู ูุจุงุดุฑุฉ ุจุนุฏ ุงููููุฉ
        remaining = text[end_pos:].lstrip()
        
        if remaining and remaining[0] in LABEL_MAP:
            label = LABEL_MAP[remaining[0]]
        else:
            label = LABEL_MAP['O']
        
        words.append(word)
        labels.append(label)
    
    return words, labels


# ุงุฎุชุจุงุฑ ุชูููุฏ ุงูุชุตูููุงุช
logger.section("๐ท๏ธ ูุซุงู ุชูููุฏ ุงูุชุตูููุงุช")

test_lines = [
    "ูุฐุง ูุต ุนุฑุจูุ ูุญุชูู ุนูู ุนูุงูุงุช ุชุฑููู.",
    "ูุง ูู ุงูุณุคุงูุ",
    "ุฃููุงูุ ุซุงููุงูุ ุซุงูุซุงู.",
]

logger.info("ุฎุฑูุทุฉ ุงูุชุตูููุงุช:")
logger.info("   0: ูุง ุชุฑููู (O)")
logger.info("   1: ููุทุฉ (.)")
logger.info("   2: ูุงุตูุฉ (ุ)")
logger.info("   3: ุงุณุชููุงู (ุ)")
logger.info("   4: ูุงุตูุฉ ููููุทุฉ (ุ)")
logger.info("   5: ููุทุชุงู (:)")
logger.info("   6: ุชุนุฌุจ (!)")
logger.info("")

for text in test_lines:
    words, labels = generate_labels_for_line(text)
    logger.info(f"ุงููุต: {text}")
    logger.info(f"ุงููููุงุช: {words}")
    logger.info(f"ุงูุชุตูููุงุช: {labels}")
    logger.info("")

# %%
logger.section("โ ุงูุชูู ุฏูุชุฑ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ")
logger.info("ุชู ุชุนุฑูู ุฌููุน ุฏูุงู ูุฎุทูุงุช ุงููุนุงูุฌุฉ.")
logger.info("ุดุบู ุงูุฎุท ุงููุงูู ุจูุถุน EXECUTE_FULL_PIPELINE = True ููุนุงูุฌุฉ ูุงูู ุงูุจูุงูุงุช.")